# Task ID: 3
# Title: Dataset Creation and Splitting
# Status: done
# Dependencies: 2
# Priority: high
# Description: Create training, validation, and test datasets for spelling tasks, ensuring proper separation between training tokens and validation/test words to evaluate if training on spelling improves model performance on position and count question metrics.
# Details:
1. Download English word list from dwyl/english-words repository
2. Create training set from tokenizer vocabulary (multi-character, letter-only tokens)
3. Create validation/test sets from English dictionary words that:
   - Split into multiple tokens by the tokenizer
   - Have at least one token in the split that is multi-character and letter-only
   - Do not appear in the training set
4. Generate letter count questions ("How many X's are in Y?")
5. Generate letter position questions ("What is the Nth letter in Y?")
6. Split data based on source rather than percentage: training from tokenizer vocabulary, validation/test from filtered external word lists
7. Format as a Hugging Face dataset with appropriate splits
8. Create notebook to visualize dataset statistics
9. Establish a Hugging Face benchmark with evaluation scripts and leaderboard integration

NOTE: The dataset split is NOT based on percentage. The training set (universal set) comes from tokenizer vocabulary, while validation and test sets (hold-out sets) come from external word lists. This source-based split is essential for the experiment's purpose of determining if training on spelling improves model performance on position and count question metrics.

File Structure:
- Raw word lists: `data/raw/word_lists/`
- Processed word lists: `data/processed/word_lists/`
- Training set: `data/splits/train.json`
- Validation set: `data/splits/val.json`
- Test set: `data/splits/test.json`
- Question generation scripts: `src/data/question_generator.py` and `src/data/utils.py`
- Dataset formatting scripts: `src/data/dataset_formatter.py` and `src/data/dataset_builder.py`
- Documentation: `docs/dataset.md` and `docs/split_methodology.md`
- Analysis notebooks: `notebooks/dataset_analysis.ipynb` and `notebooks/split_verification.ipynb`

Implementation:
```python
import json
import random
import string
import os
from datasets import Dataset
from sklearn.model_selection import train_test_split
import requests

# Create directories if they don't exist
os.makedirs("data/raw/word_lists", exist_ok=True)
os.makedirs("data/processed/word_lists", exist_ok=True)
os.makedirs("data/splits", exist_ok=True)

# Download English word list
word_list_url = "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
response = requests.get(word_list_url)
all_words = response.text.splitlines()

# Save raw word list
with open("data/raw/word_lists/english_words.txt", "w") as f:
    f.write("\n".join(all_words))

# Load tokenizer and filtered tokens
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
with open("gpt2_letter_tokens.json", "r") as f:
    tokens_data = json.load(f)

tokens = [t["token"] for t in tokens_data["tokens"]]

# Save processed tokens
with open("data/processed/word_lists/tokenizer_vocabulary.json", "w") as f:
    json.dump({"tokens": tokens}, f, indent=2)

# Find valid validation/test words
valid_words = []
for word in all_words:
    if not word.isalpha():
        continue
    
    # Tokenize the word
    token_ids = tokenizer.encode(word, add_special_tokens=False)
    tokens_in_word = tokenizer.convert_ids_to_tokens(token_ids)
    
    # Check if word splits into multiple tokens with at least one multi-character token
    if len(tokens_in_word) > 1 and any(len(tokenizer.convert_tokens_to_string([t])) > 1 for t in tokens_in_word):
        # Ensure word is not in training set
        if word.lower() not in [t.lower() for t in tokens]:
            valid_words.append(word)

# Save processed valid words
with open("data/processed/word_lists/valid_external_words.json", "w") as f:
    json.dump({"words": valid_words}, f, indent=2)

# Split valid words into validation and test sets
val_words, test_words = train_test_split(valid_words, test_size=0.5, random_state=42)

# Generate questions using question_generator.py
from src.data.question_generator import generate_questions

# Generate training, validation, and test questions
train_questions = generate_questions(tokens, "letter_count") + generate_questions(tokens, "letter_position")
val_questions = generate_questions(val_words, "letter_count") + generate_questions(val_words, "letter_position")
test_questions = generate_questions(test_words, "letter_count") + generate_questions(test_words, "letter_position")

# Save splits to JSON files
with open("data/splits/train.json", "w") as f:
    json.dump({"questions": train_questions}, f, indent=2)

with open("data/splits/val.json", "w") as f:
    json.dump({"questions": val_questions}, f, indent=2)

with open("data/splits/test.json", "w") as f:
    json.dump({"questions": test_questions}, f, indent=2)

# Create datasets using dataset_formatter.py and dataset_builder.py
from src.data.dataset_formatter import format_dataset
from src.data.dataset_builder import build_and_push_dataset

# Format and build the dataset
datasets = format_dataset(train_questions, val_questions, test_questions)
combined_dataset = build_and_push_dataset(datasets, "YOUR-USERNAME/llm-spelling-dataset")
```

# Test Strategy:
1. Verify dataset successfully generates 2,000+ questions
2. Confirm questions are grammatically correct
3. Verify train/validation/test splits come from appropriate sources (training from tokenizer vocabulary, validation/test from filtered external words)
4. Manually check 20 random samples to ensure answers correctly match questions
5. Confirm dataset is successfully pushed to Hugging Face
6. Verify local JSON files are created for each split in the correct locations:
   - `data/splits/train.json`
   - `data/splits/val.json`
   - `data/splits/test.json`
7. Create and review notebook `notebooks/dataset_analysis.ipynb` exploring dataset statistics
8. Test evaluation scripts to ensure they correctly measure performance on position and count question metrics
9. Verify benchmark integration with Hugging Face leaderboard
10. Use `notebooks/split_verification.ipynb` to verify there is no overlap between the universal set (training) and hold-out sets (validation/test) to ensure valid measurement of model performance improvements
11. Check that all documentation files (`docs/dataset.md` and `docs/split_methodology.md`) are complete and accurate

# Subtasks:
## 1. Word List Acquisition [done]
### Dependencies: None
### Description: Gather a comprehensive and diverse word list from reliable sources, ensuring coverage of the desired vocabulary scope.
### Details:
Implementation involves sourcing words from open datasets, dictionaries, or APIs. Validation criteria include checking for duplicates, ensuring language consistency, and verifying word authenticity. Challenges include handling noisy data, inconsistent formats, and ensuring the list is representative of the target domain.
<info added on 2025-05-07T14:45:31.121Z>
Implementation involves sourcing words from open datasets, dictionaries, or APIs. Validation criteria include checking for duplicates, ensuring language consistency, and verifying word authenticity. Challenges include handling noisy data, inconsistent formats, and ensuring the list is representative of the target domain.

This task is broken down into three subtasks that can be executed in parallel:

1. Sourcing: Identify and collect words from multiple reliable sources such as open datasets, dictionaries, APIs, and academic word lists. Focus on gathering a comprehensive set that covers the desired vocabulary scope. This task can be worked on independently and in parallel with others. (parallelizable: true)

2. Cleaning: Process the collected words to remove duplicates, standardize formats, handle special characters, and ensure consistent casing. Address any encoding issues and normalize variations of the same word. This task can be worked on independently and in parallel with others. (parallelizable: true)

3. Validation: Verify the authenticity and appropriateness of words in the list. Check for language consistency, filter out inappropriate content, and ensure the words meet the project's requirements. This task can be worked on independently and in parallel with others. (parallelizable: true)

The overall Word List Acquisition task is parallelizable, with team members able to work on different subtasks simultaneously to improve efficiency.
</info added on 2025-05-07T14:45:31.121Z>

## 2. Training/Validation/Test Set Creation with Filtering [done]
### Dependencies: 3.1
### Description: Create distinct datasets from different sources: training set from tokenizer vocabulary and validation/test sets from filtered external word lists to establish a true holdout set for testing.
### Details:
Implementation requires extracting tokenizer vocabulary for training and applying strict filtering criteria to external word lists for validation/test sets. Ensure no overlap between training tokens and validation/test words. Validation involves statistical checks for distribution balance and manual spot checks for leakage. Challenges include maintaining diversity across splits and implementing robust filtering logic.

File Structure:
- Raw word lists stored in: `data/raw/word_lists/`
- Processed word lists stored in: `data/processed/word_lists/`
- Training set saved to: `data/splits/train.json`
- Validation set saved to: `data/splits/val.json`
- Test set saved to: `data/splits/test.json`

Verification of splits should be documented in `notebooks/split_verification.ipynb`.

## 3. Question Generation for Each Type [done]
### Dependencies: 3.2
### Description: Automatically generate letter count and letter position questions for each word in the dataset to establish metrics for evaluating model performance.
### Details:
Implementation uses templates to generate questions per word for both letter count and letter position types. Validation includes checking for grammatical correctness, relevance, and uniqueness of questions. Challenges involve ensuring variety in question phrasing and scaling generation efficiently across the universal set and holdout set.

Implementation should be in:
- Main script: `src/data/question_generator.py`
- Utility functions: `src/data/utils.py`

The generated questions will be stored in the split files:
- `data/splits/train.json`
- `data/splits/val.json`
- `data/splits/test.json`

## 4. Dataset Formatting and Splitting [done]
### Dependencies: 3.3
### Description: Format the generated data according to Hugging Face requirements, ensuring proper structure and metadata for each split based on their distinct sources.
### Details:
Implementation involves structuring data as JSON, CSV, or other required formats, with clear fields for input, output, and metadata. Validation checks include schema compliance, correct split assignments, and tokenization compatibility. Challenges include handling edge cases in formatting and ensuring compatibility with downstream tools. Note that splits are based on source (not percentage): training uses tokenizer vocabulary while validation/test use external word lists.

Implementation should use:
- Main formatting script: `src/data/dataset_formatter.py`
- HuggingFace dataset script: `src/data/dataset_builder.py`

The formatted datasets should be saved to:
- `data/splits/train.json`
- `data/splits/val.json`
- `data/splits/test.json`

## 5. Dataset Publishing and Benchmark Creation [done]
### Dependencies: 3.4
### Description: Publish the finalized dataset to Hugging Face and establish a benchmark with evaluation scripts and leaderboard integration.
### Details:
Implementation includes uploading dataset files, creating evaluation scripts that measure performance on position and count question metrics, integrating with Hugging Face leaderboard, and writing detailed documentation. Validation involves verifying downloadability, documentation clarity, and reproducibility. Challenges include ensuring evaluation scripts accurately reflect the experiment's purpose of determining if training on spelling improves model performance.

Documentation should be created in:
- `docs/dataset.md` - General dataset documentation
- `docs/split_methodology.md` - Detailed explanation of the split methodology

Dataset analysis should be performed in:
- `notebooks/dataset_analysis.ipynb`

## 6. Universal Set and Holdout Set Verification [done]
### Dependencies: 3.2
### Description: Verify that the training set (universal set) and test set (true holdout set) are properly separated to enable valid measurement of model performance improvements.
### Details:
Implementation involves comprehensive checks to ensure no overlap between training tokens and test words. Create verification scripts to confirm the integrity of the splits. Validation includes statistical analysis of word distributions and characteristics across splits. Challenges include defining appropriate metrics to verify the splits serve the experiment's purpose of determining if training on spelling improves model performance on position and count question metrics.

Verification should be performed and documented in:
- `notebooks/split_verification.ipynb`

This notebook should analyze the splits stored in:
- `data/splits/train.json`
- `data/splits/val.json`
- `data/splits/test.json`

## 7. Source-Based Split Documentation [done]
### Dependencies: 3.2, 3.4
### Description: Document the source-based split approach and its importance for the experiment's validity.
### Details:
Create clear documentation explaining why the dataset uses a source-based split (training from tokenizer vocabulary, validation/test from external word lists) rather than a percentage-based split. Explain how this approach creates a true universal set and hold-out set, which is essential for validly measuring if training on spelling improves model performance on position and count question metrics.

Documentation should be created in:
- `docs/split_methodology.md` - Detailed explanation of the split methodology
- `docs/dataset.md` - General dataset documentation with references to the split methodology

This documentation should also be included in the dataset card when publishing to Hugging Face.

