# Task ID: 4
# Title: Training Data Formatting with Template Variations
# Status: pending
# Dependencies: 3
# Priority: medium
# Description: Format the training data using various template formats for spelling examples to maximize LLM generalization and token-awareness.
# Details:
1. Create a script to format the training data for fine-tuning
2. Implement the template variations specified in the PRD, including:
   - Simple variations (spelling first)
   - Narrative/playful versions (spelling first)
   - Educational/formal tone (spelling first)
   - Spoken word/emphatic style (spelling first)
   - Simple variations (word first)
   - Narrative/playful versions (word first)
   - Educational/formal tone (word first)
   - Spoken word/emphatic style (word first)
   - LLM-friendly structured training format (no "spell")
3. Include additional variations for token separation:
   - No separator between tokens
   - Arrows between tokens
   - Various punctuation and formatting
4. Create a Jupyter notebook to visualize example prompts and responses
5. Implement efficient DataLoader with proper batching

File Structure:
- Template definitions: `configs/templates/`
- Template categories: `configs/templates/categories.json`
- Token separation: `src/data/token_separator.py`
- Template processor: `src/data/template_processor.py`
- Example generator: `src/data/example_generator.py`
- Data loader: `src/data/data_loader.py`
- Formatted training data: `data/processed/training_data/`
- Template variations: `data/processed/template_variations/`
- Template analysis notebook: `notebooks/template_analysis.ipynb`
- Performance visualization: `notebooks/template_performance.ipynb`
- Template documentation: `docs/templates.md`
- Data format specification: `docs/data_format.md`

Implementation:
```python
def format_training_examples(dataset):
    formatted_examples = []
    
    # Template categories
    templates = {
        "spelling_first_simple": [
            "s t r a w — that spells '{word}.'\n",
            "The letters s, t, r, a, w spell the word '{word}.'\n",
            "s-t-r-a-w makes the word '{word}.'\n",
            "Put together, s t r a w spells {word}.\n",
            "When you combine s, t, r, a, and w, you get {word}.\n"
        ],
        "spelling_first_playful": [
            "Say it with me: s...t...r...a...w — {word}!\n",
            "Five little letters — s, t, r, a, w — team up to make '{word}.'\n",
            "You line up s, t, r, a, and w, and what do you get? {word}!\n",
            "It starts with an 's' and ends with a 'w' — that's '{word}.'\n",
            "One letter at a time: s, t, r, a, w. Together? {word}.\n"
        ],
        # Add all other template categories from the PRD
    }
    
    # Token separation styles
    separators = [
        "", # No separator
        " ", # Space
        ", ", # Comma and space
        "-", # Dash
        "...", # Triple dots
        " -> " # Arrow
    ]
    
    for example in dataset:
        word = example["word"]
        letters = list(word)
        
        # Randomly select template category and template
        category = random.choice(list(templates.keys()))
        template = random.choice(templates[category])
        
        # Randomly select separator
        separator = random.choice(separators)
        
        # Format the letters with the chosen separator
        spelled_letters = separator.join(letters)
        
        # Format the example using the template
        formatted_text = template.format(word=word, letters=spelled_letters)
        
        formatted_examples.append({
            "input": formatted_text,
            "output": word,
            "template_category": category,
            "separator": separator
        })
    
    return formatted_examples

# Create custom collation function for efficient batching
def custom_collate_fn(batch):
    input_ids = [item["input_ids"] for item in batch]
    attention_mask = [item["attention_mask"] for item in batch]

    # Pad sequences to the maximum length in the batch
    max_length = max(len(ids) for ids in input_ids)

    # Pad input_ids and attention_mask
    input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]
    attention_mask = [mask + [0] * (max_length - len(mask)) for mask in attention_mask]

    # Convert to tensors
    input_ids = torch.tensor(input_ids)
    attention_mask = torch.tensor(attention_mask)

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask
    }
```

# Test Strategy:
1. Verify script runs without errors
2. Confirm dataset contains all template variations specified in the PRD
3. Check that examples use a mix of punctuation and formatting
4. Ensure no template is over-represented
5. Create and review example notebook showing proper formatting
6. Test DataLoader with custom collation function
7. Verify efficient batching with varied text lengths
8. Validate that all files are created in the correct locations:
   - Check template files in `configs/templates/`
   - Verify processed data in `data/processed/training_data/`
   - Ensure analysis notebooks are functional
9. Test the complete pipeline from template processing to data loading

# Subtasks:
## 1. Template Design and Categorization [done]
### Dependencies: None
### Description: Create and categorize various template formats for training data based on different use cases and model requirements
### Details:
Develop a comprehensive template system that supports various data types (text, images, audio, video). Create templates for different ML tasks and ensure they follow best practices for data formatting. Categorize templates based on complexity, use case, and required model architecture. Quality metrics should include template coverage, flexibility, and adherence to formatting standards. Test by validating templates with sample data across different domains.
<info added on 2025-05-07T20:23:50.045Z>
Develop a comprehensive template system that supports various data types (text, images, audio, video). Create templates for different ML tasks and ensure they follow best practices for data formatting. Categorize templates based on complexity, use case, and required model architecture. Quality metrics should include template coverage, flexibility, and adherence to formatting standards. Test by validating templates with sample data across different domains.

The template design and categorization has been completed with the following structure:

1. Template Categories:
   - Spelling-first templates with variations: simple, playful, educational, and emphatic styles
   - Word-first templates with variations: simple, playful, educational, and emphatic styles
   - Structured templates: token-based and JSON-like formats

2. Documentation:
   - templates.md: Provides a comprehensive overview of the template system, categories, and usage guidelines
   - data_format.md: Contains detailed specifications for data formats and processing guidelines

3. Template Implementation Details:
   - Multiple formatting styles implemented for each category
   - Various token separation methods defined (to be implemented in next subtask)
   - Structured formats designed specifically for machine learning applications
   - Consistent variable substitution patterns established across all templates

4. Project Organization:
   - Template configurations stored in configs/templates/ directory
   - Categories defined in configs/templates/categories.json
   - Documentation placed in docs/ directory
   - Clear file structure established for implementation phase

The template system is now fully designed and categorized, providing a solid foundation for the token separation strategy implementation in the next subtask.
</info added on 2025-05-07T20:23:50.045Z>

## 2. Token Separation Strategy Implementation [done]
### Dependencies: 4.1
### Description: Develop and implement various token separation strategies for different data types and model requirements
### Details:
Research and implement multiple token separation approaches (whitespace, subword, character-level, etc.). Create a configurable system that allows switching between strategies based on language or data type. Develop custom tokenization rules for domain-specific data. Quality metrics should include tokenization accuracy, processing speed, and vocabulary coverage. Test with diverse multilingual datasets and measure impact on model performance. Implement in `src/data/token_separator.py`.
<info added on 2025-05-07T20:26:12.086Z>
Research and implement multiple token separation approaches (whitespace, subword, character-level, etc.). Create a configurable system that allows switching between strategies based on language or data type. Develop custom tokenization rules for domain-specific data. Quality metrics should include tokenization accuracy, processing speed, and vocabulary coverage. Test with diverse multilingual datasets and measure impact on model performance. Implement in `src/data/token_separator.py`.

The TokenSeparator class has been successfully implemented in src/data/token_separator.py with the following features:

1. Multiple built-in separator styles:
   - none: tokens without separators
   - space: tokens separated by spaces
   - comma: tokens separated by commas
   - dash: tokens separated by dashes
   - dots: tokens separated by dots
   - arrow: tokens separated by arrows

2. A flexible SeparatorConfig dataclass that provides configuration options:
   - Style selection from predefined styles
   - Support for custom separator strings
   - Control over spacing around separators
   - Token capitalization options

3. Utility functions to enhance usability:
   - get_all_separator_examples(): Generates examples using all available styles
   - create_custom(): Creates separators with custom configuration
   - get_random_separator(): Selects a random style for variety in outputs

4. A comprehensive test script (scripts/test_token_separator.py) that demonstrates:
   - All built-in separator styles in action
   - How to use custom separators
   - Random style selection functionality
   - Proper token processing workflow

5. Testing with sample tokens confirms:
   - All separator styles function as expected
   - Proper spacing and formatting is maintained
   - Custom separator functionality works correctly
   - Random style selection provides appropriate variation

The implementation is now ready for integration with the template processor in the next subtask (Dynamic Example Generation System).
</info added on 2025-05-07T20:26:12.086Z>

## 3. Dynamic Example Generation System [done]
### Dependencies: 4.1, 4.2
### Description: Build a system that can dynamically generate training examples with appropriate variations and augmentations
### Details:
Implement data augmentation techniques for different data types (text rotation, image transformation, etc.). Create a pipeline for generating variations of training examples to prevent overfitting. Develop rules for maintaining data balance across classes. Quality metrics should include variation diversity, generation speed, and class distribution balance. Test by measuring model performance improvements with augmented data versus baseline. Implement in `src/data/example_generator.py` and store outputs in `data/processed/template_variations/`.

## 4. Efficient Data Loading and Batching [done]
### Dependencies: 4.2, 4.3
### Description: Optimize data loading and batching processes for improved training efficiency
### Details:
Implement efficient data loading mechanisms that minimize memory usage and processing time. Develop smart batching strategies that group similar-length sequences together. Create data splitting functionality for training, validation, and testing sets. Quality metrics should include loading speed, memory efficiency, and training throughput. Test by benchmarking different loading approaches and measuring impact on training time. Implement in `src/data/data_loader.py`.

## 5. Template Variation Analysis and Visualization [pending]
### Dependencies: 4.1, 4.3, 4.4
### Description: Analyze and visualize the effectiveness of different template variations on model performance
### Details:
Develop tools to visualize how different template designs affect model training. Create metrics to quantify template effectiveness across different data types and tasks. Implement automated analysis to recommend optimal template configurations. Quality metrics should include visualization clarity, analysis accuracy, and recommendation relevance. Test by comparing model performance across different template variations and validating analysis results. Create analysis notebooks in `notebooks/template_analysis.ipynb` and `notebooks/template_performance.ipynb`.

## 6. Documentation and File Structure Setup [done]
### Dependencies: None
### Description: Create and organize the file structure and documentation for template variations and data formatting
### Details:
Set up the required directory structure for template files, implementation files, output files, and analysis files. Create comprehensive documentation in `docs/templates.md` and `docs/data_format.md` explaining the template system, data formats, and usage guidelines. Ensure all file paths are correctly referenced in the implementation code. Test by verifying that all directories exist and documentation is complete and accurate.

