# Task ID: 9
# Title: Comprehensive Model Evaluation
# Status: pending
# Dependencies: 8
# Priority: medium
# Description: Evaluate the fine-tuned models on the test set using multiple metrics and perform detailed analysis of transfer learning effectiveness from spelling training to position/count tasks, with specific focus on Qwen3-4B's non-thinking mode, English token subset, and sampling parameters, leveraging Lightning.AI Studios for efficient GPU-based evaluation.
# Details:
1. Evaluate the best model on the test set with dedicated pipelines for position/count tasks only
2. Implement comprehensive evaluation metrics for position/count tasks:
   - Letter Count Accuracy
   - Letter Position Accuracy
   - Character-Level Accuracy for position/count responses
   - Levenshtein Distance for position/count responses
   - Token-Level Perplexity for position/count tasks
   - Transfer Learning Effectiveness Metrics (from spelling training to position/count performance)
3. Evaluate Qwen3-4B in non-thinking mode only (enable_thinking=False)
4. Assess performance with the English-only token subset
5. Analyze the impact of Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)
6. Perform detailed error analysis for position and count tasks only
7. Investigate transfer learning patterns from spelling training to position/count performance
8. Analyze model's generalization capabilities on position/count tasks
9. Create visualizations of the position/count results
10. Compare position/count performance between base and fine-tuned models
11. Evaluate the effectiveness of the English token filtering approach for position/count tasks
12. Implement task-specific evaluation metrics and visualization tools for position/count tasks
13. Ensure proper handling of Qwen3's tokenizer patterns in position/count evaluation

NOTE: Use Lightning.AI Studios for all model evaluation, leveraging their GPU switching feature (CPU → T4 → A100) for cost-effective evaluation.

File Structure:
- Main evaluator: `src/evaluation/position_count_evaluator.py`
- Metrics calculator: `src/evaluation/metrics.py`
- Error analyzer: `src/evaluation/error_analysis.py`
- Transfer learning analyzer: `src/evaluation/transfer_analysis.py`
- Visualization utils: `src/evaluation/visualization.py`
- Qwen3 evaluator: `src/evaluation/qwen3_evaluator.py`
- English token analyzer: `src/evaluation/token_analysis.py`
- Test data: `data/splits/test.json`
- Challenge sets: `data/splits/challenge_sets/`
- Edge cases: `data/splits/edge_cases/`
- Error categories: `data/splits/error_categories/`
- Evaluation results: `results/evaluation/`
- Performance metrics: `results/evaluation/metrics/`
- Error analysis: `results/evaluation/error_analysis/`
- Transfer learning analysis: `results/evaluation/transfer_analysis/`
- Token subset analysis: `results/evaluation/token_analysis/`
- Visualizations: `results/evaluation/plots/`

Implementation:
```python
import torch
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import Levenshtein
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import wandb
from scipy.stats import pearsonr, spearmanr

# Load models for comparison
def load_models(base_model_name, finetuned_model_path):
    # Load base model
    base_model = AutoModelForCausalLM.from_pretrained(base_model_name)
    base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    base_tokenizer.pad_token = base_tokenizer.eos_token
    
    # Load fine-tuned model
    finetuned_model = AutoModelForCausalLM.from_pretrained(finetuned_model_path)
    finetuned_tokenizer = AutoTokenizer.from_pretrained(base_model_name)  # Use same tokenizer
    finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token
    
    return {
        "base": (base_model, base_tokenizer),
        "finetuned": (finetuned_model, finetuned_tokenizer)
    }

# Generate answer from model with support for Qwen3 in non-thinking mode
def generate_answer(model, tokenizer, question, max_length=10, 
                   temperature=0.6, top_p=0.95, top_k=20, min_p=0, english_only=False):
    inputs = tokenizer(question, return_tensors="pt")
    
    # Configure generation parameters based on Qwen3 specifications
    generation_config = {
        "max_length": len(inputs.input_ids[0]) + max_length,
        "pad_token_id": tokenizer.eos_token_id,
        "do_sample": temperature > 0,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
    }
    
    # Add min_p if supported by the model
    if hasattr(model.config, "min_p"):
        generation_config["min_p"] = min_p
    
    # For Qwen3, explicitly disable thinking mode
    if "qwen" in tokenizer.name_or_path.lower():
        generation_config["thinking"] = False
    
    # Apply English-only token filtering if requested
    if english_only:
        # Implementation would depend on the specific model and tokenizer
        # This is a placeholder for the actual implementation
        pass
    
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            **generation_config
        )
    
    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
    
    # Extract just the first token/character for letter position or first number for letter count
    if "How many" in question:
        # Extract first number
        import re
        numbers = re.findall(r'\d+', response)
        return numbers[0] if numbers else response.strip()
    elif "What is the letter" in question:
        # Extract first character
        return response.strip()[0] if response.strip() else ""
    else:
        # For other tasks, return the full response
        return response.strip()

# Filter to English-only tokens for Qwen3
def filter_english_tokens(tokenizer, input_ids):
    # This is a simplified example - actual implementation would depend on Qwen3's token structure
    english_token_ids = [id for id in range(tokenizer.vocab_size) if is_english_token(tokenizer, id)]
    english_token_set = set(english_token_ids)
    
    # Filter logits to only allow English tokens
    def filter_fn(input_ids, scores):
        for batch_idx in range(scores.shape[0]):
            for token_idx in range(scores.shape[1]):
                if token_idx not in english_token_set:
                    scores[batch_idx, token_idx] = -float('inf')
        return scores
    
    return filter_fn

# Helper function to determine if a token is English
def is_english_token(tokenizer, token_id):
    token = tokenizer.convert_ids_to_tokens(token_id)
    # Simple heuristic - can be improved with more sophisticated analysis
    return all(c.isascii() and (c.isalnum() or c.isspace() or c in string.punctuation) for c in token)

# Calculate letter count accuracy
def calc_letter_count_accuracy(model, tokenizer, test_dataset, english_only=False):
    correct = 0
    total = 0
    results = []
    
    for item in test_dataset:
        if item["question_type"] != "letter_count":
            continue
            
        prediction = generate_answer(model, tokenizer, item["question"], english_only=english_only)
        is_correct = prediction == item["answer"]
        
        results.append({
            "question": item["question"],
            "expected": item["answer"],
            "prediction": prediction,
            "correct": is_correct,
            "word": item["word"],
            "english_only": english_only
        })
        
        correct += int(is_correct)
        total += 1
    
    accuracy = correct / total if total > 0 else 0
    print(f"Letter Count Accuracy (English-only: {english_only}): {accuracy:.4f} ({correct}/{total})")
    
    return accuracy, results

# Calculate letter position accuracy
def calc_letter_position_accuracy(model, tokenizer, test_dataset, english_only=False):
    correct = 0
    total = 0
    results = []
    
    for item in test_dataset:
        if item["question_type"] != "letter_position":
            continue
            
        prediction = generate_answer(model, tokenizer, item["question"], english_only=english_only)
        is_correct = prediction.lower() == item["answer"].lower()
        
        results.append({
            "question": item["question"],
            "expected": item["answer"],
            "prediction": prediction,
            "correct": is_correct,
            "word": item["word"],
            "english_only": english_only
        })
        
        correct += int(is_correct)
        total += 1
    
    accuracy = correct / total if total > 0 else 0
    print(f"Letter Position Accuracy (English-only: {english_only}): {accuracy:.4f} ({correct}/{total})")
    
    return accuracy, results

# Calculate character-level accuracy for position/count tasks
def calc_character_level_accuracy(model, tokenizer, test_dataset, english_only=False):
    total_char_accuracy = 0
    total_samples = 0
    results = []

    for item in test_dataset:
        if item["question_type"] not in ["letter_position", "letter_count"]:
            continue

        prediction = generate_answer(model, tokenizer, item["question"], english_only=english_only)
        pred_chars = prediction.strip().lower().replace(" ", "")
        true_chars = item["answer"].lower()

        # Calculate character-by-character accuracy
        correct_chars = 0
        for i, char in enumerate(true_chars):
            if i < len(pred_chars) and pred_chars[i] == char:
                correct_chars += 1

        char_accuracy = correct_chars / len(true_chars) if len(true_chars) > 0 else 0
        
        results.append({
            "question": item["question"],
            "expected": item["answer"],
            "prediction": prediction,
            "char_accuracy": char_accuracy,
            "word": item["word"],
            "question_type": item["question_type"],
            "english_only": english_only
        })
        
        total_char_accuracy += char_accuracy
        total_samples += 1

    avg_char_accuracy = total_char_accuracy / total_samples if total_samples > 0 else 0
    print(f"Character-Level Accuracy (English-only: {english_only}): {avg_char_accuracy:.4f}")
    
    return avg_char_accuracy, results

# Calculate Levenshtein distance for position/count tasks
def calc_levenshtein_metrics(model, tokenizer, test_dataset, english_only=False):
    total_distances = 0
    total_samples = 0
    results = []

    for item in test_dataset:
        if item["question_type"] not in ["letter_position", "letter_count"]:
            continue

        prediction = generate_answer(model, tokenizer, item["question"], english_only=english_only)
        pred_text = prediction.strip().lower()
        true_text = item["answer"].lower()

        # Calculate Levenshtein distance
        distance = Levenshtein.distance(pred_text, true_text)
        normalized_distance = distance / max(len(pred_text), len(true_text)) if max(len(pred_text), len(true_text)) > 0 else 0

        results.append({
            "question": item["question"],
            "expected": true_text,
            "prediction": pred_text,
            "levenshtein_distance": distance,
            "normalized_distance": normalized_distance,
            "word": item["word"],
            "question_type": item["question_type"],
            "english_only": english_only
        })
        
        total_distances += normalized_distance
        total_samples += 1

    avg_distance = total_distances / total_samples if total_samples > 0 else 0
    print(f"Average Normalized Levenshtein Distance (English-only: {english_only}): {avg_distance:.4f}")
    
    return avg_distance, results

# Analyze transfer learning effectiveness from spelling training to position/count tasks
def analyze_transfer_learning(model_info, position_count_results):
    # Extract model training info to understand spelling training impact
    training_words = model_info.get("training_words", [])
    training_metrics = model_info.get("training_metrics", {})
    
    # Map position/count performance to corresponding training data
    transfer_data = []
    for result in position_count_results:
        word = result["word"]
        # Check if this word was in the training data
        was_trained = word in training_words
        
        transfer_data.append({
            "word": word,
            "was_trained": was_trained,
            "task_performance": 1 if result["correct"] else 0,
            "question_type": result["question_type"]
        })
    
    # Calculate performance difference between trained and untrained words
    trained_words_results = [item for item in transfer_data if item["was_trained"]]
    untrained_words_results = [item for item in transfer_data if not item["was_trained"]]
    
    trained_performance = sum([item["task_performance"] for item in trained_words_results]) / len(trained_words_results) if trained_words_results else 0
    untrained_performance = sum([item["task_performance"] for item in untrained_words_results]) / len(untrained_words_results) if untrained_words_results else 0
    
    transfer_effect = trained_performance - untrained_performance
    
    # Separate by question type
    position_data = [item for item in transfer_data if item["question_type"] == "letter_position"]
    count_data = [item for item in transfer_data if item["question_type"] == "letter_count"]
    
    # Calculate type-specific transfer effects
    position_trained = [item for item in position_data if item["was_trained"]]
    position_untrained = [item for item in position_data if not item["was_trained"]]
    
    count_trained = [item for item in count_data if item["was_trained"]]
    count_untrained = [item for item in count_data if not item["was_trained"]]
    
    position_trained_perf = sum([item["task_performance"] for item in position_trained]) / len(position_trained) if position_trained else 0
    position_untrained_perf = sum([item["task_performance"] for item in position_untrained]) / len(position_untrained) if position_untrained else 0
    position_transfer_effect = position_trained_perf - position_untrained_perf
    
    count_trained_perf = sum([item["task_performance"] for item in count_trained]) / len(count_trained) if count_trained else 0
    count_untrained_perf = sum([item["task_performance"] for item in count_untrained]) / len(count_untrained) if count_untrained else 0
    count_transfer_effect = count_trained_perf - count_untrained_perf
    
    # Identify successful and unsuccessful transfer cases
    successful_transfer = [item for item in transfer_data if item["was_trained"] and item["task_performance"] == 1]
    unsuccessful_transfer = [item for item in transfer_data if item["was_trained"] and item["task_performance"] == 0]
    
    # Group words by patterns
    pattern_performance = {}
    for item in transfer_data:
        word = item["word"]
        # Identify patterns (this is a simplified example - expand as needed)
        patterns = []
        if 'ie' in word or 'ei' in word:
            patterns.append('ie_ei_rule')
        if word.endswith('e') and any(word.endswith(f'{c}e') for c in 'aeiou'):
            patterns.append('silent_e')
        if any(c*2 in word for c in 'abcdefghijklmnopqrstuvwxyz'):
            patterns.append('double_letter')
        
        for pattern in patterns:
            if pattern not in pattern_performance:
                pattern_performance[pattern] = {
                    "position": [],
                    "count": []
                }
            
            if item["question_type"] == "letter_position":
                pattern_performance[pattern]["position"].append(item["task_performance"])
            elif item["question_type"] == "letter_count":
                pattern_performance[pattern]["count"].append(item["task_performance"])
    
    # Calculate average performance by pattern
    for pattern in pattern_performance:                
        if pattern_performance[pattern]["position"]:
            pattern_performance[pattern]["avg_position"] = sum(pattern_performance[pattern]["position"]) / len(pattern_performance[pattern]["position"])
        else:
            pattern_performance[pattern]["avg_position"] = 0
            
        if pattern_performance[pattern]["count"]:
            pattern_performance[pattern]["avg_count"] = sum(pattern_performance[pattern]["count"]) / len(pattern_performance[pattern]["count"])
        else:
            pattern_performance[pattern]["avg_count"] = 0
    
    transfer_analysis = {
        "overall_transfer_effect": transfer_effect,
        "position_transfer_effect": position_transfer_effect,
        "count_transfer_effect": count_transfer_effect,
        "successful_transfer": {
            "count": len(successful_transfer),
            "examples": successful_transfer[:10]  # Limit to 10 examples
        },
        "unsuccessful_transfer": {
            "count": len(unsuccessful_transfer),
            "examples": unsuccessful_transfer[:10]  # Limit to 10 examples
        },
        "pattern_performance": pattern_performance
    }
    
    return transfer_analysis, transfer_data

# Perform error analysis for position/count tasks
def perform_error_analysis(results_dict):
    error_analysis = {}
    
    # Analyze letter count errors
    count_errors = [r for r in results_dict["letter_count"] if not r["correct"]]
    
    # Categorize errors
    error_types = {
        "off_by_one": 0,
        "completely_wrong": 0,
        "no_number": 0,
        "other": 0
    }
    
    for error in count_errors:
        try:
            pred = int(error["prediction"])
            true = int(error["expected"])
            
            if abs(pred - true) == 1:
                error_types["off_by_one"] += 1
            else:
                error_types["completely_wrong"] += 1
        except ValueError:
            if not error["prediction"].strip():
                error_types["no_number"] += 1
            else:
                error_types["other"] += 1
    
    # Analyze letter position errors
    position_errors = [r for r in results_dict["letter_position"] if not r["correct"]]
    
    # Categorize position errors
    position_error_types = {
        "adjacent_letter": 0,
        "wrong_case": 0,
        "no_response": 0,
        "other": 0
    }
    
    for error in position_errors:
        if not error["prediction"].strip():
            position_error_types["no_response"] += 1
        elif error["prediction"].lower() == error["expected"].lower():
            position_error_types["wrong_case"] += 1
        elif error["word"] and error["prediction"] in error["word"]:
            position_error_types["adjacent_letter"] += 1
        else:
            position_error_types["other"] += 1
    
    # Analyze by word length
    word_length_performance = {}
    
    for result in results_dict["letter_count"] + results_dict["letter_position"]:
        word = result["word"]
        length = len(word)
        task_type = "position" if result in results_dict["letter_position"] else "count"
        
        if length not in word_length_performance:
            word_length_performance[length] = {
                "position": {"correct": 0, "total": 0},
                "count": {"correct": 0, "total": 0}
            }
        
        word_length_performance[length][task_type]["total"] += 1
        if result["correct"]:
            word_length_performance[length][task_type]["correct"] += 1
    
    # Calculate accuracy by word length
    for length, stats in word_length_performance.items():
        for task_type in ["position", "count"]:
            stats[task_type]["accuracy"] = stats[task_type]["correct"] / stats[task_type]["total"] if stats[task_type]["total"] > 0 else 0
    
    error_analysis = {
        "letter_count_errors": error_types,
        "letter_position_errors": position_error_types,
        "word_length_performance": word_length_performance
    }
    
    return error_analysis

# Create performance dashboard for position/count tasks
def create_performance_dashboard(results, error_analysis, transfer_analysis):
    # Set up figure
    fig = plt.figure(figsize=(20, 16))
    
    # 1. Accuracy comparison across metrics
    ax1 = fig.add_subplot(3, 3, 1)
    metrics = ['letter_count_accuracy', 'letter_position_accuracy', 'character_level_accuracy']
    
    # Extract values
    values = [results.get(m, 0) for m in metrics]
    
    ax1.bar(range(len(metrics)), values)
    ax1.set_ylabel('Accuracy')
    ax1.set_title('Position/Count Accuracy Comparison')
    ax1.set_xticks(range(len(metrics)))
    ax1.set_xticklabels([m.replace('_', ' ').title() for m in metrics])
    
    # 2. Error analysis for letter count
    ax2 = fig.add_subplot(3, 3, 2)
    error_types = list(error_analysis["letter_count_errors"].keys())
    error_counts = [error_analysis["letter_count_errors"][t] for t in error_types]
    
    ax2.bar(error_types, error_counts)
    ax2.set_title('Letter Count Error Types')
    ax2.set_ylabel('Count')
    plt.setp(ax2.get_xticklabels(), rotation=45, ha="right")
    
    # 3. Error analysis for letter position
    ax3 = fig.add_subplot(3, 3, 3)
    pos_error_types = list(error_analysis["letter_position_errors"].keys())
    pos_error_counts = [error_analysis["letter_position_errors"][t] for t in pos_error_types]
    
    ax3.bar(pos_error_types, pos_error_counts)
    ax3.set_title('Letter Position Error Types')
    ax3.set_ylabel('Count')
    plt.setp(ax3.get_xticklabels(), rotation=45, ha="right")
    
    # 4. Performance by word length
    ax4 = fig.add_subplot(3, 3, 4)
    length_perf = error_analysis["word_length_performance"]
    
    # Get all word lengths
    all_lengths = sorted(length_perf.keys())
    
    # Get position accuracy by word length
    pos_acc = [length_perf.get(l, {}).get("position", {}).get("accuracy", 0) for l in all_lengths]
    
    ax4.plot(all_lengths, pos_acc, marker='o', label='Position')
    
    # Get count accuracy by word length
    count_acc = [length_perf.get(l, {}).get("count", {}).get("accuracy", 0) for l in all_lengths]
    
    ax4.plot(all_lengths, count_acc, marker='^', linestyle='--', label='Count')
    
    ax4.set_title('Performance by Word Length')
    ax4.set_xlabel('Word Length')
    ax4.set_ylabel('Accuracy')
    ax4.legend()
    
    # 5. Transfer learning effect
    ax5 = fig.add_subplot(3, 3, 5)
    
    # Extract transfer effects
    transfer_effects = [
        transfer_analysis.get("overall_transfer_effect", 0),
        transfer_analysis.get("position_transfer_effect", 0),
        transfer_analysis.get("count_transfer_effect", 0)
    ]
    
    ax5.bar(["Overall", "Position", "Count"], transfer_effects)
    ax5.set_title('Transfer Effect from Spelling Training')
    ax5.set_ylabel('Performance Difference (Trained - Untrained)')
    
    # 6. Pattern performance
    ax6 = fig.add_subplot(3, 3, 6)
    
    # Get patterns
    patterns = transfer_analysis.get("pattern_performance", {})
    
    all_patterns = list(patterns.keys())
    
    if all_patterns:
        # Get position performance for each pattern
        pattern_pos = [patterns.get(p, {}).get("avg_position", 0) for p in all_patterns]
        
        # Get count performance for each pattern
        pattern_count = [patterns.get(p, {}).get("avg_count", 0) for p in all_patterns]
        
        # Set up grouped bar chart
        x = np.arange(len(all_patterns))
        width = 0.35
        
        ax6.bar(x - width/2, pattern_pos, width, label='Position')
        ax6.bar(x + width/2, pattern_count, width, label='Count')
        
        ax6.set_title('Pattern Performance')
        ax6.set_xticks(x)
        ax6.set_xticklabels(all_patterns)
        ax6.set_ylabel('Accuracy')
        ax6.legend()
        plt.setp(ax6.get_xticklabels(), rotation=45, ha="right")
    else:
        ax6.text(0.5, 0.5, 'No pattern data available', 
                horizontalalignment='center', verticalalignment='center')
    
    # 7. Levenshtein distance
    ax7 = fig.add_subplot(3, 3, 7)
    lev = results.get('levenshtein_distance', 0)
    
    ax7.bar(['Normalized Levenshtein Distance'], [lev])
    ax7.set_title('Normalized Levenshtein Distance')
    ax7.set_ylabel('Distance (lower is better)')
    
    # 8. Transfer success/failure counts
    ax8 = fig.add_subplot(3, 3, 8)
    
    success = transfer_analysis.get("successful_transfer", {}).get("count", 0)
    failure = transfer_analysis.get("unsuccessful_transfer", {}).get("count", 0)
    
    ax8.bar(['Successful Transfer', 'Unsuccessful Transfer'], [success, failure])
    ax8.set_title('Transfer Learning Success/Failure')
    ax8.set_ylabel('Count')
    
    # 9. English-only vs All tokens comparison
    if 'english_only' in results and 'all_tokens' in results:
        ax9 = fig.add_subplot(3, 3, 9)
        
        metrics = ['letter_count_accuracy', 'letter_position_accuracy']
        english_values = [results['english_only'].get(m, 0) for m in metrics]
        all_values = [results['all_tokens'].get(m, 0) for m in metrics]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        ax9.bar(x - width/2, english_values, width, label='English-only')
        ax9.bar(x + width/2, all_values, width, label='All tokens')
        
        ax9.set_title('Token Filtering Comparison')
        ax9.set_xticks(x)
        ax9.set_xticklabels([m.replace('_', ' ').title() for m in metrics])
        ax9.set_ylabel('Accuracy')
        ax9.legend()
    
    plt.tight_layout()
    plt.savefig('results/evaluation/plots/qwen3_position_count_dashboard.png', dpi=300)
    
    return fig

# Analyze English token subset effectiveness for position/count tasks
def analyze_english_token_subset(english_results, all_results):
    token_analysis = {}
    
    # Compare metrics between English-only and all tokens
    metrics = ['letter_count_accuracy', 'letter_position_accuracy', 'character_level_accuracy', 'levenshtein_distance']
    
    for metric in metrics:
        english_value = english_results.get(metric, 0)
        all_value = all_results.get(metric, 0)
        
        # For Levenshtein, lower is better
        if metric == 'levenshtein_distance':
            improvement = all_value - english_value
        else:
            improvement = english_value - all_value
            
        token_analysis[metric] = {
            'english_only': english_value,
            'all_tokens': all_value,
            'improvement': improvement,
            'percent_change': (improvement / all_value * 100) if all_value != 0 else 0
        }
    
    # Analyze which task types benefit most from English-only tokens
    task_benefit = {
        'letter_count': english_results.get('letter_count_accuracy', 0) - all_results.get('letter_count_accuracy', 0),
        'letter_position': english_results.get('letter_position_accuracy', 0) - all_results.get('letter_position_accuracy', 0)
    }
    
    # Determine which task benefits most
    max_benefit_task = max(task_benefit.items(), key=lambda x: x[1])[0]
    task_benefit['most_improved_task'] = max_benefit_task
    
    token_analysis['task_benefit'] = task_benefit
    
    return token_analysis

# Main evaluation function
def evaluate_models(model_path, model_info=None):
    # Initialize W&B
    wandb.init(project="llm-spelling-finetuning", name="qwen3_position_count_evaluation")
    
    # Load test dataset
    test_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset", split="test")
    # Alternative: load from local file
    # with open('data/splits/test.json', 'r') as f:
    #     test_dataset = json.load(f)
    
    # Filter test dataset to only include position and count tasks
    test_dataset = [item for item in test_dataset if item["question_type"] in ["letter_position", "letter_count"]]
    
    # Load model and tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # Evaluate with different token settings
    results = {}
    detailed_results = {}
    
    # Token settings to evaluate
    configs = [
        {"english_only": False},
        {"english_only": True}
    ]
    
    for config in configs:
        english_only = config["english_only"]
        
        token_key = "english_only" if english_only else "all_tokens"
        
        print(f"\nEvaluating with english_only={english_only}...")
        
        # Run evaluations with current configuration
        letter_count_acc, count_results = calc_letter_count_accuracy(model, tokenizer, test_dataset, english_only)
        letter_position_acc, position_results = calc_letter_position_accuracy(model, tokenizer, test_dataset, english_only)
        char_acc, char_results = calc_character_level_accuracy(model, tokenizer, test_dataset, english_only)
        lev_dist, lev_results = calc_levenshtein_metrics(model, tokenizer, test_dataset, english_only)
        
        # Store results
        results[token_key] = {
            "letter_count_accuracy": letter_count_acc,
            "letter_position_accuracy": letter_position_acc,
            "character_level_accuracy": char_acc,
            "levenshtein_distance": lev_dist
        }
        
        detailed_results[token_key] = {
            "letter_count": count_results,
            "letter_position": position_results,
            "character_level": char_results,
            "levenshtein": lev_results
        }
    
    # Use all_tokens results for main analysis
    main_results = results["all_tokens"]
    main_detailed_results = detailed_results["all_tokens"]
    
    # Perform error analysis
    error_analysis = perform_error_analysis(main_detailed_results)
    
    # Analyze transfer learning effectiveness
    transfer_result, transfer_data = analyze_transfer_learning(
        model_info or {},
        main_detailed_results["letter_count"] + main_detailed_results["letter_position"]
    )
    
    # Analyze English token subset effectiveness
    token_analysis = analyze_english_token_subset(results["english_only"], results["all_tokens"])
    
    # Create performance dashboard
    dashboard = create_performance_dashboard(main_results, error_analysis, transfer_result)
    
    # Log results to W&B
    wandb.log({
        "results": results,
        "error_analysis": error_analysis,
        "transfer_learning": transfer_result,
        "token_analysis": token_analysis,
        "evaluation_dashboard": wandb.Image(dashboard)
    })
    
    # Save results locally
    evaluation_results = {
        "results": results,
        "detailed_results": detailed_results,
        "error_analysis": error_analysis,
        "transfer_learning": transfer_result,
        "token_analysis": token_analysis
    }
    
    # Ensure directories exist
    import os
    os.makedirs('results/evaluation/metrics', exist_ok=True)
    os.makedirs('results/evaluation/error_analysis', exist_ok=True)
    os.makedirs('results/evaluation/transfer_analysis', exist_ok=True)
    os.makedirs('results/evaluation/token_analysis', exist_ok=True)
    os.makedirs('results/evaluation/plots', exist_ok=True)
    
    # Save results to appropriate locations
    with open("results/evaluation/metrics/qwen3_position_count_evaluation.json", "w") as f:
        json.dump(evaluation_results, f, indent=2)
    
    with open("results/evaluation/error_analysis/qwen3_position_count_error_patterns.json", "w") as f:
        json.dump(error_analysis, f, indent=2)
        
    with open("results/evaluation/transfer_analysis/qwen3_transfer_learning_analysis.json", "w") as f:
        json.dump(transfer_result, f, indent=2)
        
    with open("results/evaluation/token_analysis/qwen3_token_analysis.json", "w") as f:
        json.dump(token_analysis, f, indent=2)
    
    # Close W&B
    wandb.finish()
    
    return evaluation_results
```

# Test Strategy:
1. Verify all evaluation metrics are calculated correctly for position/count tasks only
2. Test Qwen3-4B in non-thinking mode only to ensure proper configuration
3. Validate English-only token subset filtering works correctly for position/count tasks
4. Test with Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)
5. Confirm error analysis provides meaningful insights for position/count task types
6. Validate transfer learning analysis metrics for how spelling training affects position/count performance
7. Check that visualizations clearly show position/count performance metrics
8. Verify results are properly logged to W&B with appropriate metrics
9. Confirm performance comparison between different token filtering approaches for position/count tasks
10. Test with different model checkpoints to ensure consistent evaluation
11. Verify final evaluation results are saved locally in the correct directories:
   - `results/evaluation/metrics/`
   - `results/evaluation/error_analysis/`
   - `results/evaluation/transfer_analysis/`
   - `results/evaluation/token_analysis/`
   - `results/evaluation/plots/`
12. Test GPU switching functionality in Lightning.AI Studio (CPU → T4 → A100) for optimal resource usage
13. Verify that all test datasets are properly filtered to include only position/count tasks
14. Check that the evaluation notebooks can successfully load and analyze the position/count results
15. Validate the pattern analysis to ensure it correctly identifies which spelling patterns in training lead to better position/count performance
16. Test Lightning.AI Studio's job system for automated evaluation pipeline execution
17. Verify proper environment isolation and dependency management in the Lightning.AI Studio
18. Test the token analysis to ensure it correctly measures the effectiveness of the English token filtering approach for position/count tasks
19. Verify that thinking mode is never enabled in any part of the evaluation process

# Subtasks:
## 1. Multi-metric evaluation framework implementation [pending]
### Dependencies: None
### Description: Develop and implement a comprehensive evaluation framework with multiple metrics to assess model performance
### Details:
Define evaluation goals and success metrics for the model assessment. Select appropriate metrics covering accuracy, precision, recall, F1-score, latency, and domain-specific measures. Create standardized test datasets with diverse examples. Implement automated evaluation pipelines that can process model outputs against ground truth. Establish baseline performance thresholds for each metric.

NOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.

## 2. Base vs. fine-tuned model comparison [pending]
### Dependencies: 9.1
### Description: Conduct systematic comparison between base models and their fine-tuned versions across all defined metrics
### Details:
Design controlled experiments to compare base and fine-tuned models. Ensure identical test conditions and datasets for fair comparison. Measure performance improvements across all metrics defined in subtask 1. Analyze trade-offs between different aspects of performance (e.g., accuracy vs. latency). Document specific improvements attributable to fine-tuning techniques. Identify areas where fine-tuning provided the most significant gains.

NOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.

## 3. Detailed error analysis system [pending]
### Dependencies: 9.1, 9.2
### Description: Create a system to categorize, analyze and track different types of model errors
### Details:
Develop error taxonomy specific to the model's domain and tasks. Implement automated error classification system. Perform qualitative analysis of error patterns and edge cases. Create error frequency distribution reports. Identify correlations between specific input characteristics and error types. Develop recommendations for targeted model improvements based on error patterns.

NOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.

## 4. Performance visualization dashboard [pending]
### Dependencies: 9.1, 9.2, 9.3
### Description: Design and implement an interactive dashboard to visualize model performance metrics and comparisons
### Details:
Select appropriate visualization types for different metrics and comparisons. Implement interactive features allowing drill-down into specific performance aspects. Create side-by-side visualizations of base vs. fine-tuned model performance. Design time-series views to track performance changes across model iterations. Ensure visualizations are accessible and interpretable for both technical and non-technical stakeholders.

NOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.

## 5. Evaluation report generation [pending]
### Dependencies: 9.1, 9.2, 9.3, 9.4
### Description: Create comprehensive evaluation reports documenting findings, methodologies, and recommendations
### Details:
Develop standardized report templates covering all evaluation aspects. Document evaluation methodology, metrics, and test datasets. Summarize key performance findings and improvements. Include detailed error analysis with examples. Provide actionable recommendations for further model improvements. Create executive summary for non-technical stakeholders. Ensure reports include all relevant visualizations from the dashboard.

NOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.

## 6. Lightning.AI Studio setup for model evaluation [pending]
### Dependencies: 9.1
### Description: Configure and set up a dedicated Lightning.AI Studio for model evaluation with GPU switching capabilities
### Details:
Create a dedicated evaluation Studio following the "one Studio, one task" principle. Configure the Studio with necessary dependencies for model evaluation. Set up GPU switching capabilities (CPU → T4 → A100) for cost-effective resource usage. Install and configure Lightning.AI plugins for experiment tracking and visualization. Set up shared filesystem access for models and datasets. Implement proper environment isolation and dependency management. Create documentation for the Lightning.AI Studio setup and usage. Test the setup with sample models to verify functionality.

## 7. File structure implementation [pending]
### Dependencies: None
### Description: Set up the file structure for evaluation components and results according to the project organization
### Details:
Create the following directory structure:
- `src/evaluation/` for evaluation code modules
- `data/splits/` for test datasets and challenge sets
- `results/evaluation/` for storing evaluation outputs
- `notebooks/` for analysis notebooks
- `docs/` for evaluation documentation

Ensure all evaluation code properly uses these paths for loading data and saving results. Update existing code to use the standardized file paths. Create placeholder files and documentation templates as needed.

## 8. Analysis notebooks development [pending]
### Dependencies: 9.1, 9.2, 9.3, 9.4, 9.5
### Description: Create Jupyter notebooks for analyzing evaluation results and visualizing model performance
### Details:
Develop the following notebooks:
- `notebooks/evaluation_analysis.ipynb`: General analysis of evaluation results
- `notebooks/error_patterns.ipynb`: Detailed analysis of error patterns and categories
- `notebooks/model_comparison.ipynb`: Comparative analysis between base and fine-tuned models

Ensure notebooks can load results from the standardized file locations. Implement interactive visualizations and filtering capabilities. Add markdown documentation explaining the analysis methodology and interpretation of results.

## 9. Transfer learning analysis implementation [pending]
### Dependencies: 9.1, 9.3
### Description: Develop and implement analysis tools to measure transfer learning effectiveness from spelling training to position/count tasks
### Details:
Create a transfer learning analysis module in `src/evaluation/transfer_analysis.py`. Implement metrics to measure how spelling training affects position/count task performance. Develop methods to identify which spelling patterns in training lead to better position/count performance. Create visualizations showing the relationship between spelling training and position/count task success. Implement statistical tests to validate transfer learning effectiveness. Design analysis tools to identify successful and unsuccessful transfer cases.

Ensure results are saved to `results/evaluation/transfer_analysis/` directory and properly visualized in the dashboard.

## 10. Position/count task evaluation pipeline [pending]
### Dependencies: 9.1
### Description: Implement dedicated evaluation pipeline for position/count tasks only
### Details:
Refactor the evaluation framework to focus exclusively on position/count tasks. Ensure metrics are calculated appropriately for position and count tasks. Implement task-specific error analysis for position/count tasks. Create mechanisms to track performance on different position/count question types. Design the system to identify how spelling training transfers to position/count tasks. Ensure all results can be combined for comprehensive reporting and visualization.

Implement the main evaluator in `src/evaluation/position_count_evaluator.py` to handle position/count evaluation.

## 11. Lightning.AI automated evaluation pipeline [pending]
### Dependencies: 9.6, 9.10
### Description: Set up automated evaluation pipelines using Lightning.AI's job system
### Details:
Configure Lightning.AI's job system for automated model evaluation. Create job templates for different evaluation scenarios. Set up job dependencies to ensure proper execution order. Implement resource optimization to use appropriate GPU tiers for different evaluation stages. Configure job notifications and alerts. Create a dashboard to monitor job status and results. Document the job system setup and usage. Test the automated pipeline with sample models.

## 12. Cost optimization for Lightning.AI resources [pending]
### Dependencies: 9.6
### Description: Implement cost optimization strategies for efficient resource usage in Lightning.AI
### Details:
Analyze resource requirements for different evaluation stages. Implement automatic GPU switching based on computational needs. Set up resource monitoring and usage alerts. Configure automatic shutdown of idle resources. Implement batch processing for efficient resource utilization. Create cost estimation tools for evaluation runs. Document cost optimization strategies and best practices. Test and validate cost savings through optimized resource usage.

## 13. Lightning.AI evaluation documentation [pending]
### Dependencies: 9.6, 9.11, 9.12
### Description: Create comprehensive documentation for the Lightning.AI evaluation setup and usage
### Details:
Document the Lightning.AI Studio setup and configuration. Create user guides for running evaluations in the Studio. Document GPU switching functionality and best practices. Create troubleshooting guides for common issues. Document cost optimization strategies and resource management. Create onboarding materials for new team members. Include examples and tutorials for different evaluation scenarios. Maintain documentation with updates for new Lightning.AI features.

## 14. Qwen3 non-thinking mode evaluation [pending]
### Dependencies: 9.1, 9.10
### Description: Implement evaluation pipeline for Qwen3-4B in non-thinking mode
### Details:
Extend the evaluation framework to support Qwen3-4B in non-thinking mode for position/count tasks. Create evaluation configurations with enable_thinking=False. Implement proper configuration in the generation pipeline. Develop metrics to assess position/count performance. Create visualizations showing performance across different position/count tasks. Analyze transfer learning capabilities from spelling training to position/count tasks. Identify position/count tasks that benefit most from the model. Document findings and recommendations for optimal usage based on task type.

## 15. English token subset analysis [pending]
### Dependencies: 9.1, 9.14
### Description: Implement analysis tools to evaluate position/count performance with English-only token subset
### Details:
Develop token filtering mechanism for Qwen3-4B to restrict generation to English-only tokens. Create analysis tools to compare position/count performance between full vocabulary and English-only subset. Implement metrics to measure the effectiveness of token filtering for position/count tasks. Analyze which position/count tasks benefit most from English token filtering. Create visualizations showing the impact of token filtering on different position/count metrics. Document findings and recommendations for token filtering usage. Ensure proper handling of Qwen3's tokenizer patterns in the filtering process.

## 16. Qwen3 sampling parameters analysis [pending]
### Dependencies: 9.1, 9.14
### Description: Analyze the impact of Qwen3's specific sampling parameters on position/count performance
### Details:
Implement evaluation with Qwen3's recommended sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) for position/count tasks. Create comparison framework to test different parameter combinations for position/count tasks. Analyze how sampling parameters affect performance on position and count task types. Identify optimal parameter settings for position and count tasks. Create visualizations showing parameter sensitivity for position/count tasks. Document findings and recommendations for parameter tuning. Ensure proper implementation of MinP parameter if supported by the model.

## 17. Performance comparison dashboard [pending]
### Dependencies: 9.4, 9.14
### Description: Create interactive dashboard for comparing position/count performance across different configurations
### Details:
Design and implement a dedicated dashboard for position/count performance comparison. Create visualizations of model performance across different configurations for position/count tasks. Implement filtering capabilities to focus on specific position/count metrics. Develop interactive features for exploring position/count error patterns. Create visualizations showing transfer learning characteristics from spelling training to position/count tasks. Ensure the dashboard is accessible and interpretable for both technical and non-technical stakeholders. Save performance comparison visualizations to `results/evaluation/plots/` directory.

## 18. Qwen3 tokenizer pattern analysis [pending]
### Dependencies: 9.1, 9.14, 9.15
### Description: Analyze and handle Qwen3's specific tokenizer patterns in position/count evaluation
### Details:
Investigate Qwen3's tokenizer patterns and their impact on position/count evaluation. Develop tools to analyze token distributions in model outputs for position/count tasks. Implement proper handling of Qwen3's tokenizer patterns in position/count evaluation metrics. Create visualizations showing token usage patterns for position/count tasks. Analyze how tokenizer patterns affect performance on position and count task types. Document findings and recommendations for handling tokenizer-specific issues in position/count evaluation. Ensure evaluation metrics properly account for tokenizer characteristics.

## 19. Non-thinking mode configuration verification [pending]
### Dependencies: 9.14
### Description: Implement verification checks to ensure thinking mode is never enabled in the evaluation process
### Details:
Create validation checks in the evaluation pipeline to verify thinking mode is never enabled. Implement configuration validation in the model loading and generation functions. Add assertions and error handling to prevent accidental use of thinking mode. Create documentation clearly stating the project policy on non-thinking mode usage. Update all code examples and documentation to explicitly show enable_thinking=False. Implement unit tests to verify thinking mode is never enabled in any part of the evaluation process. Create a pre-execution validation step in the Lightning.AI job system to verify configuration compliance.

