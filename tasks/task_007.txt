# Task ID: 7
# Title: Unsloth Integration for Optimized Fine-tuning
# Status: pending
# Dependencies: 6
# Priority: high
# Description: Set up Unsloth for optimized LoRA fine-tuning of the GPT-2 model with memory efficiency optimizations in a cloud GPU environment (Google Colab or Lightning.ai), using Python scripts instead of notebooks for better maintainability and version control. Configure the system to handle separate training (spelling variations) and evaluation (position/count) datasets.
# Details:
**NOTE: This task requires a cloud GPU environment. Do not attempt on local Mac.**

1. Install and configure Unsloth for optimized fine-tuning in Google Colab or Lightning.ai
2. Set up Unsloth-specific environment requirements in the cloud environment
3. Configure memory-efficient QLoRA training
4. Set up Flash Attention 2 if available on cloud GPU hardware
5. Implement proper tokenization for instruction fine-tuning
6. Configure GPU memory optimizations
7. Set up separate handling for spelling training data and position/count evaluation data
8. Implement efficient evaluation of position/count tasks during training

File Structure:
- Environment setup: `src/unsloth/environment.py`
- Model loading and configuration: `src/unsloth/model.py`
- Dataset preparation: `src/unsloth/dataset.py`
- Training setup: `src/unsloth/trainer.py`
- Training monitoring: `src/unsloth/monitor.py`
- HTML report generation: `src/unsloth/report.py`
- Evaluation utilities: `src/unsloth/evaluation.py`

Output Structure:
- `results/unsloth/figures/` (All PNG/PDF visualizations)
- `results/unsloth/reports/` (HTML reports)
- `results/unsloth/data/` (Training metrics)
- `results/unsloth/configs/` (Model configurations)
- `results/unsloth/evaluation/` (Position/count evaluation results)

Implementation:
```python
# Install Unsloth in Google Colab or Lightning.ai environment
!pip install unsloth

from unsloth import FastLanguageModel
import torch
from datasets import load_dataset

# Optimize GPU memory
def optimize_gpu_memory():
    if torch.cuda.is_available():
        # Set GPU memory allocation strategy
        torch.cuda.set_per_process_memory_fraction(0.9)  # Reserve 10% for system
        # Enable memory caching for faster allocation
        torch.backends.cudnn.benchmark = True
        # Use TF32 precision on Ampere GPUs or later for faster computation
        torch.backends.cuda.matmul.allow_tf32 = True

# Load model with Unsloth optimizations
def load_unsloth_model(config):
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="gpt2",
        max_seq_length=512,
        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        load_in_4bit=True,  # Use 4-bit quantization to reduce memory usage
        token=None,  # Add your HF token for private models
    )

    # Add LoRA adapters with Unsloth-specific optimizations
    model = FastLanguageModel.get_peft_model(
        model,
        r=config["lora_config"]["r"],
        target_modules=config["lora_config"]["target_modules"],
        lora_alpha=config["lora_config"]["alpha"],
        lora_dropout=config["lora_config"]["dropout"],
        bias="none",  # Unsloth-specific - sets which modules receive adapters
        use_gradient_checkpointing=True,  # Unsloth-specific - saves memory
        random_state=42,  # For reproducibility
        use_rslora=False,  # Set to True for rank-stabilized LoRA (optional)
        loftq_config=None,  # Optional LoftQ configuration
    )
    
    return model, tokenizer

# Prepare spelling dataset for Unsloth
def prepare_spelling_dataset(dataset, tokenizer):
    def formatting_prompts_func(examples):
        questions = examples["question"]
        answers = examples["answer"]

        # Special Unsloth prompt format
        prompts = [
            f"<human>: {question}\n<assistant>: "
            for question in questions
        ]

        # Format responses with EOS token
        formatted_responses = [
            f"{answer}{tokenizer.eos_token}"
            for answer in answers
        ]

        return {
            "prompt": prompts,
            "completion": formatted_responses,
        }
    
    formatted_dataset = dataset.map(formatting_prompts_func, batched=True)
    return formatted_dataset

# Prepare position/count dataset for evaluation
def prepare_position_count_dataset(dataset, tokenizer):
    def formatting_prompts_func(examples):
        questions = examples["question"]
        answers = examples["answer"]

        # Special Unsloth prompt format for position/count tasks
        prompts = [
            f"<human>: {question}\n<assistant>: "
            for question in questions
        ]

        # Format responses with EOS token
        formatted_responses = [
            f"{answer}{tokenizer.eos_token}"
            for answer in answers
        ]

        return {
            "prompt": prompts,
            "completion": formatted_responses,
            "task_type": examples.get("task_type", ["position_count"] * len(questions))
        }
    
    formatted_dataset = dataset.map(formatting_prompts_func, batched=True)
    return formatted_dataset

# Set up Unsloth trainer with dual dataset support
def create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, eval_dataset, config):
    trainer = FastLanguageModel.get_trainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,  # Spelling validation dataset
        args=FastLanguageModel.get_train_args(
            output_dir=f"./spelling-lora-{config['experiment_name']}",
            per_device_train_batch_size=config["training_config"]["per_device_train_batch_size"],
            gradient_accumulation_steps=config["training_config"]["gradient_accumulation_steps"],
            warmup_steps=config["training_config"]["warmup_steps"],
            max_steps=config["training_config"]["max_steps"],
            learning_rate=config["training_config"]["learning_rate"],
            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            logging_steps=10,
            evaluation_strategy="steps",
            eval_steps=100,
            save_strategy="steps",
            save_steps=200,
            optim="adamw_torch",  # Unsloth recommends adamw_torch over paged_adamw_8bit
            max_grad_norm=0.3,    # Gradient clipping - Unsloth recommended value
            report_to="wandb",
        ),
        data_collator=FastLanguageModel.get_data_collator(tokenizer=tokenizer),
    )
    
    # Add position/count evaluation dataset as a custom attribute
    trainer.position_count_dataset = eval_dataset
    
    # Add custom evaluation callback for position/count tasks
    class PositionCountEvaluationCallback(TrainerCallback):
        def on_evaluate(self, args, state, control, **kwargs):
            # Run evaluation on position/count dataset
            metrics = evaluate_position_count(trainer.model, trainer.tokenizer, 
                                             trainer.position_count_dataset, 
                                             config["evaluation_config"])
            # Log metrics to wandb
            wandb.log({f"position_count_{k}": v for k, v in metrics.items()}, 
                      step=state.global_step)
    
    trainer.add_callback(PositionCountEvaluationCallback())
    
    return trainer

# Evaluate model on position/count tasks
def evaluate_position_count(model, tokenizer, dataset, config):
    # Set up metrics
    metrics = {
        "position_accuracy": 0.0,
        "count_accuracy": 0.0,
        "overall_accuracy": 0.0
    }
    
    # Implement evaluation logic for position/count tasks
    # This would generate predictions and compare against ground truth
    
    return metrics

# Main training function
def train_with_unsloth(config_path):
    # Load config
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    
    # Initialize W&B
    wandb.init(project="llm-spelling-finetuning", name=config["experiment_name"], config=config)
    
    # Optimize GPU memory
    optimize_gpu_memory()
    
    # Load model and tokenizer
    model, tokenizer = load_unsloth_model(config)
    
    # Load datasets
    spelling_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset")
    position_count_dataset = load_dataset("YOUR-USERNAME/llm-position-count-dataset")
    
    # Prepare datasets for Unsloth
    train_dataset = prepare_spelling_dataset(spelling_dataset["train"], tokenizer)
    val_dataset = prepare_spelling_dataset(spelling_dataset["validation"], tokenizer)
    eval_dataset = prepare_position_count_dataset(position_count_dataset["validation"], tokenizer)
    
    # Create trainer
    trainer = create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, eval_dataset, config)
    
    # Train model
    trainer.train()
    
    # Save model
    trainer.save_model()
    
    # Final evaluation on both datasets
    spelling_eval_results = trainer.evaluate()
    position_count_eval_results = evaluate_position_count(model, tokenizer, eval_dataset, config)
    
    # Log final results
    wandb.log({
        **spelling_eval_results,
        **{f"final_position_count_{k}": v for k, v in position_count_eval_results.items()}
    })
    
    # Generate comprehensive report
    generate_evaluation_report(spelling_eval_results, position_count_eval_results, config)
    
    # Close W&B
    wandb.finish()
    
    return {
        "spelling": spelling_eval_results,
        "position_count": position_count_eval_results
    }

# Generate comprehensive evaluation report
def generate_evaluation_report(spelling_results, position_count_results, config):
    # Create HTML report with visualizations for both tasks
    # Save to results/unsloth/reports/
    pass
```

# Test Strategy:
1. Verify Unsloth installs and imports correctly in Google Colab or Lightning.ai
2. Confirm memory usage is optimized compared to standard fine-tuning
3. Test that 4-bit quantization is working correctly on cloud GPU
4. Measure training speed improvement over baseline implementation
5. Verify all Unsloth-specific optimizations are configured
6. Test with a small dataset to ensure the training loop works in cloud environment
7. Monitor GPU memory usage during training
8. Verify that the implementation does not contain any local-only dependencies
9. Test command-line interfaces for all scripts
10. Verify HTML report generation functionality
11. Test the integration between all Python modules
12. Validate output directory structure and file generation
13. Ensure proper error handling and logging in scripts
14. Test loading and processing of both spelling and position/count datasets
15. Verify that evaluation metrics for both tasks are correctly calculated and logged
16. Test the transfer performance from spelling training to position/count evaluation
17. Validate that memory usage remains optimized when handling both datasets
18. Test the custom evaluation callback for position/count tasks

# Subtasks:
## 1. Environment Setup with Optimizations [pending]
### Dependencies: None
### Description: Configure the cloud GPU environment with Unsloth and necessary dependencies for optimized LLM fine-tuning
### Details:
Install Unsloth library and compatible dependencies (bitsandbytes, transformers, PEFT). Configure GPU memory settings for optimal performance. Set up quantization libraries. Verify hardware compatibility (CUDA, ROCm, or Metal backend). Test environment with basic model loading to ensure all components work together.
<info added on 2025-05-07T14:48:05.432Z>
Install Unsloth library and compatible dependencies (bitsandbytes, transformers, PEFT). Configure GPU memory settings for optimal performance. Set up quantization libraries. Verify hardware compatibility (CUDA, ROCm, or Metal backend). Test environment with basic model loading to ensure all components work together.

This task can be worked on independently and in parallel with others. The environment setup has no dependencies and is parallelizable (parallelizable: true), allowing team members to begin this work immediately while other tasks are being planned or executed.
</info added on 2025-05-07T14:48:05.432Z>

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 2. Model Loading with Unsloth-specific Configurations [pending]
### Dependencies: 7.1
### Description: Implement efficient model loading using Unsloth's FastLanguageModel with proper quantization and LoRA setup in cloud GPU environment
### Details:
Use FastLanguageModel.from_pretrained() to load base models with quantization in Google Colab or Lightning.ai. Configure LoRA adapters with get_peft_model() using appropriate rank and target modules. Implement proper quantization settings (4-bit, 8-bit) based on available cloud GPU VRAM. Set up gradient checkpointing with 'unsloth' option. Validate model loading with memory profiling.

File location:
- Model loading and configuration: `src/unsloth/model.py`

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 3. Dataset Preparation for Unsloth [pending]
### Dependencies: 7.1
### Description: Prepare and optimize training datasets for efficient processing with Unsloth in cloud environment
### Details:
Format dataset according to Unsloth requirements in Google Colab or Lightning.ai. Implement efficient tokenization with proper sequence length handling. Set up data processing pipeline with appropriate num_proc parameter. Configure dataset caching mechanisms to reduce memory overhead. Validate dataset loading with performance metrics.

File location:
- Dataset preparation: `src/unsloth/dataset.py`

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 4. Trainer Setup with Memory Optimizations [pending]
### Dependencies: 7.2, 7.3
### Description: Configure SFTTrainer with Unsloth-optimized parameters for efficient fine-tuning in cloud GPU environment
### Details:
Set up SFTTrainer with optimized batch size and gradient accumulation in Google Colab or Lightning.ai. Configure learning rate and scheduler based on training duration. Implement proper precision settings (bf16/fp16) based on cloud GPU hardware support. Set up memory-efficient optimizers (adamw_8bit). Configure logging and checkpointing. Validate trainer setup with memory usage monitoring during initial training steps.

File location:
- Training setup: `src/unsloth/trainer.py`

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 5. Monitoring and Reporting System [pending]
### Dependencies: 7.1, 7.2, 7.3, 7.4
### Description: Create comprehensive monitoring and reporting system for Unsloth training
### Details:
Implement training monitoring system with real-time metrics tracking. Create HTML report generation functionality to summarize training results. Develop visualization utilities for training metrics. Set up proper logging and error handling. Implement command-line interfaces for all scripts.

File locations:
- Training monitoring: `src/unsloth/monitor.py`
- HTML report generation: `src/unsloth/report.py`

Output locations:
- `results/unsloth/figures/` (All PNG/PDF visualizations)
- `results/unsloth/reports/` (HTML reports)
- `results/unsloth/data/` (Training metrics)
- `results/unsloth/configs/` (Model configurations)

**NOTE: While development can be done locally, testing should be performed in a cloud GPU environment.**

## 6. Command-line Interface and Integration [pending]
### Dependencies: 7.1, 7.2, 7.3, 7.4, 7.5
### Description: Develop command-line interfaces for all Unsloth scripts and ensure proper integration
### Details:
Create command-line interfaces for all Unsloth scripts to enable flexible usage. Implement proper argument parsing with sensible defaults. Ensure proper integration between all modules. Set up configuration file handling. Implement proper error handling and user feedback. Create comprehensive documentation for CLI usage.

File locations:
- All Python scripts in `src/unsloth/`
- Main CLI entry point: `src/unsloth/__main__.py`

**NOTE: While development can be done locally, testing should be performed in a cloud GPU environment.**

## 7. Dual Dataset Handling Implementation [pending]
### Dependencies: 7.3
### Description: Implement efficient handling of both spelling training data and position/count evaluation data
### Details:
Create separate data processing pipelines for spelling and position/count datasets. Implement efficient data loading and caching mechanisms for both datasets. Configure memory-efficient data handling during training and evaluation phases. Implement dataset-specific tokenization and formatting. Validate dual dataset handling with performance metrics.

File location:
- Dataset preparation: `src/unsloth/dataset.py`
- Dual dataset handler: `src/unsloth/dual_dataset.py`

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 8. Position/Count Task Evaluation System [pending]
### Dependencies: 7.4, 7.7
### Description: Develop evaluation system for position/count tasks during spelling variation training
### Details:
Implement custom evaluation callback for position/count tasks. Create metrics calculation for position and count accuracy. Develop efficient evaluation pipeline that runs during training. Set up proper logging of transfer metrics. Implement visualization utilities for transfer performance. Create comprehensive reporting for both spelling and position/count performance.

File locations:
- Evaluation utilities: `src/unsloth/evaluation.py`
- Training monitoring: `src/unsloth/monitor.py`

Output locations:
- `results/unsloth/evaluation/` (Position/count evaluation results)
- `results/unsloth/figures/` (Transfer performance visualizations)

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

