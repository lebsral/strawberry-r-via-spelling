# Task ID: 7
# Title: Unsloth Integration for Optimized Fine-tuning
# Status: pending
# Dependencies: 6
# Priority: high
# Description: Set up Unsloth for optimized LoRA fine-tuning of the Qwen3-4B model with memory efficiency optimizations in a cloud GPU environment (Google Colab or Lightning.ai), using Python scripts instead of notebooks for better maintainability and version control. Configure the system to handle separate training (spelling variations) and evaluation (position/count) datasets, with special attention to Qwen3's tokenizer and English-only token subset.
# Details:
**NOTE: This task requires a cloud GPU environment. Do not attempt on local Mac.**

1. Install and configure Unsloth for optimized fine-tuning in Google Colab or Lightning.ai
2. Set up Unsloth-specific environment requirements in the cloud environment
3. Configure memory-efficient QLoRA training
4. Set up Flash Attention 2 if available on cloud GPU hardware
5. Implement proper tokenization for instruction fine-tuning with Qwen3-4B's tokenizer
6. Configure GPU memory optimizations
7. Set up separate handling for spelling training data and position/count evaluation data
8. Implement efficient evaluation of position/count tasks during training
9. Leverage Lightning.AI Studios for data preparation and processing
10. Filter and validate data based on Qwen3's English-only token subset
11. Handle multi-token words appropriately in Qwen3's tokenization patterns
12. Prepare data in non-thinking mode only (enable_thinking=False)

File Structure:
- Environment setup: `src/unsloth/environment.py`
- Model loading and configuration: `src/unsloth/model.py`
- Dataset preparation: `src/unsloth/dataset.py`
- Lightning DataModules: `src/unsloth/datamodules.py`
- Training setup: `src/unsloth/trainer.py`
- Training monitoring: `src/unsloth/monitor.py`
- HTML report generation: `src/unsloth/report.py`
- Evaluation utilities: `src/unsloth/evaluation.py`
- Data validation: `src/unsloth/data_validation.py`
- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`
- Token filtering utilities: `src/unsloth/token_filter.py`

Output Structure:
- `results/unsloth/figures/` (All PNG/PDF visualizations)
- `results/unsloth/reports/` (HTML reports)
- `results/unsloth/data/` (Training metrics)
- `results/unsloth/configs/` (Model configurations)
- `results/unsloth/evaluation/` (Position/count evaluation results)
- `results/unsloth/data_versions/` (Data version tracking)
- `results/unsloth/token_analysis/` (Qwen3 token usage analysis)

Implementation:
```python
# Install Unsloth in Google Colab or Lightning.ai environment
!pip install unsloth

from unsloth import FastLanguageModel
import torch
from datasets import load_dataset

# Optimize GPU memory
def optimize_gpu_memory():
    if torch.cuda.is_available():
        # Set GPU memory allocation strategy
        torch.cuda.set_per_process_memory_fraction(0.9)  # Reserve 10% for system
        # Enable memory caching for faster allocation
        torch.backends.cudnn.benchmark = True
        # Use TF32 precision on Ampere GPUs or later for faster computation
        torch.backends.cuda.matmul.allow_tf32 = True

# Load model with Unsloth optimizations
def load_unsloth_model(config):
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="Qwen/Qwen3-4B",  # Updated to Qwen3-4B
        max_seq_length=512,
        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        load_in_4bit=True,  # Use 4-bit quantization to reduce memory usage
        token=None,  # Add your HF token for private models
    )

    # Add LoRA adapters with Unsloth-specific optimizations
    model = FastLanguageModel.get_peft_model(
        model,
        r=config["lora_config"]["r"],
        target_modules=config["lora_config"]["target_modules"],
        lora_alpha=config["lora_config"]["alpha"],
        lora_dropout=config["lora_config"]["dropout"],
        bias="none",  # Unsloth-specific - sets which modules receive adapters
        use_gradient_checkpointing=True,  # Unsloth-specific - saves memory
        random_state=42,  # For reproducibility
        use_rslora=False,  # Set to True for rank-stabilized LoRA (optional)
        loftq_config=None,  # Optional LoftQ configuration
    )
    
    return model, tokenizer

# Get English-only token subset for Qwen3
def get_english_token_subset(tokenizer):
    # This function identifies the English-only token subset in Qwen3's vocabulary
    # Implementation depends on Qwen3's specific tokenization patterns
    english_tokens = []
    for token_id in range(len(tokenizer)):
        token = tokenizer.decode([token_id])
        # Apply filtering logic to identify English-only tokens
        # This is a simplified example - actual implementation would be more complex
        if all(c.isascii() for c in token):
            english_tokens.append(token_id)
    
    return set(english_tokens)

# Analyze multi-token word handling in Qwen3
def analyze_multi_token_words(tokenizer, common_words):
    results = {}
    for word in common_words:
        tokens = tokenizer.encode(word, add_special_tokens=False)
        results[word] = {
            "token_count": len(tokens),
            "tokens": [tokenizer.decode([t]) for t in tokens]
        }
    return results

# Prepare spelling dataset for Unsloth with Qwen3 tokenizer
def prepare_spelling_dataset(dataset, tokenizer, config):
    # Get English token subset if filtering is enabled
    english_tokens = get_english_token_subset(tokenizer) if config["use_english_only"] else None
    
    def formatting_prompts_func(examples):
        questions = examples["question"]
        answers = examples["answer"]
        
        # Standard prompt format (non-thinking mode only)
        prompts = [
            f"<human>: {question}\n<assistant>: "
            for question in questions
        ]

        # Format responses with EOS token
        formatted_responses = [
            f"{answer}{tokenizer.eos_token}"
            for answer in answers
        ]
        
        # Validate tokens if English-only filtering is enabled
        if english_tokens:
            valid_examples = []
            valid_responses = []
            
            for prompt, response in zip(prompts, formatted_responses):
                prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)
                response_tokens = tokenizer.encode(response, add_special_tokens=False)
                
                # Check if all tokens are in the English subset
                if all(t in english_tokens for t in prompt_tokens + response_tokens):
                    valid_examples.append(prompt)
                    valid_responses.append(response)
                    
            prompts = valid_examples
            formatted_responses = valid_responses

        return {
            "prompt": prompts,
            "completion": formatted_responses,
        }
    
    formatted_dataset = dataset.map(formatting_prompts_func, batched=True)
    
    # Analyze multi-token word handling
    if config["analyze_tokenization"]:
        common_words = [example["answer"] for example in dataset[:100]]
        tokenization_analysis = analyze_multi_token_words(tokenizer, common_words)
        
        # Save analysis to file
        import json
        with open("results/unsloth/token_analysis/multi_token_analysis.json", "w") as f:
            json.dump(tokenization_analysis, f, indent=2)
    
    return formatted_dataset

# Prepare position/count dataset for evaluation with Qwen3 tokenizer
def prepare_position_count_dataset(dataset, tokenizer, config):
    # Get English token subset if filtering is enabled
    english_tokens = get_english_token_subset(tokenizer) if config["use_english_only"] else None
    
    def formatting_prompts_func(examples):
        questions = examples["question"]
        answers = examples["answer"]

        # Standard prompt format (non-thinking mode only)
        prompts = [
            f"<human>: {question}\n<assistant>: "
            for question in questions
        ]

        # Format responses with EOS token
        formatted_responses = [
            f"{answer}{tokenizer.eos_token}"
            for answer in answers
        ]
        
        # Validate tokens if English-only filtering is enabled
        if english_tokens:
            valid_examples = []
            valid_responses = []
            valid_task_types = []
            
            for prompt, response, task_type in zip(prompts, formatted_responses, 
                                                 examples.get("task_type", ["position_count"] * len(questions))):
                prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)
                response_tokens = tokenizer.encode(response, add_special_tokens=False)
                
                # Check if all tokens are in the English subset
                if all(t in english_tokens for t in prompt_tokens + response_tokens):
                    valid_examples.append(prompt)
                    valid_responses.append(response)
                    valid_task_types.append(task_type)
                    
            prompts = valid_examples
            formatted_responses = valid_responses
            task_types = valid_task_types
        else:
            task_types = examples.get("task_type", ["position_count"] * len(questions))

        return {
            "prompt": prompts,
            "completion": formatted_responses,
            "task_type": task_types
        }
    
    formatted_dataset = dataset.map(formatting_prompts_func, batched=True)
    return formatted_dataset

# Set up Unsloth trainer with dual dataset support
def create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, eval_dataset, config):
    trainer = FastLanguageModel.get_trainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,  # Spelling validation dataset
        args=FastLanguageModel.get_train_args(
            output_dir=f"./spelling-lora-{config['experiment_name']}",
            per_device_train_batch_size=config["training_config"]["per_device_train_batch_size"],
            gradient_accumulation_steps=config["training_config"]["gradient_accumulation_steps"],
            warmup_steps=config["training_config"]["warmup_steps"],
            max_steps=config["training_config"]["max_steps"],
            learning_rate=config["training_config"]["learning_rate"],
            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            logging_steps=10,
            evaluation_strategy="steps",
            eval_steps=100,
            save_strategy="steps",
            save_steps=200,
            optim="adamw_torch",  # Unsloth recommends adamw_torch over paged_adamw_8bit
            max_grad_norm=0.3,    # Gradient clipping - Unsloth recommended value
            report_to="wandb",
        ),
        data_collator=FastLanguageModel.get_data_collator(tokenizer=tokenizer),
    )
    
    # Add position/count evaluation dataset as a custom attribute
    trainer.position_count_dataset = eval_dataset
    
    # Add custom evaluation callback for position/count tasks
    class PositionCountEvaluationCallback(TrainerCallback):
        def on_evaluate(self, args, state, control, **kwargs):
            # Run evaluation on position/count dataset
            metrics = evaluate_position_count(trainer.model, trainer.tokenizer, 
                                             trainer.position_count_dataset, 
                                             config["evaluation_config"])
            # Log metrics to wandb
            wandb.log({f"position_count_{k}": v for k, v in metrics.items()}, 
                      step=state.global_step)
    
    trainer.add_callback(PositionCountEvaluationCallback())
    
    return trainer

# Evaluate model on position/count tasks
def evaluate_position_count(model, tokenizer, dataset, config):
    # Set up metrics
    metrics = {
        "position_accuracy": 0.0,
        "count_accuracy": 0.0,
        "overall_accuracy": 0.0
    }
    
    # Implement evaluation logic for position/count tasks
    # This would generate predictions and compare against ground truth
    
    return metrics

# Main training function
def train_with_unsloth(config_path):
    # Load config
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    
    # Initialize W&B
    wandb.init(project="llm-spelling-finetuning", name=config["experiment_name"], config=config)
    
    # Optimize GPU memory
    optimize_gpu_memory()
    
    # Load model and tokenizer
    model, tokenizer = load_unsloth_model(config)
    
    # Load datasets
    spelling_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset")
    position_count_dataset = load_dataset("YOUR-USERNAME/llm-position-count-dataset")
    
    # Prepare datasets for Unsloth with Qwen3 tokenizer
    train_dataset = prepare_spelling_dataset(spelling_dataset["train"], tokenizer, config)
    val_dataset = prepare_spelling_dataset(spelling_dataset["validation"], tokenizer, config)
    eval_dataset = prepare_position_count_dataset(position_count_dataset["validation"], tokenizer, config)
    
    # Create trainer
    trainer = create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, eval_dataset, config)
    
    # Train model
    trainer.train()
    
    # Save model
    trainer.save_model()
    
    # Final evaluation on both datasets
    spelling_eval_results = trainer.evaluate()
    position_count_eval_results = evaluate_position_count(model, tokenizer, eval_dataset, config)
    
    # Log final results
    wandb.log({
        **spelling_eval_results,
        **{f"final_position_count_{k}": v for k, v in position_count_eval_results.items()}
    })
    
    # Generate comprehensive report
    generate_evaluation_report(spelling_eval_results, position_count_eval_results, config)
    
    # Close W&B
    wandb.finish()
    
    return {
        "spelling": spelling_eval_results,
        "position_count": position_count_eval_results
    }

# Generate comprehensive evaluation report
def generate_evaluation_report(spelling_results, position_count_results, config):
    # Create HTML report with visualizations for both tasks
    # Save to results/unsloth/reports/
    pass
```

# Test Strategy:
1. Verify Unsloth installs and imports correctly in Google Colab or Lightning.ai
2. Confirm memory usage is optimized compared to standard fine-tuning
3. Test that 4-bit quantization is working correctly on cloud GPU
4. Measure training speed improvement over baseline implementation
5. Verify all Unsloth-specific optimizations are configured
6. Test with a small dataset to ensure the training loop works in cloud environment
7. Monitor GPU memory usage during training
8. Verify that the implementation does not contain any local-only dependencies
9. Test command-line interfaces for all scripts
10. Verify HTML report generation functionality
11. Test the integration between all Python modules
12. Validate output directory structure and file generation
13. Ensure proper error handling and logging in scripts
14. Test loading and processing of both spelling and position/count datasets
15. Verify that evaluation metrics for both tasks are correctly calculated and logged
16. Test the transfer performance from spelling training to position/count evaluation
17. Validate that memory usage remains optimized when handling both datasets
18. Test the custom evaluation callback for position/count tasks
19. Verify Lightning.AI Studio setup and configuration for data preparation
20. Test data validation and quality check mechanisms
21. Validate data versioning and tracking functionality
22. Test Lightning DataModules integration with Unsloth training pipeline
23. Verify efficient data streaming and caching mechanisms in Lightning.AI environment
24. Test Qwen3-4B tokenizer handling for multi-token words
25. Verify English-only token subset filtering functionality
26. Test data preparation in non-thinking mode only
27. Validate token analysis reporting for Qwen3 tokenization patterns
28. Test compatibility of prepared data with Qwen3's tokenization requirements
29. Verify that filtered datasets maintain sufficient size for effective training
30. Test the impact of English-only token filtering on model performance

# Subtasks:
## 1. Environment Setup with Optimizations [pending]
### Dependencies: None
### Description: Configure the cloud GPU environment with Unsloth and necessary dependencies for optimized LLM fine-tuning
### Details:
Install Unsloth library and compatible dependencies (bitsandbytes, transformers, PEFT). Configure GPU memory settings for optimal performance. Set up quantization libraries. Verify hardware compatibility (CUDA, ROCm, or Metal backend). Test environment with basic model loading to ensure all components work together.
<info added on 2025-05-07T14:48:05.432Z>
Install Unsloth library and compatible dependencies (bitsandbytes, transformers, PEFT). Configure GPU memory settings for optimal performance. Set up quantization libraries. Verify hardware compatibility (CUDA, ROCm, or Metal backend). Test environment with basic model loading to ensure all components work together.

This task can be worked on independently and in parallel with others. The environment setup has no dependencies and is parallelizable (parallelizable: true), allowing team members to begin this work immediately while other tasks are being planned or executed.
</info added on 2025-05-07T14:48:05.432Z>

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 2. Model Loading with Unsloth-specific Configurations [pending]
### Dependencies: 7.1
### Description: Implement efficient model loading using Unsloth's FastLanguageModel with proper quantization and LoRA setup in cloud GPU environment
### Details:
Use FastLanguageModel.from_pretrained() to load base models with quantization in Google Colab or Lightning.ai. Configure LoRA adapters with get_peft_model() using appropriate rank and target modules. Implement proper quantization settings (4-bit, 8-bit) based on available cloud GPU VRAM. Set up gradient checkpointing with 'unsloth' option. Validate model loading with memory profiling.

File location:
- Model loading and configuration: `src/unsloth/model.py`

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 3. Dataset Preparation for Unsloth [pending]
### Dependencies: 7.1
### Description: Prepare and optimize training datasets for efficient processing with Unsloth using Lightning.AI Studios, with special handling for Qwen3-4B's tokenizer and English-only token subset
### Details:
Create a dedicated Lightning.AI Studio for data preparation following the "one Studio, one task" principle. Format dataset according to Unsloth and Qwen3-4B requirements using Lightning.AI's shared filesystem for efficient data storage and access. Implement Lightning DataModules for better integration with the training pipeline. Set up automated data validation and quality checks to ensure data integrity. Configure proper environment isolation and dependency management in the Studio. Use CPU-optimized instances for data processing tasks to optimize costs. Implement efficient data streaming and caching mechanisms. Set up automated data versioning and tracking for reproducibility.

Specific Qwen3-4B requirements:
1. Implement functions to identify and filter for English-only token subset
2. Create analysis tools for multi-token word handling in Qwen3's tokenization patterns
3. Set up data preparation in non-thinking mode only
4. Implement validation to ensure data compatibility with Qwen3's tokenization requirements
5. Create reporting tools to analyze token usage patterns in the dataset

File locations:
- Dataset preparation: `src/unsloth/dataset.py`
- Lightning DataModules: `src/unsloth/datamodules.py`
- Data validation: `src/unsloth/data_validation.py`
- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`
- Token filtering utilities: `src/unsloth/token_filter.py`

Output locations:
- `results/unsloth/data_versions/` (Data version tracking)
- `results/unsloth/token_analysis/` (Qwen3 token usage analysis)

**NOTE: This task requires a Lightning.AI Studio environment. Documentation should include detailed setup instructions.**

## 4. Trainer Setup with Memory Optimizations [pending]
### Dependencies: 7.2, 7.3
### Description: Configure SFTTrainer with Unsloth-optimized parameters for efficient fine-tuning in cloud GPU environment
### Details:
Set up SFTTrainer with optimized batch size and gradient accumulation in Google Colab or Lightning.ai. Configure learning rate and scheduler based on training duration. Implement proper precision settings (bf16/fp16) based on cloud GPU hardware support. Set up memory-efficient optimizers (adamw_8bit). Configure logging and checkpointing. Validate trainer setup with memory usage monitoring during initial training steps.

File location:
- Training setup: `src/unsloth/trainer.py`

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 5. Monitoring and Reporting System [pending]
### Dependencies: 7.1, 7.2, 7.3, 7.4
### Description: Create comprehensive monitoring and reporting system for Unsloth training
### Details:
Implement training monitoring system with real-time metrics tracking. Create HTML report generation functionality to summarize training results. Develop visualization utilities for training metrics. Set up proper logging and error handling. Implement command-line interfaces for all scripts.

File locations:
- Training monitoring: `src/unsloth/monitor.py`
- HTML report generation: `src/unsloth/report.py`

Output locations:
- `results/unsloth/figures/` (All PNG/PDF visualizations)
- `results/unsloth/reports/` (HTML reports)
- `results/unsloth/data/` (Training metrics)
- `results/unsloth/configs/` (Model configurations)

**NOTE: While development can be done locally, testing should be performed in a cloud GPU environment.**

## 6. Command-line Interface and Integration [pending]
### Dependencies: 7.1, 7.2, 7.3, 7.4, 7.5
### Description: Develop command-line interfaces for all Unsloth scripts and ensure proper integration
### Details:
Create command-line interfaces for all Unsloth scripts to enable flexible usage. Implement proper argument parsing with sensible defaults. Ensure proper integration between all modules. Set up configuration file handling. Implement proper error handling and user feedback. Create comprehensive documentation for CLI usage.

File locations:
- All Python scripts in `src/unsloth/`
- Main CLI entry point: `src/unsloth/__main__.py`

**NOTE: While development can be done locally, testing should be performed in a cloud GPU environment.**

## 7. Dual Dataset Handling Implementation [pending]
### Dependencies: 7.3
### Description: Implement efficient handling of both spelling training data and position/count evaluation data with Qwen3-4B tokenizer support
### Details:
Create separate data processing pipelines for spelling and position/count datasets. Implement efficient data loading and caching mechanisms for both datasets. Configure memory-efficient data handling during training and evaluation phases. Implement dataset-specific tokenization and formatting for Qwen3-4B. Validate dual dataset handling with performance metrics.

Qwen3-4B specific requirements:
1. Implement English-only token subset filtering for both datasets
2. Handle multi-token words appropriately in both datasets
3. Support non-thinking mode only in data preparation
4. Create analysis tools to validate token usage patterns
5. Implement efficient caching of tokenized data to improve performance

File location:
- Dataset preparation: `src/unsloth/dataset.py`
- Dual dataset handler: `src/unsloth/dual_dataset.py`
- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 8. Position/Count Task Evaluation System [pending]
### Dependencies: 7.4, 7.7
### Description: Develop evaluation system for position/count tasks during spelling variation training
### Details:
Implement custom evaluation callback for position/count tasks. Create metrics calculation for position and count accuracy. Develop efficient evaluation pipeline that runs during training. Set up proper logging of transfer metrics. Implement visualization utilities for transfer performance. Create comprehensive reporting for both spelling and position/count performance.

File locations:
- Evaluation utilities: `src/unsloth/evaluation.py`
- Training monitoring: `src/unsloth/monitor.py`

Output locations:
- `results/unsloth/evaluation/` (Position/count evaluation results)
- `results/unsloth/figures/` (Transfer performance visualizations)

**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**

## 9. Lightning.AI Studio Documentation [pending]
### Dependencies: 7.3
### Description: Create comprehensive documentation for Lightning.AI Studio setup and configuration
### Details:
Document the complete setup process for Lightning.AI Studios for data preparation. Include detailed instructions for environment configuration, dependency management, and resource allocation. Document the shared filesystem usage and data versioning approach. Create step-by-step guides for setting up CPU-optimized instances for data processing. Document the integration between Lightning DataModules and Unsloth training pipeline. Include troubleshooting guides and best practices for efficient data processing in Lightning.AI environment.

File location:
- `docs/lightning_studio_setup.md`
- `docs/data_processing_guide.md`

**NOTE: This documentation should be comprehensive enough for new team members to set up and use Lightning.AI Studios for data preparation.**

## 10. Qwen3 Tokenizer Analysis and Optimization [pending]
### Dependencies: 7.3
### Description: Analyze and optimize data processing for Qwen3-4B's tokenizer patterns
### Details:
Create comprehensive analysis tools for Qwen3-4B's tokenization patterns. Implement visualization utilities for token usage patterns. Develop optimization strategies for multi-token words. Create efficient filtering mechanisms for English-only token subset. Implement validation tools to ensure data compatibility with Qwen3's tokenization requirements.

Specific tasks:
1. Create token frequency analysis tools for Qwen3-4B vocabulary
2. Implement visualization utilities for token distribution in datasets
3. Develop optimization strategies for handling multi-token words
4. Create efficient filtering mechanisms for English-only token subset
5. Implement validation tools to ensure data compatibility with Qwen3's tokenization

File locations:
- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`
- Token analysis tools: `src/unsloth/token_analysis.py`
- Visualization utilities: `src/unsloth/token_visualization.py`

Output locations:
- `results/unsloth/token_analysis/` (Qwen3 token usage analysis)
- `results/unsloth/figures/token_distribution/` (Token distribution visualizations)

**NOTE: While analysis can be done locally, validation should be performed with the actual Qwen3-4B tokenizer in a cloud environment.**

## 11. Non-Thinking Mode Implementation [pending]
### Dependencies: 7.3, 7.7
### Description: Implement support for non-thinking mode in data preparation
### Details:
Create data preparation pipelines that exclusively support non-thinking mode (enable_thinking=False). Ensure all code, configuration, and documentation enforce this project policy.

Specific tasks:
1. Implement non-thinking mode formatting for direct response generation
2. Create configuration validation to ensure thinking mode is never enabled
3. Develop documentation that clearly states the non-thinking mode requirement
4. Implement validation checks to prevent accidental use of thinking mode
5. Create visualization utilities for token usage patterns in non-thinking mode

File locations:
- Dataset preparation: `src/unsloth/dataset.py`
- Configuration handling: `src/unsloth/config.py`
- Configuration validation: `src/unsloth/config_validation.py`

Output locations:
- `results/unsloth/token_analysis/` (Non-thinking mode token analysis)
- `results/unsloth/figures/token_distribution/` (Token distribution visualizations)

**NOTE: This task requires testing in a cloud GPU environment with the actual Qwen3-4B model.**

