# Task ID: 14
# Title: Migrate to Qwen3-4B Model with English Token Filtering and Mode Support
# Status: pending
# Dependencies: None
# Priority: high
# Description: Analyze and integrate the Qwen3-4B model into the codebase, extract English-only tokens from its tokenizer, and update model loading, configuration, and inference logic to support Qwen3-4B's unique features, focusing exclusively on non-thinking mode operation.
# Details:
1. Review the Qwen3-4B model documentation and Hugging Face integration requirements, ensuring compatibility with the latest transformers library (>=4.51.0) to avoid loading errors.
2. Analyze the Qwen3-4B tokenizer to identify and extract the set of English-only tokens. Implement a script or utility to filter these tokens, documenting the extraction process and results.
3. Refactor the model loading and configuration code to support Qwen3-4B, including device mapping, torch_dtype, and context length settings as per model specs.
4. Update inference logic to use Qwen3-4B in non-thinking mode only, ensuring the tokenizer's `apply_chat_template` is always called with `enable_thinking=False` parameter.
5. Ensure that appropriate sampling parameters are set and configurable for non-thinking mode operation.
6. Update documentation and code comments to reflect all changes, including migration steps, new configuration options, and any model-specific considerations, making it clear that only non-thinking mode is supported per project policy.

# Test Strategy:
- Verify that the codebase loads Qwen3-4B without errors and that model inference works correctly in non-thinking mode, producing expected outputs.
- Confirm that the English-only token extraction utility correctly identifies and outputs the relevant tokens, and validate the results against known English token sets.
- Test that the sampling parameters are correctly applied during inference, and that outputs change as expected when parameters are varied.
- Ensure that all code paths explicitly set `enable_thinking=False` when using the model's chat template.
- Review updated documentation for clarity and completeness, confirming no references to thinking mode remain, and perform code review to ensure maintainability and adherence to project standards.

# Subtasks:
## 1. Set up Qwen3-4B model with transformers library [done]
### Dependencies: None
### Description: Configure the development environment with the latest transformers library (>=4.51.0) and implement basic model loading for Qwen3-4B to ensure compatibility.
### Details:
Install or update transformers to version 4.51.0 or higher to avoid the 'KeyError: qwen3' error. Create a basic implementation to load the Qwen3-4B model using AutoModelForCausalLM and AutoTokenizer from the Hugging Face transformers library. Configure proper device mapping and torch_dtype settings for optimal performance. Test basic model loading and simple inference to verify setup.
<info added on 2025-05-10T09:22:09.195Z>
## Environment Setup
- Install or update transformers to version 4.51.0 or higher: `pip install --upgrade transformers>=4.51.0`
- Verify installation with: `import transformers; print(transformers.__version__)`
- Install any additional dependencies: `torch`, `accelerate` if needed

## Implementation Plan
1. Create implementation file at `src/models/qwen3_loader.py`
2. Use the following code structure:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def load_qwen3_model():
    model_id = "Qwen/Qwen3-4B"
    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Load model with appropriate dtype
    dtype = torch.float16 if device in ["cuda", "mps"] else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=dtype,
        device_map=device
    )
    
    return model, tokenizer, device

def test_inference(model, tokenizer, device):
    prompt = "Hello, world!"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    # Generate text
    outputs = model.generate(**inputs, max_new_tokens=20)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print(f"Input: {prompt}")
    print(f"Output: {generated_text}")
    return generated_text
```

3. Create a test script at `scripts/test_qwen3_loading.py`:
```python
from src.models.qwen3_loader import load_qwen3_model, test_inference

if __name__ == "__main__":
    print("Loading Qwen3-4B model...")
    model, tokenizer, device = load_qwen3_model()
    print("Model loaded successfully!")
    
    print("\nTesting basic inference...")
    test_inference(model, tokenizer, device)
```

## Potential Challenges
- Memory requirements: Qwen3-4B may require 8GB+ RAM
- If MPS/CUDA fails, fallback to CPU with `device_map="auto"` and `low_cpu_mem_usage=True`
- May need to use quantization for memory-constrained environments

## Success Criteria
- Model loads without errors
- Basic text generation works with a simple prompt
- Code is modular and reusable for the next subtask (token filtering)
</info added on 2025-05-10T09:22:09.195Z>
<info added on 2025-05-10T09:41:51.958Z>
Install or update transformers to version 4.51.0 or higher to avoid the 'KeyError: qwen3' error. Create a basic implementation to load the Qwen3-4B model using AutoModelForCausalLM and AutoTokenizer from the Hugging Face transformers library. Configure proper device mapping and torch_dtype settings for optimal performance. Test basic model loading and simple inference to verify setup.

<info added on 2025-05-10T09:22:09.195Z>
## Environment Setup
- Install or update transformers to version 4.51.0 or higher: `pip install --upgrade transformers>=4.51.0`
- Verify installation with: `import transformers; print(transformers.__version__)`
- Install any additional dependencies: `torch`, `accelerate` if needed

## Implementation Plan
1. Create implementation file at `src/models/qwen3_loader.py`
2. Use the following code structure:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def load_qwen3_model():
    model_id = "Qwen/Qwen3-4B"
    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Load model with appropriate dtype
    dtype = torch.float16 if device in ["cuda", "mps"] else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=dtype,
        device_map=device
    )
    
    return model, tokenizer, device

def test_inference(model, tokenizer, device):
    prompt = "Hello, world!"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    # Generate text
    outputs = model.generate(**inputs, max_new_tokens=20)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print(f"Input: {prompt}")
    print(f"Output: {generated_text}")
    return generated_text
```

3. Create a test script at `scripts/test_qwen3_loading.py`:
```python
from src.models.qwen3_loader import load_qwen3_model, test_inference

if __name__ == "__main__":
    print("Loading Qwen3-4B model...")
    model, tokenizer, device = load_qwen3_model()
    print("Model loaded successfully!")
    
    print("\nTesting basic inference...")
    test_inference(model, tokenizer, device)
```

## Potential Challenges
- Memory requirements: Qwen3-4B may require 8GB+ RAM
- If MPS/CUDA fails, fallback to CPU with `device_map="auto"` and `low_cpu_mem_usage=True`
- May need to use quantization for memory-constrained environments

## Success Criteria
- Model loads without errors
- Basic text generation works with a simple prompt
- Code is modular and reusable for the next subtask (token filtering)
</info added on 2025-05-10T09:22:09.195Z>

## Multi-Environment Implementation Strategy

For this project, we'll use different environments for different stages of the workflow:

1. **Local Development & Data Preparation**: Use Hugging Face transformers
   - Model ID: 'Qwen/Qwen3-4B'
   - Use for all data preparation, tokenization analysis, and validation
   - Ideal for token extraction and filtering work in the next subtask
   - Provides full access to tokenizer internals needed for English token filtering

2. **Quantized Inference**: Use Ollama
   - Model: 'qwen:4b'
   - Implement using the Ollama Python API
   - Example code:
   ```python
   import ollama
   
   response = ollama.chat(model='qwen:4b', 
                         messages=[{'role': 'user', 'content': 'Hello, world!'}])
   print(response['message']['content'])
   ```
   - Advantages: Lower memory footprint, faster inference on consumer hardware

3. **Fine-tuning**: Use Unsloth
   - Environment: Google Colab or Lightning.ai (NOT on Mac)
   - Provides optimized fine-tuning for Qwen models
   - Warning: Do not attempt to install or use Unsloth or xformers on Mac systems

4. **Implementation Notes**:
   - Create separate modules for each environment
   - Use factory pattern to abstract model interactions
   - Document environment-specific requirements in README.md
   - Create environment-specific setup scripts for each workflow
</info added on 2025-05-10T09:41:51.958Z>
<info added on 2025-05-10T10:03:45.695Z>
## Tokenizer-Only Approach for Data Tasks

For data preparation and token extraction tasks, we should only load the Qwen3-4B tokenizer without loading the full model. This approach significantly improves efficiency since:

1. The tokenizer is much smaller (~100MB) compared to the full model (~8GB)
2. Tokenization operations run faster without model overhead
3. Memory requirements are drastically reduced
4. Token analysis can be performed entirely with the tokenizer

### Updated Implementation Structure

1. Create a separate tokenizer-only loader function in `src/models/qwen3_loader.py`:

```python
def load_qwen3_tokenizer_only():
    model_id = "Qwen/Qwen3-4B"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    return tokenizer

def load_qwen3_model_and_tokenizer():
    """Only use this function for evaluation or prompt testing"""
    model_id = "Qwen/Qwen3-4B"
    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Load model with appropriate dtype
    dtype = torch.float16 if device in ["cuda", "mps"] else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=dtype,
        device_map=device
    )
    
    return model, tokenizer, device
```

2. Update the test script to demonstrate both approaches:

```python
from src.models.qwen3_loader import load_qwen3_tokenizer_only, load_qwen3_model_and_tokenizer, test_inference

if __name__ == "__main__":
    # For data preparation tasks (token extraction, analysis)
    print("Loading Qwen3-4B tokenizer only...")
    tokenizer = load_qwen3_tokenizer_only()
    print("Tokenizer loaded successfully!")
    
    # Example tokenization
    text = "Hello, world!"
    tokens = tokenizer.encode(text)
    decoded = tokenizer.decode(tokens)
    print(f"Tokenized '{text}' to {tokens}")
    print(f"Decoded back to: '{decoded}'")
    
    # Only for evaluation/testing (comment out when not needed)
    print("\nLoading full Qwen3-4B model (only needed for inference)...")
    model, full_tokenizer, device = load_qwen3_model_and_tokenizer()
    print("Model loaded successfully!")
    print("\nTesting basic inference...")
    test_inference(model, full_tokenizer, device)
```

### Documentation Updates

Add the following to project documentation:

- For all data preparation, token extraction, and analysis tasks, use ONLY the tokenizer
- Full model loading should be restricted to:
  - Final evaluation of filtered token sets
  - Testing prompt generation with filtered vocabulary
  - Benchmarking model performance with modified tokenizer

### Memory and Performance Benefits

- Tokenizer-only: ~100-200MB RAM usage
- Full model: 8GB+ RAM usage
- Tokenization speed: 5-10x faster without model loaded
- Startup time: Near-instant for tokenizer vs. 30+ seconds for model

This approach will be particularly important for the next subtask "Analyze and extract English-only tokens from Qwen3-4B tokenizer" which requires extensive tokenizer operations but no model inference.
</info added on 2025-05-10T10:03:45.695Z>

## 2. Analyze and extract English-only tokens from Qwen3-4B tokenizer [done]
### Dependencies: 14.1
### Description: Develop a methodology to identify and extract English tokens from the Qwen3-4B tokenizer, creating a filtered subset for English-only operations.
### Details:
Load the Qwen3-4B tokenizer and analyze its vocabulary. Develop criteria for identifying English tokens (e.g., using regex patterns, Unicode ranges, or language detection algorithms). Create a script that extracts and saves the English token subset. Document the extraction methodology, criteria used, and statistical analysis of the results (total tokens, percentage of English tokens, etc.).
<info added on 2025-05-10T10:14:27.786Z>
Load the Qwen3-4B tokenizer and analyze its vocabulary. Develop criteria for identifying English tokens (e.g., using regex patterns, Unicode ranges, or language detection algorithms). Create a script that extracts and saves the English token subset. Document the extraction methodology, criteria used, and statistical analysis of the results (total tokens, percentage of English tokens, etc.).

Implementation Plan:
1. Load the Qwen3-4B tokenizer using the existing function `load_qwen3_tokenizer_only()` in `src/models/qwen3_loader.py`.
2. Access the tokenizer's vocabulary via `tokenizer.get_vocab()`.
3. Develop criteria for identifying English tokens:
   - Use regex to match tokens containing only English alphabet characters (A-Z, a-z), common English punctuation, and optionally numbers.
   - Optionally, use Unicode ranges to further filter out non-English scripts.
   - Consider using the `regex` library for advanced pattern matching.
4. Write a script (suggested location: `scripts/extract_english_tokens.py`) that:
   - Loads the tokenizer
   - Iterates over the vocabulary
   - Applies the English-token criteria
   - Saves the filtered English tokens to a file (e.g., `data/processed/english_tokens_qwen3.txt`)
   - Outputs statistics: total tokens, number and percentage of English tokens
5. Document the methodology and criteria in the script docstring and output a summary report.
6. For validation, encode/decode a sample English sentence using both the full and filtered token sets, and compare results.

Special considerations:
- Handle subwords and tokens with special characters appropriately
- Account for byte-level or BPE encoding in the tokenizer
- Apply Unicode normalization for edge cases
- Ensure the filtered token set maintains coherence for English text processing
</info added on 2025-05-10T10:14:27.786Z>

## 3. Implement non-thinking mode support [done]
### Dependencies: 14.1
### Description: Update the inference logic to ensure Qwen3-4B is always used in non-thinking mode, following project policy.
### Details:
Modify the chat template application to explicitly set 'enable_thinking=False' in the tokenizer's apply_chat_template method. Create utility functions for generating responses that enforce non-thinking mode. Add safeguards to prevent accidental use of thinking mode. Document the implementation to make it clear that only non-thinking mode is supported per project policy.

## 4. Configure model-specific sampling parameters [done]
### Dependencies: 14.3
### Description: Implement and configure appropriate sampling parameters for Qwen3-4B in non-thinking mode operation.
### Details:
Create a configuration system for Qwen3-4B sampling parameters with reasonable defaults for non-thinking mode. Make these parameters configurable through API or configuration files. Implement parameter validation to ensure values are within acceptable ranges. Update the model generation code to apply these parameters during inference. Document recommended parameter settings for different use cases, all within non-thinking mode operation.
<info added on 2025-05-10T12:00:04.119Z>
Create a configuration system for Qwen3-4B sampling parameters with reasonable defaults for non-thinking mode. Make these parameters configurable through API or configuration files. Implement parameter validation to ensure values are within acceptable ranges. Update the model generation code to apply these parameters during inference. Document recommended parameter settings for different use cases, all within non-thinking mode operation.

Implementation plan based on codebase exploration and official Qwen3/Unsloth recommendations:

1. Add support for all recommended sampling parameters for Qwen3-4B non-thinking mode:
   - temperature: Controls randomness in generation (higher = more random)
   - top_p: Nucleus sampling parameter (cumulative probability threshold)
   - top_k: Limits vocabulary to top k tokens during sampling
   - min_p: Minimum probability filtering relative to the most likely token

2. Update the EvaluationConfig dataclass and YAML configuration format to include these parameters with the following defaults:
   - temperature: 0.6
   - top_p: 0.95
   - top_k: 20
   - min_p: 0.0

3. Modify the model inference code to explicitly pass all sampling parameters to model.generate() function calls.

4. Implement parameter validation logic to ensure all values fall within recommended ranges:
   - temperature: 0.0 to 2.0 (0.0 = deterministic)
   - top_p: 0.0 to 1.0
   - top_k: 0 to infinity (0 = disabled)
   - min_p: 0.0 to 1.0 (0.0 = disabled)

5. Add comprehensive documentation for each parameter in both code comments and configuration templates, with references to Unsloth and Qwen3 official documentation.

6. Ensure all implementation is strictly focused on non-thinking mode operation, maintaining alignment with project requirements.

References:
- Unsloth documentation: https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune
- Qwen3 example notebook: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb
</info added on 2025-05-10T12:00:04.119Z>

## 5. Integrate Qwen3-4B into existing codebase and document changes [done]
### Dependencies: 14.1, 14.2, 14.3, 14.4
### Description: Refactor the existing codebase to fully support Qwen3-4B in non-thinking mode, including model loading, configuration, and inference logic, with comprehensive documentation.
### Details:
Update model factory or loading mechanisms to recognize and properly initialize Qwen3-4B. Modify configuration schemas to include Qwen3-4B specific parameters (context length of 32,768, GQA attention heads configuration, etc.). Create migration guides for developers explaining how to transition to Qwen3-4B. Document all new features, parameters, and configuration options with examples, ensuring all documentation explicitly states that only non-thinking mode is supported. Update API documentation to reflect changes in behavior and capabilities.

## 6. Audit codebase for thinking mode references [done]
### Dependencies: 14.3, 14.4, 14.5
### Description: Perform a comprehensive audit of the codebase to ensure all thinking mode references are removed or disabled.
### Details:
Scan the entire codebase for any references to thinking mode, <think> blocks, or enable_thinking parameter. Create a checklist of all files and functions that need modification. Implement changes to ensure thinking mode is never enabled. Update comments, documentation, and variable names to reflect the non-thinking mode policy. Create automated tests that verify thinking mode is never enabled in any code path.
<info added on 2025-05-10T12:38:51.017Z>
The audit of the codebase for thinking mode references has been completed with the following findings:

1. Codebase Audit Results:
   - Conducted a comprehensive search across all code, documentation, and configuration files for:
     - `enable_thinking` parameter, config, or variable references
     - `<think>` blocks
     - "thinking mode" mentions in comments, documentation, or code

2. Key Findings:
   - All code paths, particularly in `src/models/qwen3_loader.py` and `src/evaluation/framework.py`, strictly enforce `enable_thinking=False`
   - Any attempt to use `enable_thinking=True` triggers a `ValueError` through the `apply_qwen3_chat_template_non_thinking` function
   - All `<think>` blocks and related logic have been completely removed
   - Documentation (README, docs/analysis.md, docs/templates.md, docs/data_format.md, docs/token_extraction.md) clearly states that only non-thinking mode is supported
   - All configuration files and example code explicitly set `enable_thinking=False`
   - No references to thinking mode remain except as warnings or policy clarifications

3. Files Audited:
   - `src/models/qwen3_loader.py` - Verified enforcement of non-thinking mode with error handling
   - `src/evaluation/framework.py` - Confirmed policy comments with no thinking mode logic
   - All documentation files - Updated to reflect non-thinking mode only policy
   - All task description files (`tasks/task_*.txt` and `tasks/tasks.json`) - Verified policy compliance

4. Conclusion:
   - The codebase is fully compliant with the non-thinking mode policy
   - No code, configuration, or documentation supports thinking mode except to explicitly prohibit it
   - All examples, comments, and documentation consistently reflect the project policy

5. Next Steps:
   - Continue monitoring for potential regressions through code review and CI processes
   - Proceed to the next subtask for removing GPT-2 references and completing migration to Qwen3-4B
</info added on 2025-05-10T12:38:51.017Z>

## 7. Remove GPT-2 references and fully migrate to Qwen3-4B [pending]
### Dependencies: None
### Description: Update all configuration files, scripts, and code to remove references to GPT-2 and ensure Qwen3-4B is the default model for all workflows. This includes updating the model name in configs, ensuring model loaders use the config value, updating documentation, and cleaning up any GPT-2-specific scripts or tokenizers.
### Details:
- Change model.name in configs/evaluation/base_config.yaml to 'Qwen/Qwen3-4B'.
- Update model loading code to use the config value, not hardcoded model names.
- Remove or refactor scripts that use GPT-2-specific tokenizers (e.g., src/data/token_extractor.py).
- Update documentation and comments to reference Qwen3-4B as the default model.
- Ensure all test and evaluation scripts work with Qwen3-4B.
- Clean up any remaining GPT-2 artifacts or dependencies.

