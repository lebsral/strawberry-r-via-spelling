# Task ID: 14
# Title: Migrate to Qwen3-4B Model with English Token Filtering and Mode Support
# Status: pending
# Dependencies: None
# Priority: high
# Description: Analyze and integrate the Qwen3-4B model into the codebase, extract English-only tokens from its tokenizer, and update model loading, configuration, and inference logic to support Qwen3-4B’s unique features, including thinking/non-thinking modes and specific sampling parameters.
# Details:
1. Review the Qwen3-4B model documentation and Hugging Face integration requirements, ensuring compatibility with the latest transformers library (>=4.51.0) to avoid loading errors.
2. Analyze the Qwen3-4B tokenizer to identify and extract the set of English-only tokens. Implement a script or utility to filter these tokens, documenting the extraction process and results.
3. Refactor the model loading and configuration code to support Qwen3-4B, including device mapping, torch_dtype, and context length settings as per model specs.
4. Update inference logic to handle Qwen3-4B’s thinking and non-thinking modes by leveraging the tokenizer’s `apply_chat_template` with `enable_thinking` parameter, and implement parsing logic to separate thinking content from final output (e.g., using the </think> token ID).
5. Ensure that sampling parameters for thinking mode are set as follows: Temperature=0.6, TopP=0.95, TopK=20, MinP=0. Make these configurable and document their usage.
6. Update documentation and code comments to reflect all changes, including migration steps, new configuration options, and any model-specific considerations.

# Test Strategy:
- Verify that the codebase loads Qwen3-4B without errors and that model inference works in both thinking and non-thinking modes, producing expected outputs.
- Confirm that the English-only token extraction utility correctly identifies and outputs the relevant tokens, and validate the results against known English token sets.
- Test that the sampling parameters are correctly applied during inference in thinking mode, and that outputs change as expected when parameters are varied.
- Ensure that the separation of thinking and content outputs works by running sample prompts and checking that both sections are parsed and displayed correctly.
- Review updated documentation for clarity and completeness, and perform code review to ensure maintainability and adherence to project standards.

# Subtasks:
## 1. Set up Qwen3-4B model with transformers library [done]
### Dependencies: None
### Description: Configure the development environment with the latest transformers library (>=4.51.0) and implement basic model loading for Qwen3-4B to ensure compatibility.
### Details:
Install or update transformers to version 4.51.0 or higher to avoid the 'KeyError: qwen3' error. Create a basic implementation to load the Qwen3-4B model using AutoModelForCausalLM and AutoTokenizer from the Hugging Face transformers library. Configure proper device mapping and torch_dtype settings for optimal performance. Test basic model loading and simple inference to verify setup.
<info added on 2025-05-10T09:22:09.195Z>
## Environment Setup
- Install or update transformers to version 4.51.0 or higher: `pip install --upgrade transformers>=4.51.0`
- Verify installation with: `import transformers; print(transformers.__version__)`
- Install any additional dependencies: `torch`, `accelerate` if needed

## Implementation Plan
1. Create implementation file at `src/models/qwen3_loader.py`
2. Use the following code structure:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def load_qwen3_model():
    model_id = "Qwen/Qwen3-4B"
    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Load model with appropriate dtype
    dtype = torch.float16 if device in ["cuda", "mps"] else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=dtype,
        device_map=device
    )
    
    return model, tokenizer, device

def test_inference(model, tokenizer, device):
    prompt = "Hello, world!"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    # Generate text
    outputs = model.generate(**inputs, max_new_tokens=20)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print(f"Input: {prompt}")
    print(f"Output: {generated_text}")
    return generated_text
```

3. Create a test script at `scripts/test_qwen3_loading.py`:
```python
from src.models.qwen3_loader import load_qwen3_model, test_inference

if __name__ == "__main__":
    print("Loading Qwen3-4B model...")
    model, tokenizer, device = load_qwen3_model()
    print("Model loaded successfully!")
    
    print("\nTesting basic inference...")
    test_inference(model, tokenizer, device)
```

## Potential Challenges
- Memory requirements: Qwen3-4B may require 8GB+ RAM
- If MPS/CUDA fails, fallback to CPU with `device_map="auto"` and `low_cpu_mem_usage=True`
- May need to use quantization for memory-constrained environments

## Success Criteria
- Model loads without errors
- Basic text generation works with a simple prompt
- Code is modular and reusable for the next subtask (token filtering)
</info added on 2025-05-10T09:22:09.195Z>
<info added on 2025-05-10T09:41:51.958Z>
Install or update transformers to version 4.51.0 or higher to avoid the 'KeyError: qwen3' error. Create a basic implementation to load the Qwen3-4B model using AutoModelForCausalLM and AutoTokenizer from the Hugging Face transformers library. Configure proper device mapping and torch_dtype settings for optimal performance. Test basic model loading and simple inference to verify setup.

<info added on 2025-05-10T09:22:09.195Z>
## Environment Setup
- Install or update transformers to version 4.51.0 or higher: `pip install --upgrade transformers>=4.51.0`
- Verify installation with: `import transformers; print(transformers.__version__)`
- Install any additional dependencies: `torch`, `accelerate` if needed

## Implementation Plan
1. Create implementation file at `src/models/qwen3_loader.py`
2. Use the following code structure:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def load_qwen3_model():
    model_id = "Qwen/Qwen3-4B"
    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Load model with appropriate dtype
    dtype = torch.float16 if device in ["cuda", "mps"] else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=dtype,
        device_map=device
    )
    
    return model, tokenizer, device

def test_inference(model, tokenizer, device):
    prompt = "Hello, world!"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    # Generate text
    outputs = model.generate(**inputs, max_new_tokens=20)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print(f"Input: {prompt}")
    print(f"Output: {generated_text}")
    return generated_text
```

3. Create a test script at `scripts/test_qwen3_loading.py`:
```python
from src.models.qwen3_loader import load_qwen3_model, test_inference

if __name__ == "__main__":
    print("Loading Qwen3-4B model...")
    model, tokenizer, device = load_qwen3_model()
    print("Model loaded successfully!")
    
    print("\nTesting basic inference...")
    test_inference(model, tokenizer, device)
```

## Potential Challenges
- Memory requirements: Qwen3-4B may require 8GB+ RAM
- If MPS/CUDA fails, fallback to CPU with `device_map="auto"` and `low_cpu_mem_usage=True`
- May need to use quantization for memory-constrained environments

## Success Criteria
- Model loads without errors
- Basic text generation works with a simple prompt
- Code is modular and reusable for the next subtask (token filtering)
</info added on 2025-05-10T09:22:09.195Z>

## Multi-Environment Implementation Strategy

For this project, we'll use different environments for different stages of the workflow:

1. **Local Development & Data Preparation**: Use Hugging Face transformers
   - Model ID: 'Qwen/Qwen3-4B'
   - Use for all data preparation, tokenization analysis, and validation
   - Ideal for token extraction and filtering work in the next subtask
   - Provides full access to tokenizer internals needed for English token filtering

2. **Quantized Inference**: Use Ollama
   - Model: 'qwen:4b'
   - Implement using the Ollama Python API
   - Example code:
   ```python
   import ollama
   
   response = ollama.chat(model='qwen:4b', 
                         messages=[{'role': 'user', 'content': 'Hello, world!'}])
   print(response['message']['content'])
   ```
   - Advantages: Lower memory footprint, faster inference on consumer hardware

3. **Fine-tuning**: Use Unsloth
   - Environment: Google Colab or Lightning.ai (NOT on Mac)
   - Provides optimized fine-tuning for Qwen models
   - Warning: Do not attempt to install or use Unsloth or xformers on Mac systems

4. **Implementation Notes**:
   - Create separate modules for each environment
   - Use factory pattern to abstract model interactions
   - Document environment-specific requirements in README.md
   - Create environment-specific setup scripts for each workflow
</info added on 2025-05-10T09:41:51.958Z>
<info added on 2025-05-10T10:03:45.695Z>
## Tokenizer-Only Approach for Data Tasks

For data preparation and token extraction tasks, we should only load the Qwen3-4B tokenizer without loading the full model. This approach significantly improves efficiency since:

1. The tokenizer is much smaller (~100MB) compared to the full model (~8GB)
2. Tokenization operations run faster without model overhead
3. Memory requirements are drastically reduced
4. Token analysis can be performed entirely with the tokenizer

### Updated Implementation Structure

1. Create a separate tokenizer-only loader function in `src/models/qwen3_loader.py`:

```python
def load_qwen3_tokenizer_only():
    model_id = "Qwen/Qwen3-4B"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    return tokenizer

def load_qwen3_model_and_tokenizer():
    """Only use this function for evaluation or prompt testing"""
    model_id = "Qwen/Qwen3-4B"
    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Load model with appropriate dtype
    dtype = torch.float16 if device in ["cuda", "mps"] else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=dtype,
        device_map=device
    )
    
    return model, tokenizer, device
```

2. Update the test script to demonstrate both approaches:

```python
from src.models.qwen3_loader import load_qwen3_tokenizer_only, load_qwen3_model_and_tokenizer, test_inference

if __name__ == "__main__":
    # For data preparation tasks (token extraction, analysis)
    print("Loading Qwen3-4B tokenizer only...")
    tokenizer = load_qwen3_tokenizer_only()
    print("Tokenizer loaded successfully!")
    
    # Example tokenization
    text = "Hello, world!"
    tokens = tokenizer.encode(text)
    decoded = tokenizer.decode(tokens)
    print(f"Tokenized '{text}' to {tokens}")
    print(f"Decoded back to: '{decoded}'")
    
    # Only for evaluation/testing (comment out when not needed)
    print("\nLoading full Qwen3-4B model (only needed for inference)...")
    model, full_tokenizer, device = load_qwen3_model_and_tokenizer()
    print("Model loaded successfully!")
    print("\nTesting basic inference...")
    test_inference(model, full_tokenizer, device)
```

### Documentation Updates

Add the following to project documentation:

- For all data preparation, token extraction, and analysis tasks, use ONLY the tokenizer
- Full model loading should be restricted to:
  - Final evaluation of filtered token sets
  - Testing prompt generation with filtered vocabulary
  - Benchmarking model performance with modified tokenizer

### Memory and Performance Benefits

- Tokenizer-only: ~100-200MB RAM usage
- Full model: 8GB+ RAM usage
- Tokenization speed: 5-10x faster without model loaded
- Startup time: Near-instant for tokenizer vs. 30+ seconds for model

This approach will be particularly important for the next subtask "Analyze and extract English-only tokens from Qwen3-4B tokenizer" which requires extensive tokenizer operations but no model inference.
</info added on 2025-05-10T10:03:45.695Z>

## 2. Analyze and extract English-only tokens from Qwen3-4B tokenizer [pending]
### Dependencies: 14.1
### Description: Develop a methodology to identify and extract English tokens from the Qwen3-4B tokenizer, creating a filtered subset for English-only operations.
### Details:
Load the Qwen3-4B tokenizer and analyze its vocabulary. Develop criteria for identifying English tokens (e.g., using regex patterns, Unicode ranges, or language detection algorithms). Create a script that extracts and saves the English token subset. Document the extraction methodology, criteria used, and statistical analysis of the results (total tokens, percentage of English tokens, etc.).

## 3. Implement thinking/non-thinking mode support [pending]
### Dependencies: 14.1
### Description: Update the inference logic to handle Qwen3-4B's thinking and non-thinking modes, including parsing logic for separating thinking content from final output.
### Details:
Modify the chat template application to use the 'enable_thinking' parameter in the tokenizer's apply_chat_template method. Implement parsing logic to identify and separate thinking content from final output using the </think> token ID (151668). Create utility functions to extract thinking content and final response from generation results. Add configuration options to enable/disable thinking mode based on use case requirements.

## 4. Configure model-specific sampling parameters [pending]
### Dependencies: 14.3
### Description: Implement and configure the specific sampling parameters required for Qwen3-4B, particularly for thinking mode operation.
### Details:
Create a configuration system for Qwen3-4B sampling parameters with defaults set to: Temperature=0.6, TopP=0.95, TopK=20, MinP=0 for thinking mode. Make these parameters configurable through API or configuration files. Implement parameter validation to ensure values are within acceptable ranges. Update the model generation code to apply these parameters during inference.

## 5. Integrate Qwen3-4B into existing codebase and document changes [pending]
### Dependencies: 14.1, 14.2, 14.3, 14.4
### Description: Refactor the existing codebase to fully support Qwen3-4B, including model loading, configuration, and inference logic, with comprehensive documentation.
### Details:
Update model factory or loading mechanisms to recognize and properly initialize Qwen3-4B. Modify configuration schemas to include Qwen3-4B specific parameters (context length of 32,768, GQA attention heads configuration, etc.). Create migration guides for developers explaining how to transition to Qwen3-4B. Document all new features, parameters, and configuration options with examples. Update API documentation to reflect changes in behavior and capabilities.

