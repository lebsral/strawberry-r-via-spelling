# Task ID: 10
# Title: Model Publishing and Documentation
# Status: pending
# Dependencies: 9
# Priority: medium
# Description: Prepare the final model, documentation, and publish to Hugging Face with comprehensive model card.
# Details:
1. Prepare model card documentation
2. Create detailed README for the project
3. Upload the best model to Hugging Face
4. Ensure dataset is properly published
5. Create final report with results and findings
6. Organize documentation in the specified file structure

NOTE: If publishing Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local publishing is only for CPU-compatible models.

File Structure:
- API docs: `docs/api.md`
- Deployment guide: `docs/deployment.md`
- Monitoring guide: `docs/monitoring.md`

Implementation:
```python
from huggingface_hub import HfApi
import os
import json
import yaml

# Create model card
def create_model_card(evaluation_results, config):
    base_metrics = evaluation_results["base_model"]["metrics"]
    finetuned_metrics = evaluation_results["finetuned_model"]["metrics"]
    
    # Calculate improvements
    improvements = {}
    for metric in base_metrics.keys():
        if metric == "levenshtein_distance":
            # Lower is better for Levenshtein
            improvements[metric] = ((base_metrics[metric] - finetuned_metrics[metric]) / base_metrics[metric]) * 100
        else:
            # Higher is better for accuracy metrics
            improvements[metric] = ((finetuned_metrics[metric] - base_metrics[metric]) / base_metrics[metric]) * 100
    
    model_card = f"""---
language: en
tags:
- gpt2
- spelling
- lora
- fine-tuning
- unsloth
datasets:
- YOUR-USERNAME/llm-spelling-dataset
metrics:
- accuracy
license: mit
---

# GPT-2 Spelling Fine-tuned Model

This model is a fine-tuned version of [gpt2](https://huggingface.co/gpt2) on spelling tasks. It was fine-tuned using LoRA adapters to improve the model's understanding of letter count and character position via spelling mechanics.

## Model description

This model was fine-tuned to address the "strawberry problem" - improving a language model's ability to answer questions about letter counts and positions in words without explicitly training on those tasks. Instead, the model was trained on spelling tasks, which implicitly teaches it to understand character-level patterns.

### Training hyperparameters

- LoRA rank (r): {config['lora_config']['r']}
- LoRA alpha: {config['lora_config']['alpha']}
- Learning rate: {config['training_config']['learning_rate']}
- Batch size: {config['training_config']['per_device_train_batch_size']}
- Gradient accumulation steps: {config['training_config']['gradient_accumulation_steps']}
- Training steps: {config['training_config']['max_steps']}

## Evaluation results

### Base model performance
- Letter Count Accuracy: {base_metrics['letter_count_accuracy']:.4f}
- Letter Position Accuracy: {base_metrics['letter_position_accuracy']:.4f}
- Character-Level Accuracy: {base_metrics['character_level_accuracy']:.4f}
- Normalized Levenshtein Distance: {base_metrics['levenshtein_distance']:.4f}

### Fine-tuned model performance
- Letter Count Accuracy: {finetuned_metrics['letter_count_accuracy']:.4f}
- Letter Position Accuracy: {finetuned_metrics['letter_position_accuracy']:.4f}
- Character-Level Accuracy: {finetuned_metrics['character_level_accuracy']:.4f}
- Normalized Levenshtein Distance: {finetuned_metrics['levenshtein_distance']:.4f}

### Improvements
- Letter Count Accuracy: {improvements['letter_count_accuracy']:.2f}%
- Letter Position Accuracy: {improvements['letter_position_accuracy']:.2f}%
- Character-Level Accuracy: {improvements['character_level_accuracy']:.2f}%
- Normalized Levenshtein Distance: {improvements['levenshtein_distance']:.2f}%

## Usage

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load model and tokenizer
model = GPT2LMHeadModel.from_pretrained("YOUR-USERNAME/gpt2-spelling-lora")
tokenizer = GPT2Tokenizer.from_pretrained("YOUR-USERNAME/gpt2-spelling-lora")

# Example questions
questions = [
    "How many r's are in the word 'strawberry'?",
    "What is the 3rd letter in 'strawberry'?"
]

# Generate answers
for question in questions:
    inputs = tokenizer(question, return_tensors="pt")
    outputs = model.generate(inputs.input_ids, max_length=50)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Question: {question}")
    print(f"Answer: {answer}\n")
```

## Training procedure

### Training data

The model was trained on a dataset of spelling examples generated from GPT-2 tokens. The training data included various template formats for spelling examples to maximize generalization.

The validation and test sets were created from English dictionary words that were not in the training set, ensuring proper evaluation of the model's ability to generalize to new words.

### Training method

The model was fine-tuned using LoRA (Low-Rank Adaptation) with Unsloth optimizations. This approach allows for efficient fine-tuning with minimal memory requirements while maintaining performance.

## Limitations and bias

This model is specifically fine-tuned for spelling tasks and may not perform well on other tasks. It is also limited by the vocabulary of the base GPT-2 model and may struggle with rare or complex words.

## License

This model is licensed under the MIT License.
"""
    
    return model_card

# Create README
def create_readme(evaluation_results, config):
    base_metrics = evaluation_results["base_model"]["metrics"]
    finetuned_metrics = evaluation_results["finetuned_model"]["metrics"]
    
    readme = f"""# LLM Strawberry Problem Solution via Fine-tuning of Spelling

## Project Overview

This project addresses the "strawberry problem" in language models - improving a model's ability to answer questions about letter counts and positions in words without explicitly training on those tasks. Instead, the model is trained on spelling tasks, which implicitly teaches it to understand character-level patterns.

## Approach

We fine-tuned a GPT-2 language model using LoRA adapters with the following approach:

1. Extracted multi-character, letter-based tokens from the GPT-2 tokenizer vocabulary
2. Created a dataset of spelling examples with various template formats
3. Split the data into training, validation, and test sets
4. Fine-tuned the model using Unsloth optimizations
5. Evaluated the model on letter count and position tasks

## Results

### Base model performance
- Letter Count Accuracy: {base_metrics['letter_count_accuracy']:.4f}
- Letter Position Accuracy: {base_metrics['letter_position_accuracy']:.4f}

### Fine-tuned model performance
- Letter Count Accuracy: {finetuned_metrics['letter_count_accuracy']:.4f}
- Letter Position Accuracy: {finetuned_metrics['letter_position_accuracy']:.4f}

## Repository Structure

```
.
├── data/                 # Data files and datasets
├── notebooks/            # Jupyter notebooks for experimentation
├── src/                  # Source code
│   ├── api/              # API implementation
│   │   ├── app.py        # FastAPI application
│   │   ├── routes/       # API routes
│   │   ├── services/     # Model services
│   │   └── utils/        # Utility functions
├── deployment/           # Deployment configurations
│   ├── docker-compose.yml # Docker compose configuration
│   ├── env/              # Environment configurations
│   ├── k8s/              # Kubernetes manifests
│   └── monitoring/       # Monitoring configurations
│       ├── prometheus/   # Prometheus configuration
│       ├── grafana/      # Grafana dashboards
│       └── alerts/       # Alert rules
├── tests/                # Test files
│   ├── api/              # API tests
│   ├── load/             # Load tests
│   ├── integration/      # Integration tests
│   └── data/             # Test data
├── docs/                 # Documentation
│   ├── api.md            # API documentation
│   ├── deployment.md     # Deployment guide
│   └── monitoring.md     # Monitoring guide
├── configs/              # Configuration files
├── results/              # Experimental results and visualizations
├── checkpoints/          # Model checkpoints
├── README.md             # This file
└── requirements.txt      # Python dependencies
```

## Setup and Installation

```bash
# Clone the repository
git clone https://github.com/YOUR-USERNAME/llm-spelling-finetuning.git
cd llm-spelling-finetuning

# Install dependencies with uv
curl -fsSL https://astral.sh/uv/install.sh | bash
uv pip install -r requirements.txt
```

## Usage

### Training

```bash
python src/train.py --config configs/experiment_config.yaml
```

### Evaluation

```bash
python src/evaluate.py --model_path checkpoints/best_model
```

### API Usage

Start the API server:
```bash
uvicorn src.api.app:app --reload
```

See the API documentation at `docs/api.md` for detailed usage instructions.

## Deployment

Refer to `docs/deployment.md` for detailed deployment instructions using Docker or Kubernetes.

## Monitoring

Refer to `docs/monitoring.md` for information on monitoring the deployed model using Prometheus and Grafana.

## Resources

- [Fine-tuned Model on Hugging Face](https://huggingface.co/YOUR-USERNAME/gpt2-spelling-lora)
- [Dataset on Hugging Face](https://huggingface.co/datasets/YOUR-USERNAME/llm-spelling-dataset)
- [Experiment Tracking on W&B](https://wandb.ai/YOUR-USERNAME/llm-spelling-finetuning)

## License

This project is licensed under the MIT License.
"""
    
    return readme

# Publish model to Hugging Face
def publish_to_hugging_face(model_path, model_card, readme, evaluation_results):
    # Initialize Hugging Face API
    api = HfApi()
    
    # Create repository if it doesn't exist
    repo_id = "YOUR-USERNAME/gpt2-spelling-lora"
    try:
        api.create_repo(repo_id=repo_id, private=False)
    except Exception as e:
        print(f"Repository already exists or error: {e}")
    
    # Save model card
    with open(os.path.join(model_path, "README.md"), "w") as f:
        f.write(model_card)
    
    # Save evaluation results
    with open(os.path.join(model_path, "evaluation_results.json"), "w") as f:
        json.dump(evaluation_results, f, indent=2)
    
    # Upload model to Hugging Face
    api.upload_folder(
        folder_path=model_path,
        repo_id=repo_id,
        commit_message="Upload fine-tuned spelling model"
    )
    
    # Create project README
    with open("README.md", "w") as f:
        f.write(readme)
    
    print(f"Model published to Hugging Face: https://huggingface.co/{repo_id}")
    
    return repo_id

# Main publishing function
def publish_final_model(best_model_path, evaluation_results_path, config_path):
    # Load evaluation results
    with open(evaluation_results_path, "r") as f:
        evaluation_results = json.load(f)
    
    # Load config
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    
    # Create model card
    model_card = create_model_card(evaluation_results, config)
    
    # Create README
    readme = create_readme(evaluation_results, config)
    
    # Publish to Hugging Face
    repo_id = publish_to_hugging_face(best_model_path, model_card, readme, evaluation_results)
    
    # Create final report
    create_final_report(evaluation_results, config, repo_id)
    
    # Create additional documentation
    create_additional_docs()
    
    return repo_id

# Create final report
def create_final_report(evaluation_results, config, repo_id):
    # Create a comprehensive final report with all results and findings
    report = {
        "project_name": "LLM Strawberry Problem Solution via Fine-tuning of Spelling",
        "model_repository": f"https://huggingface.co/{repo_id}",
        "dataset_repository": "https://huggingface.co/datasets/YOUR-USERNAME/llm-spelling-dataset",
        "configuration": config,
        "evaluation_results": evaluation_results,
        "conclusion": {
            "success_criteria_met": {
                "letter_count_improvement": evaluation_results["finetuned_model"]["metrics"]["letter_count_accuracy"] > 
                                          evaluation_results["base_model"]["metrics"]["letter_count_accuracy"],
                "letter_position_improvement": evaluation_results["finetuned_model"]["metrics"]["letter_position_accuracy"] > 
                                              evaluation_results["base_model"]["metrics"]["letter_position_accuracy"],
                "properly_documented": True,
                "uploaded_to_hugging_face": True,
                "tracked_in_wandb": True
            }
        }
    }
    
    # Save report
    with open("final_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print("Final report created: final_report.json")
    
    return report

# Create additional documentation
def create_additional_docs():
    # Ensure docs directory exists
    os.makedirs("docs", exist_ok=True)
    
    # Create API documentation
    api_doc = """# API Documentation

## Overview

This API provides access to the fine-tuned spelling model for inference and evaluation.

## Endpoints

### GET /health

Health check endpoint to verify the API is running.

**Response:**
```json
{
  "status": "ok",
  "version": "1.0.0"
}
```

### POST /predict

Make predictions using the fine-tuned model.

**Request:**
```json
{
  "text": "How many r's are in the word 'strawberry'?"
}
```

**Response:**
```json
{
  "input": "How many r's are in the word 'strawberry'?",
  "output": "There are 2 r's in the word 'strawberry'.",
  "processing_time": 0.125
}
```

### GET /metrics

Get model performance metrics.

**Response:**
```json
{
  "letter_count_accuracy": 0.85,
  "letter_position_accuracy": 0.78,
  "character_level_accuracy": 0.92,
  "levenshtein_distance": 0.15
}
```

## Error Handling

All endpoints return standard HTTP status codes:
- 200: Success
- 400: Bad request (invalid input)
- 500: Server error

Error responses include a message field with details:
```json
{
  "error": "Invalid input text",
  "details": "Text field cannot be empty"
}
```
"""
    
    with open("docs/api.md", "w") as f:
        f.write(api_doc)
    
    # Create deployment documentation
    deployment_doc = """# Deployment Guide

## Docker Deployment

### Prerequisites

- Docker and Docker Compose installed
- Access to the model files

### Steps

1. Build the Docker image:
   ```bash
   docker build -t spelling-model-api ./deployment/
   ```

2. Run with Docker Compose:
   ```bash
   docker-compose -f deployment/docker-compose.yml up -d
   ```

3. Verify the deployment:
   ```bash
   curl http://localhost:8000/health
   ```

## Kubernetes Deployment

### Prerequisites

- Kubernetes cluster
- kubectl configured
- Container registry access

### Steps

1. Push the Docker image to a registry:
   ```bash
   docker tag spelling-model-api:latest your-registry/spelling-model-api:latest
   docker push your-registry/spelling-model-api:latest
   ```

2. Apply Kubernetes manifests:
   ```bash
   kubectl apply -f deployment/k8s/
   ```

3. Verify the deployment:
   ```bash
   kubectl get pods
   kubectl get services
   ```

## Environment Configuration

The application can be configured using environment variables defined in `deployment/env/` files:

- `MODEL_PATH`: Path to the model files
- `API_KEY`: API key for authentication (if enabled)
- `LOG_LEVEL`: Logging level (debug, info, warning, error)
- `MAX_BATCH_SIZE`: Maximum batch size for inference

## Scaling

### Horizontal Scaling

For Kubernetes deployments, you can scale the number of replicas:

```bash
kubectl scale deployment spelling-model-api --replicas=3
```

### Resource Allocation

Adjust CPU and memory resources in the Kubernetes manifests based on your workload requirements.
"""
    
    with open("docs/deployment.md", "w") as f:
        f.write(deployment_doc)
    
    # Create monitoring documentation
    monitoring_doc = """# Monitoring Guide

## Overview

This guide describes how to monitor the deployed spelling model API using Prometheus and Grafana.

## Metrics Collection

### Prometheus Configuration

The API exposes metrics at the `/metrics` endpoint in Prometheus format. Configure Prometheus to scrape these metrics by adding the following to your `prometheus.yml`:

```yaml
scrape_configs:
  - job_name: 'spelling-model-api'
    scrape_interval: 15s
    static_configs:
      - targets: ['spelling-model-api:8000']
```

A complete Prometheus configuration is available at `deployment/monitoring/prometheus/prometheus.yml`.

## Dashboards

### Grafana Setup

1. Add Prometheus as a data source in Grafana
2. Import the pre-built dashboards from `deployment/monitoring/grafana/`

### Available Dashboards

- **API Overview**: General API metrics (request rate, latency, errors)
- **Model Performance**: Inference time, batch size, memory usage
- **System Resources**: CPU, memory, and network usage

## Alerting

### Alert Rules

Predefined alert rules are available in `deployment/monitoring/alerts/` and include:

- High error rate (>5% of requests)
- High latency (p95 > 500ms)
- Resource constraints (CPU > 80%, memory > 80%)

### Alert Configuration

To configure alerts with Alertmanager:

1. Deploy Alertmanager using the configuration in `deployment/monitoring/alertmanager/`
2. Configure notification channels (email, Slack, PagerDuty)
3. Apply the alert rules

## Logging

Logs are output in JSON format and can be collected using Fluentd, Logstash, or similar tools.

Key log fields:
- `timestamp`: Log timestamp
- `level`: Log level (info, warning, error)
- `message`: Log message
- `request_id`: Unique ID for request tracing
- `component`: Component generating the log

## Tracing

The API supports distributed tracing with OpenTelemetry. Configure your tracing backend (Jaeger, Zipkin) using the environment variables in `deployment/env/`.
"""
    
    with open("docs/monitoring.md", "w") as f:
        f.write(monitoring_doc)
    
    print("Additional documentation created in docs/ directory")
```

# Test Strategy:
1. Verify model card is comprehensive and follows Hugging Face guidelines
2. Confirm README provides clear instructions for using the model
3. Test model upload to Hugging Face
4. Verify dataset is properly published and accessible
5. Check that final report includes all required information
6. Test model loading from Hugging Face
7. Verify all success criteria from the PRD are met and documented
8. For Unsloth or GPU-fine-tuned models, verify the publishing process works in a cloud GPU environment
9. Validate that all documentation files are created in the correct locations
10. Test API documentation against actual API implementation
11. Verify deployment instructions work in both Docker and Kubernetes environments
12. Test monitoring setup with Prometheus and Grafana
13. Ensure all file paths in documentation match the actual project structure

# Subtasks:
## 1. Model Card Creation [pending]
### Dependencies: None
### Description: Create a comprehensive model card following Hugging Face guidelines with metadata and detailed sections
### Details:
Develop a model card as a Markdown file with YAML metadata section. Include: model description, intended uses & limitations, training parameters, datasets used, evaluation results, biases and ethical considerations. Follow the structure from Mitchell, 2018 paper and use the Hugging Face template. Ensure all metadata supports discovery (license, datasets, language identifiers).

## 2. Project README and Documentation [pending]
### Dependencies: None
### Description: Prepare comprehensive project documentation including installation, usage examples, and technical details
### Details:
Create a detailed README.md for the project repository (separate from the model card). Include: project overview, installation instructions, dependency requirements, usage examples with code snippets, architecture diagrams, limitations, and acknowledgments. Document the preprocessing and postprocessing steps to ensure reproducibility. Add inline code comments and generate API documentation if applicable.

Organize documentation in the specified file structure:
- API docs: `docs/api.md`
- Deployment guide: `docs/deployment.md`
- Monitoring guide: `docs/monitoring.md`

## 3. Hugging Face Model Publishing and Verification [pending]
### Dependencies: 10.1
### Description: Publish the model to Hugging Face Hub and verify its functionality
### Details:
Use the huggingface_hub library to upload the model, tokenizer, and model card. Configure model tags, set appropriate visibility settings, and verify the model card renders correctly. Test the uploaded model with sample inference code to ensure it works as expected. Validate that all metadata is correctly displayed on the model page and that links to datasets are functional.

NOTE: If publishing Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local publishing is only for CPU-compatible models.

## 4. Final Report Generation [pending]
### Dependencies: 10.1, 10.2, 10.3
### Description: Create a comprehensive report summarizing the model development, performance, and publication process
### Details:
Generate a final report documenting the entire model development lifecycle. Include: executive summary, methodology, training process details, evaluation metrics with visualizations, comparison to baseline models, limitations discovered during testing, deployment considerations, and future improvement recommendations. Format as a professional document with proper citations and appendices for detailed results.

## 5. Cloud Environment Setup for Model Publishing [pending]
### Dependencies: None
### Description: Configure cloud GPU environment for publishing Unsloth or GPU-fine-tuned models
### Details:
Set up a cloud GPU environment (Google Colab or https://lightning.ai/lars/home) for publishing models that require GPU resources. Create a notebook or script that handles authentication with Hugging Face, loads the model from local storage or cloud storage, and publishes it to the Hugging Face Hub. Include clear instructions for users on how to use this environment for model publishing. Test the workflow to ensure it works seamlessly with Unsloth-optimized models.

## 6. API Documentation Creation [pending]
### Dependencies: 10.2
### Description: Create detailed API documentation for model inference endpoints
### Details:
Develop comprehensive API documentation in `docs/api.md` that includes:
- Endpoint descriptions and usage examples
- Request/response formats with JSON examples
- Authentication requirements
- Error handling and status codes
- Rate limiting information
- Performance considerations

Ensure documentation matches the actual implementation in `src/api/app.py` and `src/api/routes/`.

## 7. Deployment Documentation [pending]
### Dependencies: 10.2
### Description: Create deployment guide for Docker and Kubernetes environments
### Details:
Develop a detailed deployment guide in `docs/deployment.md` covering:
- Docker deployment instructions
- Kubernetes deployment configuration
- Environment variable configuration
- Resource requirements and scaling recommendations
- Security considerations

Reference the actual configuration files in `deployment/docker-compose.yml` and `deployment/k8s/`.

## 8. Monitoring Documentation [pending]
### Dependencies: 10.2
### Description: Create monitoring guide for Prometheus and Grafana setup
### Details:
Develop a monitoring guide in `docs/monitoring.md` that includes:
- Prometheus configuration for metrics collection
- Grafana dashboard setup and import instructions
- Alert configuration with Alertmanager
- Log collection and analysis recommendations
- Performance monitoring best practices

Reference the actual configuration files in `deployment/monitoring/`.

