# Task ID: 6
# Title: Hyperparameter Tuning Infrastructure
# Status: pending
# Dependencies: 1, 5
# Priority: medium
# Description: Create a configuration system for hyperparameter experiments and set up experiment tracking with Weights & Biases.
# Details:
1. Create a configuration system for hyperparameter experiments
2. Set up a Jupyter notebook for experiment tracking
3. Create a script that can run training with different hyperparameters
4. Set up W&B for experiment tracking
5. Define a clear set of metrics for comparing experiments

File Structure:
- Base configs: `configs/hyperparameters/`
- Model configs: `configs/hyperparameters/models/`
- Training configs: `configs/hyperparameters/training/`
- Search space definitions: `configs/hyperparameters/search_spaces/`
- Config manager: `src/training/config_manager.py`
- W&B integration: `src/training/wandb_logger.py`
- Grid search: `src/training/grid_search.py`
- Experiment runner: `src/training/experiment_runner.py`
- Experiment results: `results/hyperparameter_tuning/`
- Best configurations: `results/hyperparameter_tuning/best_configs/`
- Performance plots: `results/hyperparameter_tuning/plots/`
- Comparison reports: `results/hyperparameter_tuning/reports/`
- Tuning analysis notebook: `notebooks/hyperparameter_analysis.ipynb`
- Results comparison notebook: `notebooks/tuning_comparison.ipynb`
- Documentation: `docs/hyperparameter_tuning.md`, `docs/config_system.md`, `docs/tuning_results.md`

Implementation:
```python
import yaml
import os
from datetime import datetime
import wandb

def create_experiment_config(
    exp_name,
    lora_r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    learning_rate=2e-4,
    batch_size=8,
    grad_accum_steps=4,
    max_steps=1000,
    warmup_steps=100,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
):
    """Create and save an experiment configuration."""
    config = {
        "experiment_name": exp_name,
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "lora_config": {
            "r": lora_r,
            "alpha": lora_alpha,
            "dropout": lora_dropout,
            "target_modules": target_modules,
        },
        "training_config": {
            "learning_rate": learning_rate,
            "per_device_train_batch_size": batch_size,
            "gradient_accumulation_steps": grad_accum_steps,
            "max_steps": max_steps,
            "warmup_steps": warmup_steps,
        },
    }

    # Create experiments directory if it doesn't exist
    os.makedirs("configs/hyperparameters/", exist_ok=True)

    # Save config to file
    config_path = f"configs/hyperparameters/{exp_name}_{config['timestamp']}.yaml"
    with open(config_path, "w") as f:
        yaml.dump(config, f)

    print(f"Created experiment config: {config_path}")
    return config_path

# Define hyperparameter grid
def create_hyperparameter_grid():
    grid = {
        "lora_r": [4, 8, 16, 32],
        "lora_alpha": [8, 16, 32, 64],
        "learning_rate": [1e-4, 2e-4, 5e-4, 1e-3],
        "batch_size": [4, 8, 16, 32],
        "grad_accum_steps": [1, 2, 4, 8],
        "max_steps": [500, 1000, 2000, 5000]
    }
    return grid

# Create experiment configs for grid search
def create_grid_search_configs(base_name="experiment"):
    grid = create_hyperparameter_grid()
    configs = []
    
    # Start with default configuration
    configs.append(create_experiment_config(f"{base_name}_default"))
    
    # Create configs for each hyperparameter variation
    for param, values in grid.items():
        for value in values:
            # Skip the default value
            if param == "lora_r" and value == 16:
                continue
            if param == "lora_alpha" and value == 32:
                continue
            if param == "learning_rate" and value == 2e-4:
                continue
            if param == "batch_size" and value == 8:
                continue
            if param == "grad_accum_steps" and value == 4:
                continue
            if param == "max_steps" and value == 1000:
                continue
                
            kwargs = {param: value}
            config_path = create_experiment_config(f"{base_name}_{param}_{value}", **kwargs)
            configs.append(config_path)
    
    return configs

# Initialize W&B sweep
def create_wandb_sweep():
    sweep_config = {
        "method": "grid",
        "metric": {"name": "validation_letter_position_accuracy", "goal": "maximize"},
        "parameters": {
            "lora_r": {"values": [4, 8, 16, 32]},
            "lora_alpha": {"values": [8, 16, 32, 64]},
            "learning_rate": {"values": [1e-4, 2e-4, 5e-4, 1e-3]},
            "batch_size": {"values": [4, 8, 16, 32]},
            "grad_accum_steps": {"values": [1, 2, 4, 8]},
            "max_steps": {"values": [500, 1000, 2000, 5000]}
        }
    }
    
    sweep_id = wandb.sweep(sweep_config, project="llm-spelling-finetuning")
    return sweep_id
```

# Test Strategy:
1. Verify configuration system creates valid YAML files in the correct directories (`configs/hyperparameters/`)
2. Confirm W&B experiment tracking is properly set up through `src/training/wandb_logger.py`
3. Test that the hyperparameter grid generates the expected number of configurations
4. Verify W&B sweep configuration is valid
5. Test the experiment notebooks: `notebooks/hyperparameter_analysis.ipynb` and `notebooks/tuning_comparison.ipynb`
6. Ensure metrics for comparing experiments are clearly defined
7. Verify that results are properly saved to `results/hyperparameter_tuning/` directories
8. Test the experiment runner to ensure it correctly uses configurations and logs results

# Subtasks:
## 1. Configuration System Design and Implementation [pending]
### Dependencies: None
### Description: Design and implement a flexible configuration system for hyperparameter management
### Details:
Create a configuration framework that supports defining, validating, and loading hyperparameter configurations. Implement serialization/deserialization of configurations to JSON/YAML formats. Design a hierarchical configuration structure that allows for inheritance and overrides. Include validation mechanisms to ensure hyperparameter values fall within acceptable ranges. Support both discrete values (HPARAM_CANDIDATES) and continuous ranges (HPARAM_RANGE) for different hyperparameter types.

## 2. Experiment Tracking Setup with W&B Integration [pending]
### Dependencies: 6.1
### Description: Implement experiment tracking infrastructure with Weights & Biases integration
### Details:
Set up W&B project structure for hyperparameter experiments. Implement logging mechanisms for metrics, hyperparameters, and model artifacts in `src/training/wandb_logger.py`. Create utilities for experiment initialization, updating, and finalization. Design a consistent naming convention for experiments. Implement automatic synchronization between local experiment state and W&B. Add support for experiment grouping and comparison within the W&B interface.

## 3. Hyperparameter Grid Definition and Validation [pending]
### Dependencies: 6.1
### Description: Create a system for defining and validating hyperparameter search spaces
### Details:
Implement a framework for defining hyperparameter search spaces including random, grid, and Bayesian optimization strategies in `src/training/grid_search.py`. Create validation mechanisms to ensure search spaces are properly defined. Support both continuous ranges and discrete value sets for different hyperparameter types. Implement utilities for sampling from defined search spaces. Add functionality to estimate the total number of trials based on the search space definition. Create interfaces for custom search space definitions. Store search space definitions in `configs/hyperparameters/search_spaces/`.

## 4. Experiment Execution Framework [pending]
### Dependencies: 6.1, 6.2, 6.3
### Description: Build a framework for executing hyperparameter tuning experiments
### Details:
Implement a job scheduler in `src/training/experiment_runner.py` for running multiple trials with different hyperparameter configurations. Create mechanisms for early stopping of underperforming trials. Design parallel execution capabilities to utilize available computational resources efficiently. Implement checkpointing and resumption of interrupted experiments. Add support for distributed training across multiple machines. Create a monitoring system for active experiments with real-time status updates. Save experiment results to `results/hyperparameter_tuning/` with best configurations in `results/hyperparameter_tuning/best_configs/`.

## 5. Results Comparison and Visualization System [pending]
### Dependencies: 6.2, 6.4
### Description: Develop tools for comparing and visualizing hyperparameter tuning results
### Details:
Create visualization tools for comparing metrics across different hyperparameter configurations. Implement automated analysis to identify the most influential hyperparameters. Design interactive dashboards for exploring the hyperparameter search space in `notebooks/hyperparameter_analysis.ipynb` and `notebooks/tuning_comparison.ipynb`. Add functionality to export comparison reports to `results/hyperparameter_tuning/reports/`. Implement statistical analysis tools to evaluate the significance of performance differences. Create recommendation system for suggesting optimal hyperparameter configurations for future experiments. Generate performance plots in `results/hyperparameter_tuning/plots/`.

## 6. Documentation and User Guides [pending]
### Dependencies: 6.1, 6.2, 6.3, 6.4, 6.5
### Description: Create comprehensive documentation for the hyperparameter tuning system
### Details:
Develop detailed documentation covering the hyperparameter tuning system in `docs/hyperparameter_tuning.md`. Create a configuration guide explaining the structure and usage of the configuration system in `docs/config_system.md`. Write a results analysis guide detailing how to interpret and utilize tuning results in `docs/tuning_results.md`. Include examples, best practices, and troubleshooting information in all documentation.

