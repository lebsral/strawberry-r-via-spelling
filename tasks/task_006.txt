# Task ID: 6
# Title: Hyperparameter Tuning Infrastructure
# Status: pending
# Dependencies: 1, 5
# Priority: medium
# Description: Create a configuration system for hyperparameter experiments and set up experiment tracking with Weights & Biases using Python scripts instead of notebooks.
# Details:
1. Create a configuration system for hyperparameter experiments
2. Set up Python scripts for experiment tracking
3. Create a script that can run training with different hyperparameters
4. Set up W&B for experiment tracking
5. Define a clear set of metrics for comparing experiments

File Structure:
- Base configs: `configs/hyperparameters/`
- Model configs: `configs/hyperparameters/models/`
- Training configs: `configs/hyperparameters/training/`
- Search space definitions: `configs/hyperparameters/search_spaces/`

Python Module Structure:
- Config manager: `src/tuning/config.py`
- W&B integration: `src/tuning/wandb_integration.py`
- Grid search: `src/tuning/grid.py`
- Experiment executor: `src/tuning/executor.py`
- Visualization tools: `src/tuning/visualization.py`
- Report generation: `src/tuning/report.py`

Results Structure:
- Experiment results: `results/tuning/data/`
- Best configurations: `results/tuning/configs/`
- Performance plots: `results/tuning/figures/`
- HTML reports: `results/tuning/reports/`
- Documentation: `docs/hyperparameter_tuning.md`, `docs/config_system.md`, `docs/tuning_results.md`

Implementation:
```python
import yaml
import os
from datetime import datetime
import wandb
import argparse

def create_experiment_config(
    exp_name,
    lora_r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    learning_rate=2e-4,
    batch_size=8,
    grad_accum_steps=4,
    max_steps=1000,
    warmup_steps=100,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
):
    """Create and save an experiment configuration."""
    config = {
        "experiment_name": exp_name,
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "lora_config": {
            "r": lora_r,
            "alpha": lora_alpha,
            "dropout": lora_dropout,
            "target_modules": target_modules,
        },
        "training_config": {
            "learning_rate": learning_rate,
            "per_device_train_batch_size": batch_size,
            "gradient_accumulation_steps": grad_accum_steps,
            "max_steps": max_steps,
            "warmup_steps": warmup_steps,
        },
    }

    # Create experiments directory if it doesn't exist
    os.makedirs("configs/hyperparameters/", exist_ok=True)

    # Save config to file
    config_path = f"configs/hyperparameters/{exp_name}_{config['timestamp']}.yaml"
    with open(config_path, "w") as f:
        yaml.dump(config, f)

    print(f"Created experiment config: {config_path}")
    return config_path

# Define hyperparameter grid
def create_hyperparameter_grid():
    grid = {
        "lora_r": [4, 8, 16, 32],
        "lora_alpha": [8, 16, 32, 64],
        "learning_rate": [1e-4, 2e-4, 5e-4, 1e-3],
        "batch_size": [4, 8, 16, 32],
        "grad_accum_steps": [1, 2, 4, 8],
        "max_steps": [500, 1000, 2000, 5000]
    }
    return grid

# Create experiment configs for grid search
def create_grid_search_configs(base_name="experiment"):
    grid = create_hyperparameter_grid()
    configs = []
    
    # Start with default configuration
    configs.append(create_experiment_config(f"{base_name}_default"))
    
    # Create configs for each hyperparameter variation
    for param, values in grid.items():
        for value in values:
            # Skip the default value
            if param == "lora_r" and value == 16:
                continue
            if param == "lora_alpha" and value == 32:
                continue
            if param == "learning_rate" and value == 2e-4:
                continue
            if param == "batch_size" and value == 8:
                continue
            if param == "grad_accum_steps" and value == 4:
                continue
            if param == "max_steps" and value == 1000:
                continue
                
            kwargs = {param: value}
            config_path = create_experiment_config(f"{base_name}_{param}_{value}", **kwargs)
            configs.append(config_path)
    
    return configs

# Initialize W&B sweep
def create_wandb_sweep():
    sweep_config = {
        "method": "grid",
        "metric": {"name": "validation_letter_position_accuracy", "goal": "maximize"},
        "parameters": {
            "lora_r": {"values": [4, 8, 16, 32]},
            "lora_alpha": {"values": [8, 16, 32, 64]},
            "learning_rate": {"values": [1e-4, 2e-4, 5e-4, 1e-3]},
            "batch_size": {"values": [4, 8, 16, 32]},
            "grad_accum_steps": {"values": [1, 2, 4, 8]},
            "max_steps": {"values": [500, 1000, 2000, 5000]}
        }
    }
    
    sweep_id = wandb.sweep(sweep_config, project="llm-spelling-finetuning")
    return sweep_id

# Command-line interface for experiment execution
def main():
    parser = argparse.ArgumentParser(description="Run hyperparameter tuning experiments")
    parser.add_argument("--mode", choices=["grid", "sweep", "single"], default="single",
                        help="Experiment mode: grid search, W&B sweep, or single experiment")
    parser.add_argument("--name", type=str, default="experiment",
                        help="Base name for the experiment")
    parser.add_argument("--config", type=str, help="Path to a specific config file (for single mode)")
    
    args = parser.parse_args()
    
    if args.mode == "grid":
        configs = create_grid_search_configs(args.name)
        print(f"Created {len(configs)} configurations for grid search")
    elif args.mode == "sweep":
        sweep_id = create_wandb_sweep()
        print(f"Created W&B sweep with ID: {sweep_id}")
    elif args.mode == "single":
        if args.config:
            print(f"Using provided config: {args.config}")
        else:
            config_path = create_experiment_config(args.name)
            print(f"Created single experiment config: {config_path}")

if __name__ == "__main__":
    main()
```

# Test Strategy:
1. Verify configuration system creates valid YAML files in the correct directories (`configs/hyperparameters/`)
2. Confirm W&B experiment tracking is properly set up through `src/tuning/wandb_integration.py`
3. Test that the hyperparameter grid generates the expected number of configurations
4. Verify W&B sweep configuration is valid
5. Test the command-line interfaces for all Python scripts
6. Ensure metrics for comparing experiments are clearly defined
7. Verify that results are properly saved to `results/tuning/` directories
8. Test the experiment executor to ensure it correctly uses configurations and logs results
9. Verify HTML report generation produces valid and informative reports
10. Test visualization tools to ensure they generate the expected figures

# Subtasks:
## 1. Configuration System Design and Implementation [pending]
### Dependencies: None
### Description: Design and implement a flexible configuration system for hyperparameter management
### Details:
Create a configuration framework that supports defining, validating, and loading hyperparameter configurations. Implement serialization/deserialization of configurations to JSON/YAML formats. Design a hierarchical configuration structure that allows for inheritance and overrides. Include validation mechanisms to ensure hyperparameter values fall within acceptable ranges. Support both discrete values (HPARAM_CANDIDATES) and continuous ranges (HPARAM_RANGE) for different hyperparameter types.

## 2. Experiment Tracking Setup with W&B Integration [pending]
### Dependencies: 6.1
### Description: Implement experiment tracking infrastructure with Weights & Biases integration
### Details:
Set up W&B project structure for hyperparameter experiments. Implement logging mechanisms for metrics, hyperparameters, and model artifacts in `src/tuning/wandb_integration.py`. Create utilities for experiment initialization, updating, and finalization. Design a consistent naming convention for experiments. Implement automatic synchronization between local experiment state and W&B. Add support for experiment grouping and comparison within the W&B interface.

## 3. Hyperparameter Grid Definition and Validation [pending]
### Dependencies: 6.1
### Description: Create a system for defining and validating hyperparameter search spaces
### Details:
Implement a framework for defining hyperparameter search spaces including random, grid, and Bayesian optimization strategies in `src/tuning/grid.py`. Create validation mechanisms to ensure search spaces are properly defined. Support both continuous ranges and discrete value sets for different hyperparameter types. Implement utilities for sampling from defined search spaces. Add functionality to estimate the total number of trials based on the search space definition. Create interfaces for custom search space definitions. Store search space definitions in `configs/hyperparameters/search_spaces/`.

## 4. Experiment Execution Framework [pending]
### Dependencies: 6.1, 6.2, 6.3
### Description: Build a framework for executing hyperparameter tuning experiments
### Details:
Implement a job scheduler in `src/tuning/executor.py` for running multiple trials with different hyperparameter configurations. Create mechanisms for early stopping of underperforming trials. Design parallel execution capabilities to utilize available computational resources efficiently. Implement checkpointing and resumption of interrupted experiments. Add support for distributed training across multiple machines. Create a monitoring system for active experiments with real-time status updates. Save experiment results to `results/tuning/data/` with best configurations in `results/tuning/configs/`. Implement command-line interfaces for flexible experiment execution.

## 5. Results Visualization and Reporting System [pending]
### Dependencies: 6.2, 6.4
### Description: Develop tools for visualizing and reporting hyperparameter tuning results
### Details:
Create visualization tools in `src/tuning/visualization.py` for comparing metrics across different hyperparameter configurations. Implement automated analysis to identify the most influential hyperparameters. Design HTML report generation in `src/tuning/report.py` for exploring the hyperparameter search space. Add functionality to export comparison reports to `results/tuning/reports/`. Implement statistical analysis tools to evaluate the significance of performance differences. Create recommendation system for suggesting optimal hyperparameter configurations for future experiments. Generate performance plots in `results/tuning/figures/`.

## 6. Documentation and User Guides [pending]
### Dependencies: 6.1, 6.2, 6.3, 6.4, 6.5
### Description: Create comprehensive documentation for the hyperparameter tuning system
### Details:
Develop detailed documentation covering the hyperparameter tuning system in `docs/hyperparameter_tuning.md`. Create a configuration guide explaining the structure and usage of the configuration system in `docs/config_system.md`. Write a results analysis guide detailing how to interpret and utilize tuning results in `docs/tuning_results.md`. Include examples, best practices, and troubleshooting information in all documentation. Document command-line interfaces and provide usage examples for all scripts.

## 7. Python Package Structure and Testing [pending]
### Dependencies: 6.1, 6.2, 6.3, 6.4, 6.5
### Description: Implement proper Python packaging and testing for the tuning infrastructure
### Details:
Organize the tuning code as a proper Python package with appropriate imports and dependencies. Create unit tests for each component of the tuning infrastructure. Implement integration tests to verify the end-to-end workflow. Set up continuous integration for automated testing. Create a requirements.txt or setup.py file to manage dependencies. Ensure compatibility with the rest of the codebase. Add type hints and docstrings for better code documentation.

