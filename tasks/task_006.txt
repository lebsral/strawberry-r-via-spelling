# Task ID: 6
# Title: Hyperparameter Tuning Infrastructure
# Status: pending
# Dependencies: 1, 5
# Priority: medium
# Description: Create a configuration system for hyperparameter experiments and set up experiment tracking with Weights & Biases.
# Details:
1. Create a configuration system for hyperparameter experiments
2. Set up a Jupyter notebook for experiment tracking
3. Create a script that can run training with different hyperparameters
4. Set up W&B for experiment tracking
5. Define a clear set of metrics for comparing experiments

Implementation:
```python
import yaml
import os
from datetime import datetime
import wandb

def create_experiment_config(
    exp_name,
    lora_r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    learning_rate=2e-4,
    batch_size=8,
    grad_accum_steps=4,
    max_steps=1000,
    warmup_steps=100,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
):
    """Create and save an experiment configuration."""
    config = {
        "experiment_name": exp_name,
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "lora_config": {
            "r": lora_r,
            "alpha": lora_alpha,
            "dropout": lora_dropout,
            "target_modules": target_modules,
        },
        "training_config": {
            "learning_rate": learning_rate,
            "per_device_train_batch_size": batch_size,
            "gradient_accumulation_steps": grad_accum_steps,
            "max_steps": max_steps,
            "warmup_steps": warmup_steps,
        },
    }

    # Create experiments directory if it doesn't exist
    os.makedirs("experiments", exist_ok=True)

    # Save config to file
    config_path = f"experiments/{exp_name}_{config['timestamp']}.yaml"
    with open(config_path, "w") as f:
        yaml.dump(config, f)

    print(f"Created experiment config: {config_path}")
    return config_path

# Define hyperparameter grid
def create_hyperparameter_grid():
    grid = {
        "lora_r": [4, 8, 16, 32],
        "lora_alpha": [8, 16, 32, 64],
        "learning_rate": [1e-4, 2e-4, 5e-4, 1e-3],
        "batch_size": [4, 8, 16, 32],
        "grad_accum_steps": [1, 2, 4, 8],
        "max_steps": [500, 1000, 2000, 5000]
    }
    return grid

# Create experiment configs for grid search
def create_grid_search_configs(base_name="experiment"):
    grid = create_hyperparameter_grid()
    configs = []
    
    # Start with default configuration
    configs.append(create_experiment_config(f"{base_name}_default"))
    
    # Create configs for each hyperparameter variation
    for param, values in grid.items():
        for value in values:
            # Skip the default value
            if param == "lora_r" and value == 16:
                continue
            if param == "lora_alpha" and value == 32:
                continue
            if param == "learning_rate" and value == 2e-4:
                continue
            if param == "batch_size" and value == 8:
                continue
            if param == "grad_accum_steps" and value == 4:
                continue
            if param == "max_steps" and value == 1000:
                continue
                
            kwargs = {param: value}
            config_path = create_experiment_config(f"{base_name}_{param}_{value}", **kwargs)
            configs.append(config_path)
    
    return configs

# Initialize W&B sweep
def create_wandb_sweep():
    sweep_config = {
        "method": "grid",
        "metric": {"name": "validation_letter_position_accuracy", "goal": "maximize"},
        "parameters": {
            "lora_r": {"values": [4, 8, 16, 32]},
            "lora_alpha": {"values": [8, 16, 32, 64]},
            "learning_rate": {"values": [1e-4, 2e-4, 5e-4, 1e-3]},
            "batch_size": {"values": [4, 8, 16, 32]},
            "grad_accum_steps": {"values": [1, 2, 4, 8]},
            "max_steps": {"values": [500, 1000, 2000, 5000]}
        }
    }
    
    sweep_id = wandb.sweep(sweep_config, project="llm-spelling-finetuning")
    return sweep_id
```

# Test Strategy:
1. Verify configuration system creates valid YAML files
2. Confirm W&B experiment tracking is properly set up
3. Test that the hyperparameter grid generates the expected number of configurations
4. Verify W&B sweep configuration is valid
5. Create and test the experiment notebook
6. Ensure metrics for comparing experiments are clearly defined

# Subtasks:
## 1. Configuration System Design and Implementation [pending]
### Dependencies: None
### Description: Design and implement a flexible configuration system for hyperparameter management
### Details:
Create a configuration framework that supports defining, validating, and loading hyperparameter configurations. Implement serialization/deserialization of configurations to JSON/YAML formats. Design a hierarchical configuration structure that allows for inheritance and overrides. Include validation mechanisms to ensure hyperparameter values fall within acceptable ranges. Support both discrete values (HPARAM_CANDIDATES) and continuous ranges (HPARAM_RANGE) for different hyperparameter types.

## 2. Experiment Tracking Setup with W&B Integration [pending]
### Dependencies: 6.1
### Description: Implement experiment tracking infrastructure with Weights & Biases integration
### Details:
Set up W&B project structure for hyperparameter experiments. Implement logging mechanisms for metrics, hyperparameters, and model artifacts. Create utilities for experiment initialization, updating, and finalization. Design a consistent naming convention for experiments. Implement automatic synchronization between local experiment state and W&B. Add support for experiment grouping and comparison within the W&B interface.

## 3. Hyperparameter Grid Definition and Validation [pending]
### Dependencies: 6.1
### Description: Create a system for defining and validating hyperparameter search spaces
### Details:
Implement a framework for defining hyperparameter search spaces including random, grid, and Bayesian optimization strategies. Create validation mechanisms to ensure search spaces are properly defined. Support both continuous ranges and discrete value sets for different hyperparameter types. Implement utilities for sampling from defined search spaces. Add functionality to estimate the total number of trials based on the search space definition. Create interfaces for custom search space definitions.

## 4. Experiment Execution Framework [pending]
### Dependencies: 6.1, 6.2, 6.3
### Description: Build a framework for executing hyperparameter tuning experiments
### Details:
Implement a job scheduler for running multiple trials with different hyperparameter configurations. Create mechanisms for early stopping of underperforming trials. Design parallel execution capabilities to utilize available computational resources efficiently. Implement checkpointing and resumption of interrupted experiments. Add support for distributed training across multiple machines. Create a monitoring system for active experiments with real-time status updates.

## 5. Results Comparison and Visualization System [pending]
### Dependencies: 6.2, 6.4
### Description: Develop tools for comparing and visualizing hyperparameter tuning results
### Details:
Create visualization tools for comparing metrics across different hyperparameter configurations. Implement automated analysis to identify the most influential hyperparameters. Design interactive dashboards for exploring the hyperparameter search space. Add functionality to export comparison reports in various formats. Implement statistical analysis tools to evaluate the significance of performance differences. Create recommendation system for suggesting optimal hyperparameter configurations for future experiments.

