{
  "tasks": [
    {
      "id": 1,
      "title": "Development Environment Setup",
      "description": "Set up the Python development environment with all required dependencies using uv for package management.",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "details": "1. Install uv package manager: `curl -fsSL https://astral.sh/uv/install.sh | bash`\n2. Create project directory structure following the repository structure in the PRD\n3. Install dependencies directly using uv commands instead of editing requirements.txt:\n   - `uv pip install torch transformers datasets wandb unsloth dspy lightning matplotlib seaborn pandas jupyter notebook ipywidgets`\n4. After installation, generate requirements.txt for documentation: `uv pip freeze > requirements.txt`\n5. Set up Git repository with proper .gitignore\n6. Configure Weights & Biases account: `wandb login`\n7. Set up Hugging Face account and API access: `huggingface-cli login`\n8. Create initial Jupyter notebook for experimentation\n9. Verify all imports work correctly",
      "testStrategy": "1. Verify all libraries install without errors using uv\n2. Confirm successful authentication with W&B\n3. Confirm successful authentication with Hugging Face\n4. Test import of all required libraries in a Jupyter notebook\n5. Make initial commit with environment setup completed\n6. Verify environment reproducibility by creating a new environment using the generated requirements.txt",
      "subtasks": [
        {
          "id": 1,
          "title": "Installation Phase",
          "description": "Set up the development environment by installing all necessary software, tools, and dependencies using uv package manager",
          "dependencies": [],
          "details": "Install uv package manager first, then use it to install all required libraries directly with 'uv pip install' commands. Document all installation paths and versions for future reference. Use uv for all Python package management to ensure reproducibility and faster installation times.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Configuration Phase",
          "description": "Configure all installed components to work together properly and set up authentication for external services",
          "dependencies": [
            1
          ],
          "details": "Set environment variables, configure IDE settings, establish database connections, set up version control repositories, configure build tools, and establish authentication for any external services or APIs required for development. Generate requirements.txt using 'uv pip freeze > requirements.txt' for documentation purposes.\n<info added on 2025-05-07T14:26:08.046Z>\nSet environment variables, configure IDE settings, establish database connections, set up version control repositories, configure build tools, and establish authentication for any external services or APIs required for development. Generate requirements.txt using 'uv pip freeze > requirements.txt' for documentation purposes.\n\nCreate a proper environment configuration by duplicating the .env.example file to .env:\n1. Copy the .env.example file to .env using the command: `cp .env.example .env` (Unix/Mac) or `copy .env.example .env` (Windows)\n2. Open the newly created .env file and fill in all required values\n3. Ensure all environment variables are properly set according to your local development environment\n4. Document any custom environment variables added to the project in the .env.example file with appropriate comments\n5. Verify that sensitive information (API keys, passwords, etc.) is not committed to version control\n6. Update the project documentation to include information about required environment variables\n</info added on 2025-05-07T14:26:08.046Z>",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Verification Phase",
          "description": "Test the complete development environment to ensure all components work together properly",
          "dependencies": [
            2
          ],
          "details": "Run test scripts to verify installations, validate configurations, test connections to external services, perform a sample build process, and document any issues encountered along with their resolutions. Test environment reproducibility by creating a new environment using the generated requirements.txt and uv.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Create and Maintain README.md for Developer Onboarding",
          "description": "Develop and maintain a comprehensive README.md file to help new developers set up the environment, understand the project structure, and follow best practices.",
          "details": "1. Write a clear project overview and purpose.\n2. Document the setup process, including using uv for environment management, installing dependencies, and configuring the .env file.\n3. Explain the directory structure and the role of key files (e.g., requirements.txt, .env, .env.example, notebooks, scripts).\n4. Provide instructions for running Jupyter notebooks and scripts.\n5. List common troubleshooting tips and FAQs.\n6. Include contribution guidelines and code style references.\n7. Update the README.md as the project evolves to ensure accuracy and completeness.",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 1
        }
      ]
    },
    {
      "id": 2,
      "title": "Token Extraction from GPT-2 Vocabulary",
      "description": "Extract all multi-character, letter-based tokens from the GPT-2 tokenizer vocabulary and save them to a JSON file.",
      "details": "1. Load the GPT-2 tokenizer from Hugging Face\n2. Extract all tokens from the vocabulary\n3. Filter tokens to include only multi-character, letter-based tokens\n4. Save the filtered token list to a JSON file with the structure specified in the PRD\n5. Create a Jupyter notebook to verify token selection\n6. Analyze token frequency and length distribution\n\nImplementation:\n```python\nfrom transformers import GPT2Tokenizer\nimport json\nimport re\n\ndef extract_tokens():\n    # Load tokenizer\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n    # Extract and filter tokens\n    filtered_tokens = []\n    for token_id, token_text in tokenizer.get_vocab().items():\n        # Remove special tokens and decode byte tokens\n        decoded_token = tokenizer.convert_tokens_to_string([token_text])\n\n        # Filter for multi-character letter-based tokens\n        if len(decoded_token) > 1 and re.match(r'^[a-zA-Z]+$', decoded_token):\n            filtered_tokens.append({\n                \"token\": decoded_token,\n                \"token_id\": token_id,\n                \"char_length\": len(decoded_token)\n            })\n\n    # Save to file\n    with open(\"gpt2_letter_tokens.json\", \"w\") as f:\n        json.dump({\"tokens\": filtered_tokens}, f, indent=2)\n\n    return filtered_tokens\n\ntokens = extract_tokens()\nprint(f\"Extracted {len(tokens)} multi-character letter-based tokens\")\n```",
      "testStrategy": "1. Verify JSON file is successfully created\n2. Confirm file contains at least 5,000 tokens\n3. Randomly sample tokens to confirm they are multi-character and letter-based\n4. Verify no special tokens (like <|endoftext|>) are included\n5. Create visualizations of token length distribution\n6. Commit token extraction script and results",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Dataset Creation and Splitting",
      "description": "Create training, validation, and test datasets for spelling tasks, ensuring proper separation between training tokens and validation/test words to evaluate if training on spelling improves model performance on position and count question metrics.",
      "status": "pending",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "1. Download English word list from dwyl/english-words repository\n2. Create training set from tokenizer vocabulary (multi-character, letter-only tokens)\n3. Create validation/test sets from English dictionary words that:\n   - Split into multiple tokens by the tokenizer\n   - Have at least one token in the split that is multi-character and letter-only\n   - Do not appear in the training set\n4. Generate letter count questions (\"How many X's are in Y?\")\n5. Generate letter position questions (\"What is the Nth letter in Y?\")\n6. Split data based on source rather than percentage: training from tokenizer vocabulary, validation/test from filtered external word lists\n7. Format as a Hugging Face dataset with appropriate splits\n8. Create notebook to visualize dataset statistics\n9. Establish a Hugging Face benchmark with evaluation scripts and leaderboard integration\n\nImplementation:\n```python\nimport json\nimport random\nimport string\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nimport requests\n\n# Download English word list\nword_list_url = \"https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt\"\nresponse = requests.get(word_list_url)\nall_words = response.text.splitlines()\n\n# Load tokenizer and filtered tokens\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nwith open(\"gpt2_letter_tokens.json\", \"r\") as f:\n    tokens_data = json.load(f)\n\ntokens = [t[\"token\"] for t in tokens_data[\"tokens\"]]\n\n# Find valid validation/test words\nvalid_words = []\nfor word in all_words:\n    if not word.isalpha():\n        continue\n    \n    # Tokenize the word\n    token_ids = tokenizer.encode(word, add_special_tokens=False)\n    tokens_in_word = tokenizer.convert_ids_to_tokens(token_ids)\n    \n    # Check if word splits into multiple tokens with at least one multi-character token\n    if len(tokens_in_word) > 1 and any(len(tokenizer.convert_tokens_to_string([t])) > 1 for t in tokens_in_word):\n        # Ensure word is not in training set\n        if word.lower() not in [t.lower() for t in tokens]:\n            valid_words.append(word)\n\n# Split valid words into validation and test sets\nval_words, test_words = train_test_split(valid_words, test_size=0.5, random_state=42)\n\n# Generate questions\ndef generate_questions(words, question_type):\n    questions = []\n    for i, word in enumerate(words):\n        if question_type == \"letter_count\":\n            letter = random.choice(string.ascii_lowercase)\n            count = word.lower().count(letter)\n            questions.append({\n                \"id\": f\"{question_type}_{i}\",\n                \"question\": f\"How many {letter}'s are in the word '{word}'?\",\n                \"answer\": str(count),\n                \"question_type\": question_type,\n                \"word\": word\n            })\n        elif question_type == \"letter_position\":\n            position = random.randint(1, len(word))\n            letter = word[position-1]\n            questions.append({\n                \"id\": f\"{question_type}_{i}\",\n                \"question\": f\"What is the {position}{'st' if position == 1 else 'nd' if position == 2 else 'rd' if position == 3 else 'th'} letter in the word '{word}'?\",\n                \"answer\": letter,\n                \"question_type\": question_type,\n                \"word\": word\n            })\n    return questions\n\n# Generate training, validation, and test questions\ntrain_questions = generate_questions(tokens, \"letter_count\") + generate_questions(tokens, \"letter_position\")\nval_questions = generate_questions(val_words, \"letter_count\") + generate_questions(val_words, \"letter_position\")\ntest_questions = generate_questions(test_words, \"letter_count\") + generate_questions(test_words, \"letter_position\")\n\n# Create datasets\ndatasets = {}\nfor split_name, split_data in [\n    (\"train\", train_questions),\n    (\"validation\", val_questions),\n    (\"test\", test_questions)\n]:\n    datasets[split_name] = Dataset.from_dict({\n        \"id\": [q[\"id\"] for q in split_data],\n        \"question\": [q[\"question\"] for q in split_data],\n        \"answer\": [q[\"answer\"] for q in split_data],\n        \"question_type\": [q[\"question_type\"] for q in split_data],\n        \"word\": [q[\"word\"] for q in split_data]\n    })\n\n# Push to Hugging Face\nfrom datasets import DatasetDict\ncombined_dataset = DatasetDict(datasets)\ncombined_dataset.push_to_hub(\"YOUR-USERNAME/llm-spelling-dataset\")\n```",
      "testStrategy": "1. Verify dataset successfully generates 2,000+ questions\n2. Confirm questions are grammatically correct\n3. Verify train/validation/test splits come from appropriate sources (training from tokenizer vocabulary, validation/test from filtered external words)\n4. Manually check 20 random samples to ensure answers correctly match questions\n5. Confirm dataset is successfully pushed to Hugging Face\n6. Verify local JSON files are created for each split\n7. Create and review notebook exploring dataset statistics\n8. Test evaluation scripts to ensure they correctly measure performance on position and count question metrics\n9. Verify benchmark integration with Hugging Face leaderboard",
      "subtasks": [
        {
          "id": 1,
          "title": "Word List Acquisition",
          "description": "Gather a comprehensive and diverse word list from reliable sources, ensuring coverage of the desired vocabulary scope.",
          "dependencies": [],
          "details": "Implementation involves sourcing words from open datasets, dictionaries, or APIs. Validation criteria include checking for duplicates, ensuring language consistency, and verifying word authenticity. Challenges include handling noisy data, inconsistent formats, and ensuring the list is representative of the target domain.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Training/Validation/Test Set Creation with Filtering",
          "description": "Create distinct datasets from different sources: training set from tokenizer vocabulary and validation/test sets from filtered external word lists to establish a true holdout set for testing.",
          "dependencies": [
            1
          ],
          "details": "Implementation requires extracting tokenizer vocabulary for training and applying strict filtering criteria to external word lists for validation/test sets. Ensure no overlap between training tokens and validation/test words. Validation involves statistical checks for distribution balance and manual spot checks for leakage. Challenges include maintaining diversity across splits and implementing robust filtering logic.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Question Generation for Each Type",
          "description": "Automatically generate letter count and letter position questions for each word in the dataset to establish metrics for evaluating model performance.",
          "dependencies": [
            2
          ],
          "details": "Implementation uses templates to generate questions per word for both letter count and letter position types. Validation includes checking for grammatical correctness, relevance, and uniqueness of questions. Challenges involve ensuring variety in question phrasing and scaling generation efficiently across the universal set and holdout set.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Dataset Formatting and Splitting",
          "description": "Format the generated data according to Hugging Face requirements, ensuring proper structure and metadata for each split based on their distinct sources.",
          "dependencies": [
            3
          ],
          "details": "Implementation involves structuring data as JSON, CSV, or other required formats, with clear fields for input, output, and metadata. Validation checks include schema compliance, correct split assignments, and tokenization compatibility. Challenges include handling edge cases in formatting and ensuring compatibility with downstream tools.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Dataset Publishing and Benchmark Creation",
          "description": "Publish the finalized dataset to Hugging Face and establish a benchmark with evaluation scripts and leaderboard integration.",
          "dependencies": [
            4
          ],
          "details": "Implementation includes uploading dataset files, creating evaluation scripts that measure performance on position and count question metrics, integrating with Hugging Face leaderboard, and writing detailed documentation. Validation involves verifying downloadability, documentation clarity, and reproducibility. Challenges include ensuring evaluation scripts accurately reflect the experiment's purpose of determining if training on spelling improves model performance.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Universal Set and Holdout Set Verification",
          "description": "Verify that the training set (universal set) and test set (true holdout set) are properly separated to enable valid measurement of model performance improvements.",
          "dependencies": [
            2
          ],
          "details": "Implementation involves comprehensive checks to ensure no overlap between training tokens and test words. Create verification scripts to confirm the integrity of the splits. Validation includes statistical analysis of word distributions and characteristics across splits. Challenges include defining appropriate metrics to verify the splits serve the experiment's purpose.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 4,
      "title": "Training Data Formatting with Template Variations",
      "description": "Format the training data using various template formats for spelling examples to maximize LLM generalization and token-awareness.",
      "details": "1. Create a script to format the training data for fine-tuning\n2. Implement the template variations specified in the PRD, including:\n   - Simple variations (spelling first)\n   - Narrative/playful versions (spelling first)\n   - Educational/formal tone (spelling first)\n   - Spoken word/emphatic style (spelling first)\n   - Simple variations (word first)\n   - Narrative/playful versions (word first)\n   - Educational/formal tone (word first)\n   - Spoken word/emphatic style (word first)\n   - LLM-friendly structured training format (no \"spell\")\n3. Include additional variations for token separation:\n   - No separator between tokens\n   - Arrows between tokens\n   - Various punctuation and formatting\n4. Create a Jupyter notebook to visualize example prompts and responses\n5. Implement efficient DataLoader with proper batching\n\nImplementation:\n```python\ndef format_training_examples(dataset):\n    formatted_examples = []\n    \n    # Template categories\n    templates = {\n        \"spelling_first_simple\": [\n            \"s t r a w — that spells '{word}.'\\n\",\n            \"The letters s, t, r, a, w spell the word '{word}.'\\n\",\n            \"s-t-r-a-w makes the word '{word}.'\\n\",\n            \"Put together, s t r a w spells {word}.\\n\",\n            \"When you combine s, t, r, a, and w, you get {word}.\\n\"\n        ],\n        \"spelling_first_playful\": [\n            \"Say it with me: s...t...r...a...w — {word}!\\n\",\n            \"Five little letters — s, t, r, a, w — team up to make '{word}.'\\n\",\n            \"You line up s, t, r, a, and w, and what do you get? {word}!\\n\",\n            \"It starts with an 's' and ends with a 'w' — that's '{word}.'\\n\",\n            \"One letter at a time: s, t, r, a, w. Together? {word}.\\n\"\n        ],\n        # Add all other template categories from the PRD\n    }\n    \n    # Token separation styles\n    separators = [\n        \"\", # No separator\n        \" \", # Space\n        \", \", # Comma and space\n        \"-\", # Dash\n        \"...\", # Triple dots\n        \" -> \" # Arrow\n    ]\n    \n    for example in dataset:\n        word = example[\"word\"]\n        letters = list(word)\n        \n        # Randomly select template category and template\n        category = random.choice(list(templates.keys()))\n        template = random.choice(templates[category])\n        \n        # Randomly select separator\n        separator = random.choice(separators)\n        \n        # Format the letters with the chosen separator\n        spelled_letters = separator.join(letters)\n        \n        # Format the example using the template\n        formatted_text = template.format(word=word, letters=spelled_letters)\n        \n        formatted_examples.append({\n            \"input\": formatted_text,\n            \"output\": word,\n            \"template_category\": category,\n            \"separator\": separator\n        })\n    \n    return formatted_examples\n\n# Create custom collation function for efficient batching\ndef custom_collate_fn(batch):\n    input_ids = [item[\"input_ids\"] for item in batch]\n    attention_mask = [item[\"attention_mask\"] for item in batch]\n\n    # Pad sequences to the maximum length in the batch\n    max_length = max(len(ids) for ids in input_ids)\n\n    # Pad input_ids and attention_mask\n    input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n    attention_mask = [mask + [0] * (max_length - len(mask)) for mask in attention_mask]\n\n    # Convert to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_mask = torch.tensor(attention_mask)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask\n    }\n```",
      "testStrategy": "1. Verify script runs without errors\n2. Confirm dataset contains all template variations specified in the PRD\n3. Check that examples use a mix of punctuation and formatting\n4. Ensure no template is over-represented\n5. Create and review example notebook showing proper formatting\n6. Test DataLoader with custom collation function\n7. Verify efficient batching with varied text lengths",
      "priority": "medium",
      "dependencies": [
        3
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Baseline Model Evaluation",
      "description": "Evaluate the base GPT-2 model on letter count and position tasks to establish a performance baseline.",
      "details": "1. Set up DsPy for multi-shot prompting\n2. Create a Jupyter notebook to evaluate the base GPT-2 model\n3. Test on both primary metrics (letter count, letter position)\n4. Ensure output format is correct (single integer for character count or single letter for character position)\n5. Document the baseline performance for comparison\n\nImplementation:\n```python\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport dspy\nimport wandb\n\n# Initialize W&B\nwandb.init(project=\"llm-spelling-finetuning\", name=\"baseline-evaluation\")\n\n# Load base model and tokenizer\nmodel_name = \"gpt2\"\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\nmodel.eval()\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load test dataset\nfrom datasets import load_dataset\ntest_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\", split=\"test\")\n\n# Define generation function\ndef generate_answer(model, tokenizer, question, max_length=10):\n    inputs = tokenizer(question, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs.input_ids,\n            max_length=len(inputs.input_ids[0]) + max_length,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=False\n        )\n    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n    # Extract just the first token/character for letter position or first number for letter count\n    if \"How many\" in question:\n        # Extract first number\n        import re\n        numbers = re.findall(r'\\d+', response)\n        return numbers[0] if numbers else response.strip()\n    else:\n        # Extract first character\n        return response.strip()[0] if response.strip() else \"\"\n\n# Evaluate on letter count questions\ndef evaluate_letter_count(model, tokenizer, dataset):\n    correct = 0\n    total = 0\n    results = []\n    \n    for item in dataset:\n        if item[\"question_type\"] != \"letter_count\":\n            continue\n            \n        prediction = generate_answer(model, tokenizer, item[\"question\"])\n        is_correct = prediction == item[\"answer\"]\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"correct\": is_correct\n        })\n        \n        correct += int(is_correct)\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    \n    return accuracy, results\n\n# Evaluate on letter position questions\ndef evaluate_letter_position(model, tokenizer, dataset):\n    correct = 0\n    total = 0\n    results = []\n    \n    for item in dataset:\n        if item[\"question_type\"] != \"letter_position\":\n            continue\n            \n        prediction = generate_answer(model, tokenizer, item[\"question\"])\n        is_correct = prediction.lower() == item[\"answer\"].lower()\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"correct\": is_correct\n        })\n        \n        correct += int(is_correct)\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    \n    return accuracy, results\n\n# Run evaluation\ncount_accuracy, count_results = evaluate_letter_count(model, tokenizer, test_dataset)\nposition_accuracy, position_results = evaluate_letter_position(model, tokenizer, test_dataset)\n\n# Log results to W&B\nwandb.log({\n    \"letter_count_accuracy\": count_accuracy,\n    \"letter_position_accuracy\": position_accuracy,\n    \"count_examples\": wandb.Table(dataframe=pd.DataFrame(count_results[:20])),\n    \"position_examples\": wandb.Table(dataframe=pd.DataFrame(position_results[:20]))\n})\n\n# Save results locally\nimport json\nwith open(\"baseline_evaluation.json\", \"w\") as f:\n    json.dump({\n        \"letter_count_accuracy\": count_accuracy,\n        \"letter_position_accuracy\": position_accuracy,\n        \"count_results\": count_results,\n        \"position_results\": position_results\n    }, f, indent=2)\n```",
      "testStrategy": "1. Verify the evaluation script runs without errors\n2. Confirm output format is correct (single integer for count, single letter for position)\n3. Check that results are properly logged to W&B\n4. Verify baseline performance metrics are saved locally\n5. Create visualizations comparing performance on different question types\n6. Analyze error patterns in baseline model predictions",
      "priority": "medium",
      "dependencies": [
        3
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Hyperparameter Tuning Infrastructure",
      "description": "Create a configuration system for hyperparameter experiments and set up experiment tracking with Weights & Biases.",
      "details": "1. Create a configuration system for hyperparameter experiments\n2. Set up a Jupyter notebook for experiment tracking\n3. Create a script that can run training with different hyperparameters\n4. Set up W&B for experiment tracking\n5. Define a clear set of metrics for comparing experiments\n\nImplementation:\n```python\nimport yaml\nimport os\nfrom datetime import datetime\nimport wandb\n\ndef create_experiment_config(\n    exp_name,\n    lora_r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    learning_rate=2e-4,\n    batch_size=8,\n    grad_accum_steps=4,\n    max_steps=1000,\n    warmup_steps=100,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n):\n    \"\"\"Create and save an experiment configuration.\"\"\"\n    config = {\n        \"experiment_name\": exp_name,\n        \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n        \"lora_config\": {\n            \"r\": lora_r,\n            \"alpha\": lora_alpha,\n            \"dropout\": lora_dropout,\n            \"target_modules\": target_modules,\n        },\n        \"training_config\": {\n            \"learning_rate\": learning_rate,\n            \"per_device_train_batch_size\": batch_size,\n            \"gradient_accumulation_steps\": grad_accum_steps,\n            \"max_steps\": max_steps,\n            \"warmup_steps\": warmup_steps,\n        },\n    }\n\n    # Create experiments directory if it doesn't exist\n    os.makedirs(\"experiments\", exist_ok=True)\n\n    # Save config to file\n    config_path = f\"experiments/{exp_name}_{config['timestamp']}.yaml\"\n    with open(config_path, \"w\") as f:\n        yaml.dump(config, f)\n\n    print(f\"Created experiment config: {config_path}\")\n    return config_path\n\n# Define hyperparameter grid\ndef create_hyperparameter_grid():\n    grid = {\n        \"lora_r\": [4, 8, 16, 32],\n        \"lora_alpha\": [8, 16, 32, 64],\n        \"learning_rate\": [1e-4, 2e-4, 5e-4, 1e-3],\n        \"batch_size\": [4, 8, 16, 32],\n        \"grad_accum_steps\": [1, 2, 4, 8],\n        \"max_steps\": [500, 1000, 2000, 5000]\n    }\n    return grid\n\n# Create experiment configs for grid search\ndef create_grid_search_configs(base_name=\"experiment\"):\n    grid = create_hyperparameter_grid()\n    configs = []\n    \n    # Start with default configuration\n    configs.append(create_experiment_config(f\"{base_name}_default\"))\n    \n    # Create configs for each hyperparameter variation\n    for param, values in grid.items():\n        for value in values:\n            # Skip the default value\n            if param == \"lora_r\" and value == 16:\n                continue\n            if param == \"lora_alpha\" and value == 32:\n                continue\n            if param == \"learning_rate\" and value == 2e-4:\n                continue\n            if param == \"batch_size\" and value == 8:\n                continue\n            if param == \"grad_accum_steps\" and value == 4:\n                continue\n            if param == \"max_steps\" and value == 1000:\n                continue\n                \n            kwargs = {param: value}\n            config_path = create_experiment_config(f\"{base_name}_{param}_{value}\", **kwargs)\n            configs.append(config_path)\n    \n    return configs\n\n# Initialize W&B sweep\ndef create_wandb_sweep():\n    sweep_config = {\n        \"method\": \"grid\",\n        \"metric\": {\"name\": \"validation_letter_position_accuracy\", \"goal\": \"maximize\"},\n        \"parameters\": {\n            \"lora_r\": {\"values\": [4, 8, 16, 32]},\n            \"lora_alpha\": {\"values\": [8, 16, 32, 64]},\n            \"learning_rate\": {\"values\": [1e-4, 2e-4, 5e-4, 1e-3]},\n            \"batch_size\": {\"values\": [4, 8, 16, 32]},\n            \"grad_accum_steps\": {\"values\": [1, 2, 4, 8]},\n            \"max_steps\": {\"values\": [500, 1000, 2000, 5000]}\n        }\n    }\n    \n    sweep_id = wandb.sweep(sweep_config, project=\"llm-spelling-finetuning\")\n    return sweep_id\n```",
      "testStrategy": "1. Verify configuration system creates valid YAML files\n2. Confirm W&B experiment tracking is properly set up\n3. Test that the hyperparameter grid generates the expected number of configurations\n4. Verify W&B sweep configuration is valid\n5. Create and test the experiment notebook\n6. Ensure metrics for comparing experiments are clearly defined",
      "priority": "medium",
      "dependencies": [
        1,
        5
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Unsloth Integration for Optimized Fine-tuning",
      "description": "Set up Unsloth for optimized LoRA fine-tuning of the GPT-2 model with memory efficiency optimizations.",
      "details": "1. Install and configure Unsloth for optimized fine-tuning\n2. Set up Unsloth-specific environment requirements\n3. Configure memory-efficient QLoRA training\n4. Set up Flash Attention 2 if available on hardware\n5. Implement proper tokenization for instruction fine-tuning\n6. Configure GPU memory optimizations\n\nImplementation:\n```python\n# Install Unsloth\n!pip install unsloth\n\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\n\n# Optimize GPU memory\ndef optimize_gpu_memory():\n    if torch.cuda.is_available():\n        # Set GPU memory allocation strategy\n        torch.cuda.set_per_process_memory_fraction(0.9)  # Reserve 10% for system\n        # Enable memory caching for faster allocation\n        torch.backends.cudnn.benchmark = True\n        # Use TF32 precision on Ampere GPUs or later for faster computation\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n# Load model with Unsloth optimizations\ndef load_unsloth_model(config):\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"gpt2\",\n        max_seq_length=512,\n        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n        load_in_4bit=True,  # Use 4-bit quantization to reduce memory usage\n        token=None,  # Add your HF token for private models\n    )\n\n    # Add LoRA adapters with Unsloth-specific optimizations\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=config[\"lora_config\"][\"r\"],\n        target_modules=config[\"lora_config\"][\"target_modules\"],\n        lora_alpha=config[\"lora_config\"][\"alpha\"],\n        lora_dropout=config[\"lora_config\"][\"dropout\"],\n        bias=\"none\",  # Unsloth-specific - sets which modules receive adapters\n        use_gradient_checkpointing=True,  # Unsloth-specific - saves memory\n        random_state=42,  # For reproducibility\n        use_rslora=False,  # Set to True for rank-stabilized LoRA (optional)\n        loftq_config=None,  # Optional LoftQ configuration\n    )\n    \n    return model, tokenizer\n\n# Prepare dataset for Unsloth\ndef prepare_unsloth_dataset(dataset):\n    def formatting_prompts_func(examples):\n        questions = examples[\"question\"]\n        answers = examples[\"answer\"]\n\n        # Special Unsloth prompt format\n        prompts = [\n            f\"<human>: {question}\\n<assistant>: \"\n            for question in questions\n        ]\n\n        # Format responses with EOS token\n        formatted_responses = [\n            f\"{answer}{tokenizer.eos_token}\"\n            for answer in answers\n        ]\n\n        return {\n            \"prompt\": prompts,\n            \"completion\": formatted_responses,\n        }\n    \n    formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n    return formatted_dataset\n\n# Set up Unsloth trainer\ndef create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, config):\n    trainer = FastLanguageModel.get_trainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        args=FastLanguageModel.get_train_args(\n            output_dir=f\"./spelling-lora-{config['experiment_name']}\",\n            per_device_train_batch_size=config[\"training_config\"][\"per_device_train_batch_size\"],\n            gradient_accumulation_steps=config[\"training_config\"][\"gradient_accumulation_steps\"],\n            warmup_steps=config[\"training_config\"][\"warmup_steps\"],\n            max_steps=config[\"training_config\"][\"max_steps\"],\n            learning_rate=config[\"training_config\"][\"learning_rate\"],\n            fp16=not torch.cuda.is_bf16_supported(),\n            bf16=torch.cuda.is_bf16_supported(),\n            logging_steps=10,\n            evaluation_strategy=\"steps\",\n            eval_steps=100,\n            save_strategy=\"steps\",\n            save_steps=200,\n            optim=\"adamw_torch\",  # Unsloth recommends adamw_torch over paged_adamw_8bit\n            max_grad_norm=0.3,    # Gradient clipping - Unsloth recommended value\n            report_to=\"wandb\",\n        ),\n        data_collator=FastLanguageModel.get_data_collator(tokenizer=tokenizer),\n    )\n    \n    return trainer\n\n# Main training function\ndef train_with_unsloth(config_path):\n    # Load config\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    \n    # Initialize W&B\n    wandb.init(project=\"llm-spelling-finetuning\", name=config[\"experiment_name\"], config=config)\n    \n    # Optimize GPU memory\n    optimize_gpu_memory()\n    \n    # Load model and tokenizer\n    model, tokenizer = load_unsloth_model(config)\n    \n    # Load dataset\n    dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\")\n    \n    # Prepare dataset for Unsloth\n    train_dataset = prepare_unsloth_dataset(dataset[\"train\"])\n    val_dataset = prepare_unsloth_dataset(dataset[\"validation\"])\n    \n    # Create trainer\n    trainer = create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, config)\n    \n    # Train model\n    trainer.train()\n    \n    # Save model\n    trainer.save_model()\n    \n    # Evaluate model\n    eval_results = trainer.evaluate()\n    wandb.log(eval_results)\n    \n    # Close W&B\n    wandb.finish()\n    \n    return eval_results\n```",
      "testStrategy": "1. Verify Unsloth installs and imports correctly\n2. Confirm memory usage is optimized compared to standard fine-tuning\n3. Test that 4-bit quantization is working correctly\n4. Measure training speed improvement over baseline implementation\n5. Verify all Unsloth-specific optimizations are configured\n6. Test with a small dataset to ensure the training loop works\n7. Monitor GPU memory usage during training",
      "priority": "high",
      "dependencies": [
        6
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Environment Setup with Optimizations",
          "description": "Configure the development environment with Unsloth and necessary dependencies for optimized LLM fine-tuning",
          "dependencies": [],
          "details": "Install Unsloth library and compatible dependencies (bitsandbytes, transformers, PEFT). Configure GPU memory settings for optimal performance. Set up quantization libraries. Verify hardware compatibility (CUDA, ROCm, or Metal backend). Test environment with basic model loading to ensure all components work together.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Model Loading with Unsloth-specific Configurations",
          "description": "Implement efficient model loading using Unsloth's FastLanguageModel with proper quantization and LoRA setup",
          "dependencies": [
            1
          ],
          "details": "Use FastLanguageModel.from_pretrained() to load base models with quantization. Configure LoRA adapters with get_peft_model() using appropriate rank and target modules. Implement proper quantization settings (4-bit, 8-bit) based on available VRAM. Set up gradient checkpointing with 'unsloth' option. Validate model loading with memory profiling.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Dataset Preparation for Unsloth",
          "description": "Prepare and optimize training datasets for efficient processing with Unsloth",
          "dependencies": [
            1
          ],
          "details": "Format dataset according to Unsloth requirements. Implement efficient tokenization with proper sequence length handling. Set up data processing pipeline with appropriate num_proc parameter. Configure dataset caching mechanisms to reduce memory overhead. Validate dataset loading with performance metrics.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Trainer Setup with Memory Optimizations",
          "description": "Configure SFTTrainer with Unsloth-optimized parameters for efficient fine-tuning",
          "dependencies": [
            2,
            3
          ],
          "details": "Set up SFTTrainer with optimized batch size and gradient accumulation. Configure learning rate and scheduler based on training duration. Implement proper precision settings (bf16/fp16) based on hardware support. Set up memory-efficient optimizers (adamw_8bit). Configure logging and checkpointing. Validate trainer setup with memory usage monitoring during initial training steps.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 8,
      "title": "Model Fine-tuning and Experimentation",
      "description": "Implement the training loop and run experiments with different hyperparameters to find the optimal configuration.",
      "details": "1. Create a reusable training script that accepts hyperparameter configs\n2. Implement the training loop using Unsloth\n3. Set up checkpoint saving and loading system\n4. Implement early stopping based on validation metrics\n5. Run experiments with different hyperparameters:\n   - LoRA rank (r): [4, 8, 16, 32]\n   - LoRA alpha: [8, 16, 32, 64]\n   - Learning rate: [1e-4, 2e-4, 5e-4, 1e-3]\n   - Batch size: [4, 8, 16, 32]\n   - Gradient accumulation steps: [1, 2, 4, 8]\n   - Training steps: [500, 1000, 2000, 5000]\n6. Track all experiments in W&B\n\nImplementation:\n```python\nimport os\nimport yaml\nimport torch\nimport wandb\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n\n# Main experiment runner\ndef run_experiment(config_path):\n    # Load config\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    \n    # Initialize W&B\n    run = wandb.init(\n        project=\"llm-spelling-finetuning\",\n        name=config[\"experiment_name\"],\n        config=config,\n        reinit=True\n    )\n    \n    # Load model and tokenizer with Unsloth\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"gpt2\",\n        max_seq_length=512,\n        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n        load_in_4bit=True\n    )\n    \n    # Add LoRA adapters\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=config[\"lora_config\"][\"r\"],\n        target_modules=config[\"lora_config\"][\"target_modules\"],\n        lora_alpha=config[\"lora_config\"][\"alpha\"],\n        lora_dropout=config[\"lora_config\"][\"dropout\"],\n        bias=\"none\",\n        use_gradient_checkpointing=True,\n        random_state=42\n    )\n    \n    # Load dataset\n    dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\")\n    \n    # Format dataset for instruction fine-tuning\n    def formatting_func(examples):\n        questions = examples[\"question\"]\n        answers = examples[\"answer\"]\n        \n        prompts = [f\"<human>: {q}\\n<assistant>: \" for q in questions]\n        completions = [f\"{a}{tokenizer.eos_token}\" for a in answers]\n        \n        return {\"prompt\": prompts, \"completion\": completions}\n    \n    train_dataset = dataset[\"train\"].map(formatting_func, batched=True)\n    eval_dataset = dataset[\"validation\"].map(formatting_func, batched=True)\n    \n    # Set up output directory\n    output_dir = f\"./results/{config['experiment_name']}_{config['timestamp']}\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create training arguments\n    training_args = FastLanguageModel.get_train_args(\n        output_dir=output_dir,\n        per_device_train_batch_size=config[\"training_config\"][\"per_device_train_batch_size\"],\n        gradient_accumulation_steps=config[\"training_config\"][\"gradient_accumulation_steps\"],\n        warmup_steps=config[\"training_config\"][\"warmup_steps\"],\n        max_steps=config[\"training_config\"][\"max_steps\"],\n        learning_rate=config[\"training_config\"][\"learning_rate\"],\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=10,\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        save_strategy=\"steps\",\n        save_steps=200,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        optim=\"adamw_torch\",\n        max_grad_norm=0.3,\n        report_to=\"wandb\"\n    )\n    \n    # Create trainer with early stopping\n    trainer = FastLanguageModel.get_trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=FastLanguageModel.get_data_collator(tokenizer),\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n    )\n    \n    # Train model\n    trainer.train()\n    \n    # Save final model\n    trainer.save_model(f\"{output_dir}/final\")\n    \n    # Evaluate on validation set\n    eval_results = trainer.evaluate()\n    \n    # Log final results\n    wandb.log(eval_results)\n    \n    # Save evaluation results\n    with open(f\"{output_dir}/eval_results.yaml\", \"w\") as f:\n        yaml.dump(eval_results, f)\n    \n    # Close wandb run\n    wandb.finish()\n    \n    return output_dir, eval_results\n\n# Run multiple experiments\ndef run_experiments(config_paths):\n    results = {}\n    for config_path in config_paths:\n        print(f\"Running experiment with config: {config_path}\")\n        output_dir, eval_results = run_experiment(config_path)\n        \n        # Extract config name\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n        \n        results[config[\"experiment_name\"]] = {\n            \"output_dir\": output_dir,\n            \"eval_results\": eval_results\n        }\n    \n    # Save all results\n    with open(\"experiment_results_summary.yaml\", \"w\") as f:\n        yaml.dump(results, f)\n    \n    return results\n```",
      "testStrategy": "1. Verify training script runs without errors\n2. Confirm experiments are properly tracked in W&B\n3. Check that checkpoints are saved correctly\n4. Verify early stopping works as expected\n5. Test that the best model is loaded at the end of training\n6. Compare performance across different hyperparameter configurations\n7. Ensure all experiment results are properly saved",
      "priority": "high",
      "dependencies": [
        4,
        6,
        7
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Training Script Implementation",
          "description": "Develop a robust training script that handles the fine-tuning process for pre-trained models",
          "dependencies": [],
          "details": "Create a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Checkpoint Management System",
          "description": "Implement a comprehensive checkpoint system to save and restore model states",
          "dependencies": [
            1
          ],
          "details": "Design a checkpoint manager that: saves model weights at configurable intervals, stores optimizer states, implements versioning for checkpoints, provides functionality to resume training from any checkpoint, includes cleanup mechanisms for old checkpoints, and ensures compatibility across different hardware configurations.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Early Stopping Mechanism",
          "description": "Develop an early stopping system to prevent overfitting and optimize training time",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement a configurable early stopping mechanism that: monitors validation metrics (loss, accuracy), applies patience parameters to allow for fluctuations, saves best model states when improvements occur, provides restoration of best model after training, includes visualization of stopping point, and allows for custom stopping criteria definition.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Hyperparameter Experimentation Framework",
          "description": "Create a framework for systematic hyperparameter tuning and experimentation",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Develop a hyperparameter experimentation system that: supports grid search and random search methods, enables parallel experiment execution, provides configuration management for experiments, implements parameter scheduling (learning rate decay), integrates with checkpoint system, and includes mechanisms to handle failed experiments gracefully.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Results Tracking and Analysis System",
          "description": "Build a comprehensive system to track, visualize and compare experiment results",
          "dependencies": [
            4
          ],
          "details": "Implement a results management system that: stores metrics for all experiments, generates comparative visualizations across runs, calculates statistical significance of improvements, exports results in standard formats, provides filtering and sorting capabilities, and integrates with external visualization tools if needed.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 9,
      "title": "Comprehensive Model Evaluation",
      "description": "Evaluate the fine-tuned models on the test set using multiple metrics and perform detailed error analysis.",
      "details": "1. Evaluate the best model on the test set\n2. Implement comprehensive evaluation metrics:\n   - Letter Count Accuracy\n   - Letter Position Accuracy\n   - Character-Level Accuracy\n   - Levenshtein Distance\n   - Token-Level Perplexity\n3. Perform detailed error analysis\n4. Create visualizations of the results\n5. Compare performance between base and fine-tuned models\n\nImplementation:\n```python\nimport torch\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport Levenshtein\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom datasets import load_dataset\nimport wandb\n\n# Load models for comparison\ndef load_models(base_model_name, finetuned_model_path):\n    # Load base model\n    base_model = GPT2LMHeadModel.from_pretrained(base_model_name)\n    base_tokenizer = GPT2Tokenizer.from_pretrained(base_model_name)\n    base_tokenizer.pad_token = base_tokenizer.eos_token\n    \n    # Load fine-tuned model\n    finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path)\n    finetuned_tokenizer = GPT2Tokenizer.from_pretrained(base_model_name)  # Use same tokenizer\n    finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token\n    \n    return {\n        \"base\": (base_model, base_tokenizer),\n        \"finetuned\": (finetuned_model, finetuned_tokenizer)\n    }\n\n# Generate answer from model\ndef generate_answer(model, tokenizer, question, max_length=10):\n    inputs = tokenizer(question, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs.input_ids,\n            max_length=len(inputs.input_ids[0]) + max_length,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=False\n        )\n    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n    \n    # Extract just the first token/character for letter position or first number for letter count\n    if \"How many\" in question:\n        # Extract first number\n        import re\n        numbers = re.findall(r'\\d+', response)\n        return numbers[0] if numbers else response.strip()\n    else:\n        # Extract first character\n        return response.strip()[0] if response.strip() else \"\"\n\n# Calculate letter count accuracy\ndef calc_letter_count_accuracy(model, tokenizer, test_dataset):\n    correct = 0\n    total = 0\n    results = []\n    \n    for item in test_dataset:\n        if item[\"question_type\"] != \"letter_count\":\n            continue\n            \n        prediction = generate_answer(model, tokenizer, item[\"question\"])\n        is_correct = prediction == item[\"answer\"]\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"correct\": is_correct,\n            \"word\": item[\"word\"]\n        })\n        \n        correct += int(is_correct)\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    \n    return accuracy, results\n\n# Calculate letter position accuracy\ndef calc_letter_position_accuracy(model, tokenizer, test_dataset):\n    correct = 0\n    total = 0\n    results = []\n    \n    for item in test_dataset:\n        if item[\"question_type\"] != \"letter_position\":\n            continue\n            \n        prediction = generate_answer(model, tokenizer, item[\"question\"])\n        is_correct = prediction.lower() == item[\"answer\"].lower()\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"correct\": is_correct,\n            \"word\": item[\"word\"]\n        })\n        \n        correct += int(is_correct)\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    \n    return accuracy, results\n\n# Calculate character-level accuracy\ndef calc_character_level_accuracy(model, tokenizer, test_dataset):\n    total_char_accuracy = 0\n    total_samples = 0\n    results = []\n\n    for item in test_dataset:\n        if item[\"question_type\"] not in [\"letter_position\", \"letter_count\"]:\n            continue\n\n        prediction = generate_answer(model, tokenizer, item[\"question\"])\n        pred_chars = prediction.strip().lower().replace(\" \", \"\")\n        true_chars = item[\"answer\"].lower()\n\n        # Calculate character-by-character accuracy\n        correct_chars = 0\n        for i, char in enumerate(true_chars):\n            if i < len(pred_chars) and pred_chars[i] == char:\n                correct_chars += 1\n\n        char_accuracy = correct_chars / len(true_chars) if len(true_chars) > 0 else 0\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"char_accuracy\": char_accuracy,\n            \"word\": item[\"word\"]\n        })\n        \n        total_char_accuracy += char_accuracy\n        total_samples += 1\n\n    avg_char_accuracy = total_char_accuracy / total_samples if total_samples > 0 else 0\n    print(f\"Character-Level Accuracy: {avg_char_accuracy:.4f}\")\n    \n    return avg_char_accuracy, results\n\n# Calculate Levenshtein distance\ndef calc_levenshtein_metrics(model, tokenizer, test_dataset):\n    total_distances = 0\n    total_samples = 0\n    results = []\n\n    for item in test_dataset:\n        if item[\"question_type\"] not in [\"letter_position\", \"letter_count\"]:\n            continue\n\n        prediction = generate_answer(model, tokenizer, item[\"question\"])\n        pred_text = prediction.strip().lower()\n        true_text = item[\"answer\"].lower()\n\n        # Calculate Levenshtein distance\n        distance = Levenshtein.distance(pred_text, true_text)\n        normalized_distance = distance / max(len(pred_text), len(true_text)) if max(len(pred_text), len(true_text)) > 0 else 0\n\n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": true_text,\n            \"prediction\": pred_text,\n            \"levenshtein_distance\": distance,\n            \"normalized_distance\": normalized_distance,\n            \"word\": item[\"word\"]\n        })\n        \n        total_distances += normalized_distance\n        total_samples += 1\n\n    avg_distance = total_distances / total_samples if total_samples > 0 else 0\n    print(f\"Average Normalized Levenshtein Distance: {avg_distance:.4f}\")\n    \n    return avg_distance, results\n\n# Perform error analysis\ndef perform_error_analysis(results_dict):\n    error_analysis = {}\n    \n    # Analyze letter count errors\n    count_errors = [r for r in results_dict[\"letter_count\"] if not r[\"correct\"]]\n    \n    # Categorize errors\n    error_types = {\n        \"off_by_one\": 0,\n        \"completely_wrong\": 0,\n        \"no_number\": 0,\n        \"other\": 0\n    }\n    \n    for error in count_errors:\n        try:\n            pred = int(error[\"prediction\"])\n            true = int(error[\"expected\"])\n            \n            if abs(pred - true) == 1:\n                error_types[\"off_by_one\"] += 1\n            else:\n                error_types[\"completely_wrong\"] += 1\n        except ValueError:\n            if not error[\"prediction\"].strip():\n                error_types[\"no_number\"] += 1\n            else:\n                error_types[\"other\"] += 1\n    \n    error_analysis[\"letter_count_errors\"] = error_types\n    \n    # Analyze letter position errors\n    position_errors = [r for r in results_dict[\"letter_position\"] if not r[\"correct\"]]\n    \n    # Categorize position errors\n    position_error_types = {\n        \"adjacent_letter\": 0,\n        \"wrong_case\": 0,\n        \"no_response\": 0,\n        \"other\": 0\n    }\n    \n    for error in position_errors:\n        if not error[\"prediction\"].strip():\n            position_error_types[\"no_response\"] += 1\n        elif error[\"prediction\"].lower() == error[\"expected\"].lower():\n            position_error_types[\"wrong_case\"] += 1\n        elif error[\"word\"] and error[\"prediction\"] in error[\"word\"]:\n            position_error_types[\"adjacent_letter\"] += 1\n        else:\n            position_error_types[\"other\"] += 1\n    \n    error_analysis[\"letter_position_errors\"] = position_error_types\n    \n    # Analyze by word length\n    word_length_performance = {}\n    \n    for result in results_dict[\"letter_count\"] + results_dict[\"letter_position\"]:\n        word = result[\"word\"]\n        length = len(word)\n        \n        if length not in word_length_performance:\n            word_length_performance[length] = {\"correct\": 0, \"total\": 0}\n        \n        word_length_performance[length][\"total\"] += 1\n        if result[\"correct\"]:\n            word_length_performance[length][\"correct\"] += 1\n    \n    # Calculate accuracy by word length\n    for length, stats in word_length_performance.items():\n        stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"] if stats[\"total\"] > 0 else 0\n    \n    error_analysis[\"word_length_performance\"] = word_length_performance\n    \n    return error_analysis\n\n# Create performance dashboard\ndef create_performance_dashboard(base_results, finetuned_results, error_analysis):\n    # Set up figure\n    fig = plt.figure(figsize=(18, 12))\n    \n    # 1. Accuracy comparison across metrics\n    ax1 = fig.add_subplot(2, 3, 1)\n    metrics = ['letter_count_accuracy', 'letter_position_accuracy', 'character_level_accuracy']\n    base_values = [base_results[m] for m in metrics]\n    finetuned_values = [finetuned_results[m] for m in metrics]\n    \n    x = np.arange(len(metrics))\n    width = 0.35\n    \n    ax1.bar(x - width/2, base_values, width, label='Base Model')\n    ax1.bar(x + width/2, finetuned_values, width, label='Fine-tuned Model')\n    \n    ax1.set_ylabel('Accuracy')\n    ax1.set_title('Accuracy Comparison')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n    ax1.legend()\n    \n    # 2. Error analysis for letter count\n    ax2 = fig.add_subplot(2, 3, 2)\n    error_types = error_analysis[\"letter_count_errors\"]\n    ax2.bar(error_types.keys(), error_types.values())\n    ax2.set_title('Letter Count Error Types')\n    ax2.set_ylabel('Count')\n    plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\")\n    \n    # 3. Error analysis for letter position\n    ax3 = fig.add_subplot(2, 3, 3)\n    position_error_types = error_analysis[\"letter_position_errors\"]\n    ax3.bar(position_error_types.keys(), position_error_types.values())\n    ax3.set_title('Letter Position Error Types')\n    ax3.set_ylabel('Count')\n    plt.setp(ax3.get_xticklabels(), rotation=45, ha=\"right\")\n    \n    # 4. Performance by word length\n    ax4 = fig.add_subplot(2, 3, 4)\n    word_length_perf = error_analysis[\"word_length_performance\"]\n    lengths = sorted(word_length_perf.keys())\n    accuracies = [word_length_perf[l][\"accuracy\"] for l in lengths]\n    \n    ax4.plot(lengths, accuracies, marker='o')\n    ax4.set_title('Performance by Word Length')\n    ax4.set_xlabel('Word Length')\n    ax4.set_ylabel('Accuracy')\n    \n    # 5. Levenshtein distance comparison\n    ax5 = fig.add_subplot(2, 3, 5)\n    ax5.bar(['Base Model', 'Fine-tuned Model'], \n            [base_results['levenshtein_distance'], finetuned_results['levenshtein_distance']])\n    ax5.set_title('Normalized Levenshtein Distance')\n    ax5.set_ylabel('Distance (lower is better)')\n    \n    plt.tight_layout()\n    plt.savefig('evaluation_dashboard.png', dpi=300)\n    \n    return fig\n\n# Main evaluation function\ndef evaluate_models(base_model_name, finetuned_model_path):\n    # Initialize W&B\n    wandb.init(project=\"llm-spelling-finetuning\", name=\"final_evaluation\")\n    \n    # Load test dataset\n    test_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\", split=\"test\")\n    \n    # Load models\n    models = load_models(base_model_name, finetuned_model_path)\n    \n    # Evaluate base model\n    base_model, base_tokenizer = models[\"base\"]\n    base_results = {}\n    \n    print(\"Evaluating base model...\")\n    base_results[\"letter_count_accuracy\"], base_count_results = calc_letter_count_accuracy(base_model, base_tokenizer, test_dataset)\n    base_results[\"letter_position_accuracy\"], base_position_results = calc_letter_position_accuracy(base_model, base_tokenizer, test_dataset)\n    base_results[\"character_level_accuracy\"], base_char_results = calc_character_level_accuracy(base_model, base_tokenizer, test_dataset)\n    base_results[\"levenshtein_distance\"], base_levenshtein_results = calc_levenshtein_metrics(base_model, base_tokenizer, test_dataset)\n    \n    base_detailed_results = {\n        \"letter_count\": base_count_results,\n        \"letter_position\": base_position_results,\n        \"character_level\": base_char_results,\n        \"levenshtein\": base_levenshtein_results\n    }\n    \n    # Evaluate fine-tuned model\n    finetuned_model, finetuned_tokenizer = models[\"finetuned\"]\n    finetuned_results = {}\n    \n    print(\"\\nEvaluating fine-tuned model...\")\n    finetuned_results[\"letter_count_accuracy\"], finetuned_count_results = calc_letter_count_accuracy(finetuned_model, finetuned_tokenizer, test_dataset)\n    finetuned_results[\"letter_position_accuracy\"], finetuned_position_results = calc_letter_position_accuracy(finetuned_model, finetuned_tokenizer, test_dataset)\n    finetuned_results[\"character_level_accuracy\"], finetuned_char_results = calc_character_level_accuracy(finetuned_model, finetuned_tokenizer, test_dataset)\n    finetuned_results[\"levenshtein_distance\"], finetuned_levenshtein_results = calc_levenshtein_metrics(finetuned_model, finetuned_tokenizer, test_dataset)\n    \n    finetuned_detailed_results = {\n        \"letter_count\": finetuned_count_results,\n        \"letter_position\": finetuned_position_results,\n        \"character_level\": finetuned_char_results,\n        \"levenshtein\": finetuned_levenshtein_results\n    }\n    \n    # Perform error analysis\n    error_analysis = perform_error_analysis(finetuned_detailed_results)\n    \n    # Create performance dashboard\n    dashboard = create_performance_dashboard(base_results, finetuned_results, error_analysis)\n    \n    # Log results to W&B\n    wandb.log({\n        \"base_model\": base_results,\n        \"finetuned_model\": finetuned_results,\n        \"error_analysis\": error_analysis,\n        \"evaluation_dashboard\": wandb.Image(dashboard)\n    })\n    \n    # Save results locally\n    evaluation_results = {\n        \"base_model\": {\n            \"metrics\": base_results,\n            \"detailed_results\": base_detailed_results\n        },\n        \"finetuned_model\": {\n            \"metrics\": finetuned_results,\n            \"detailed_results\": finetuned_detailed_results,\n            \"error_analysis\": error_analysis\n        }\n    }\n    \n    with open(\"final_evaluation_results.json\", \"w\") as f:\n        json.dump(evaluation_results, f, indent=2)\n    \n    # Close W&B\n    wandb.finish()\n    \n    return evaluation_results\n```",
      "testStrategy": "1. Verify all evaluation metrics are calculated correctly\n2. Confirm error analysis provides meaningful insights\n3. Check that visualizations are clear and informative\n4. Verify results are properly logged to W&B\n5. Confirm performance comparison between base and fine-tuned models\n6. Test with different model checkpoints to ensure consistent evaluation\n7. Verify final evaluation results are saved locally",
      "priority": "medium",
      "dependencies": [
        8
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Model Publishing and Documentation",
      "description": "Prepare the final model, documentation, and publish to Hugging Face with comprehensive model card.",
      "details": "1. Prepare model card documentation\n2. Create detailed README for the project\n3. Upload the best model to Hugging Face\n4. Ensure dataset is properly published\n5. Create final report with results and findings\n\nImplementation:\n```python\nfrom huggingface_hub import HfApi\nimport os\nimport json\nimport yaml\n\n# Create model card\ndef create_model_card(evaluation_results, config):\n    base_metrics = evaluation_results[\"base_model\"][\"metrics\"]\n    finetuned_metrics = evaluation_results[\"finetuned_model\"][\"metrics\"]\n    \n    # Calculate improvements\n    improvements = {}\n    for metric in base_metrics.keys():\n        if metric == \"levenshtein_distance\":\n            # Lower is better for Levenshtein\n            improvements[metric] = ((base_metrics[metric] - finetuned_metrics[metric]) / base_metrics[metric]) * 100\n        else:\n            # Higher is better for accuracy metrics\n            improvements[metric] = ((finetuned_metrics[metric] - base_metrics[metric]) / base_metrics[metric]) * 100\n    \n    model_card = f\"\"\"---\nlanguage: en\ntags:\n- gpt2\n- spelling\n- lora\n- fine-tuning\n- unsloth\ndatasets:\n- YOUR-USERNAME/llm-spelling-dataset\nmetrics:\n- accuracy\nlicense: mit\n---\n\n# GPT-2 Spelling Fine-tuned Model\n\nThis model is a fine-tuned version of [gpt2](https://huggingface.co/gpt2) on spelling tasks. It was fine-tuned using LoRA adapters to improve the model's understanding of letter count and character position via spelling mechanics.\n\n## Model description\n\nThis model was fine-tuned to address the \"strawberry problem\" - improving a language model's ability to answer questions about letter counts and positions in words without explicitly training on those tasks. Instead, the model was trained on spelling tasks, which implicitly teaches it to understand character-level patterns.\n\n### Training hyperparameters\n\n- LoRA rank (r): {config['lora_config']['r']}\n- LoRA alpha: {config['lora_config']['alpha']}\n- Learning rate: {config['training_config']['learning_rate']}\n- Batch size: {config['training_config']['per_device_train_batch_size']}\n- Gradient accumulation steps: {config['training_config']['gradient_accumulation_steps']}\n- Training steps: {config['training_config']['max_steps']}\n\n## Evaluation results\n\n### Base model performance\n- Letter Count Accuracy: {base_metrics['letter_count_accuracy']:.4f}\n- Letter Position Accuracy: {base_metrics['letter_position_accuracy']:.4f}\n- Character-Level Accuracy: {base_metrics['character_level_accuracy']:.4f}\n- Normalized Levenshtein Distance: {base_metrics['levenshtein_distance']:.4f}\n\n### Fine-tuned model performance\n- Letter Count Accuracy: {finetuned_metrics['letter_count_accuracy']:.4f}\n- Letter Position Accuracy: {finetuned_metrics['letter_position_accuracy']:.4f}\n- Character-Level Accuracy: {finetuned_metrics['character_level_accuracy']:.4f}\n- Normalized Levenshtein Distance: {finetuned_metrics['levenshtein_distance']:.4f}\n\n### Improvements\n- Letter Count Accuracy: {improvements['letter_count_accuracy']:.2f}%\n- Letter Position Accuracy: {improvements['letter_position_accuracy']:.2f}%\n- Character-Level Accuracy: {improvements['character_level_accuracy']:.2f}%\n- Normalized Levenshtein Distance: {improvements['levenshtein_distance']:.2f}%\n\n## Usage\n\n```python\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"YOUR-USERNAME/gpt2-spelling-lora\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"YOUR-USERNAME/gpt2-spelling-lora\")\n\n# Example questions\nquestions = [\n    \"How many r's are in the word 'strawberry'?\",\n    \"What is the 3rd letter in 'strawberry'?\"\n]\n\n# Generate answers\nfor question in questions:\n    inputs = tokenizer(question, return_tensors=\"pt\")\n    outputs = model.generate(inputs.input_ids, max_length=50)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")\n```\n\n## Training procedure\n\n### Training data\n\nThe model was trained on a dataset of spelling examples generated from GPT-2 tokens. The training data included various template formats for spelling examples to maximize generalization.\n\nThe validation and test sets were created from English dictionary words that were not in the training set, ensuring proper evaluation of the model's ability to generalize to new words.\n\n### Training method\n\nThe model was fine-tuned using LoRA (Low-Rank Adaptation) with Unsloth optimizations. This approach allows for efficient fine-tuning with minimal memory requirements while maintaining performance.\n\n## Limitations and bias\n\nThis model is specifically fine-tuned for spelling tasks and may not perform well on other tasks. It is also limited by the vocabulary of the base GPT-2 model and may struggle with rare or complex words.\n\n## License\n\nThis model is licensed under the MIT License.\n\"\"\"\n    \n    return model_card\n\n# Create README\ndef create_readme(evaluation_results, config):\n    base_metrics = evaluation_results[\"base_model\"][\"metrics\"]\n    finetuned_metrics = evaluation_results[\"finetuned_model\"][\"metrics\"]\n    \n    readme = f\"\"\"# LLM Strawberry Problem Solution via Fine-tuning of Spelling\n\n## Project Overview\n\nThis project addresses the \"strawberry problem\" in language models - improving a model's ability to answer questions about letter counts and positions in words without explicitly training on those tasks. Instead, the model is trained on spelling tasks, which implicitly teaches it to understand character-level patterns.\n\n## Approach\n\nWe fine-tuned a GPT-2 language model using LoRA adapters with the following approach:\n\n1. Extracted multi-character, letter-based tokens from the GPT-2 tokenizer vocabulary\n2. Created a dataset of spelling examples with various template formats\n3. Split the data into training, validation, and test sets\n4. Fine-tuned the model using Unsloth optimizations\n5. Evaluated the model on letter count and position tasks\n\n## Results\n\n### Base model performance\n- Letter Count Accuracy: {base_metrics['letter_count_accuracy']:.4f}\n- Letter Position Accuracy: {base_metrics['letter_position_accuracy']:.4f}\n\n### Fine-tuned model performance\n- Letter Count Accuracy: {finetuned_metrics['letter_count_accuracy']:.4f}\n- Letter Position Accuracy: {finetuned_metrics['letter_position_accuracy']:.4f}\n\n## Repository Structure\n\n```\n.\n├── data/                 # Data files and datasets\n├── notebooks/            # Jupyter notebooks for experimentation\n├── src/                  # Source code\n├── configs/              # Configuration files\n├── results/              # Experimental results and visualizations\n├── checkpoints/          # Model checkpoints\n├── README.md             # This file\n└── requirements.txt      # Python dependencies\n```\n\n## Setup and Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/YOUR-USERNAME/llm-spelling-finetuning.git\ncd llm-spelling-finetuning\n\n# Install dependencies with uv\ncurl -fsSL https://astral.sh/uv/install.sh | bash\nuv pip install -r requirements.txt\n```\n\n## Usage\n\n### Training\n\n```bash\npython src/train.py --config configs/experiment_config.yaml\n```\n\n### Evaluation\n\n```bash\npython src/evaluate.py --model_path checkpoints/best_model\n```\n\n## Resources\n\n- [Fine-tuned Model on Hugging Face](https://huggingface.co/YOUR-USERNAME/gpt2-spelling-lora)\n- [Dataset on Hugging Face](https://huggingface.co/datasets/YOUR-USERNAME/llm-spelling-dataset)\n- [Experiment Tracking on W&B](https://wandb.ai/YOUR-USERNAME/llm-spelling-finetuning)\n\n## License\n\nThis project is licensed under the MIT License.\n\"\"\"\n    \n    return readme\n\n# Publish model to Hugging Face\ndef publish_to_hugging_face(model_path, model_card, readme, evaluation_results):\n    # Initialize Hugging Face API\n    api = HfApi()\n    \n    # Create repository if it doesn't exist\n    repo_id = \"YOUR-USERNAME/gpt2-spelling-lora\"\n    try:\n        api.create_repo(repo_id=repo_id, private=False)\n    except Exception as e:\n        print(f\"Repository already exists or error: {e}\")\n    \n    # Save model card\n    with open(os.path.join(model_path, \"README.md\"), \"w\") as f:\n        f.write(model_card)\n    \n    # Save evaluation results\n    with open(os.path.join(model_path, \"evaluation_results.json\"), \"w\") as f:\n        json.dump(evaluation_results, f, indent=2)\n    \n    # Upload model to Hugging Face\n    api.upload_folder(\n        folder_path=model_path,\n        repo_id=repo_id,\n        commit_message=\"Upload fine-tuned spelling model\"\n    )\n    \n    # Create project README\n    with open(\"README.md\", \"w\") as f:\n        f.write(readme)\n    \n    print(f\"Model published to Hugging Face: https://huggingface.co/{repo_id}\")\n    \n    return repo_id\n\n# Main publishing function\ndef publish_final_model(best_model_path, evaluation_results_path, config_path):\n    # Load evaluation results\n    with open(evaluation_results_path, \"r\") as f:\n        evaluation_results = json.load(f)\n    \n    # Load config\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    \n    # Create model card\n    model_card = create_model_card(evaluation_results, config)\n    \n    # Create README\n    readme = create_readme(evaluation_results, config)\n    \n    # Publish to Hugging Face\n    repo_id = publish_to_hugging_face(best_model_path, model_card, readme, evaluation_results)\n    \n    # Create final report\n    create_final_report(evaluation_results, config, repo_id)\n    \n    return repo_id\n\n# Create final report\ndef create_final_report(evaluation_results, config, repo_id):\n    # Create a comprehensive final report with all results and findings\n    report = {\n        \"project_name\": \"LLM Strawberry Problem Solution via Fine-tuning of Spelling\",\n        \"model_repository\": f\"https://huggingface.co/{repo_id}\",\n        \"dataset_repository\": \"https://huggingface.co/datasets/YOUR-USERNAME/llm-spelling-dataset\",\n        \"configuration\": config,\n        \"evaluation_results\": evaluation_results,\n        \"conclusion\": {\n            \"success_criteria_met\": {\n                \"letter_count_improvement\": evaluation_results[\"finetuned_model\"][\"metrics\"][\"letter_count_accuracy\"] > \n                                          evaluation_results[\"base_model\"][\"metrics\"][\"letter_count_accuracy\"],\n                \"letter_position_improvement\": evaluation_results[\"finetuned_model\"][\"metrics\"][\"letter_position_accuracy\"] > \n                                              evaluation_results[\"base_model\"][\"metrics\"][\"letter_position_accuracy\"],\n                \"properly_documented\": True,\n                \"uploaded_to_hugging_face\": True,\n                \"tracked_in_wandb\": True\n            }\n        }\n    }\n    \n    # Save report\n    with open(\"final_report.json\", \"w\") as f:\n        json.dump(report, f, indent=2)\n    \n    print(\"Final report created: final_report.json\")\n    \n    return report\n```",
      "testStrategy": "1. Verify model card is comprehensive and follows Hugging Face guidelines\n2. Confirm README provides clear instructions for using the model\n3. Test model upload to Hugging Face\n4. Verify dataset is properly published and accessible\n5. Check that final report includes all required information\n6. Test model loading from Hugging Face\n7. Verify all success criteria from the PRD are met and documented",
      "priority": "medium",
      "dependencies": [
        9
      ],
      "status": "pending",
      "subtasks": []
    }
  ]
}