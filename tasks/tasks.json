{
  "tasks": [
    {
      "id": 1,
      "title": "Development Environment Setup",
      "description": "Set up the Python development environment with all required dependencies using uv for package management.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "1. Install uv package manager: `curl -fsSL https://astral.sh/uv/install.sh | bash`\n2. Create project directory structure following the repository structure in the PRD\n3. Install dependencies directly using uv commands instead of editing requirements.txt:\n   - `uv pip install torch transformers datasets wandb dspy lightning matplotlib seaborn pandas jupyter notebook ipywidgets`\n4. After installation, generate requirements.txt for documentation: `uv pip freeze > requirements.txt`\n5. Set up Git repository with proper .gitignore\n6. Configure Weights & Biases account: `wandb login`\n7. Set up Hugging Face account and API access: `huggingface-cli login`\n8. Create initial Jupyter notebook for experimentation\n9. Verify all imports work correctly\n\nNote: For Unsloth or GPU-based fine-tuning, use Google Colab or https://lightning.ai/lars/home.",
      "testStrategy": "1. Verify all libraries install without errors using uv\n2. Confirm successful authentication with W&B\n3. Confirm successful authentication with Hugging Face\n4. Test import of all required libraries in a Jupyter notebook\n5. Make initial commit with environment setup completed\n6. Verify environment reproducibility by creating a new environment using the generated requirements.txt",
      "subtasks": [
        {
          "id": 1,
          "title": "Installation Phase",
          "description": "Set up the development environment by installing all necessary software, tools, and dependencies using uv package manager",
          "dependencies": [],
          "details": "Install uv package manager first, then use it to install all required libraries directly with 'uv pip install' commands. Document all installation paths and versions for future reference. Use uv for all Python package management to ensure reproducibility and faster installation times.\n<info added on 2025-05-07T14:46:36.027Z>\nInstall uv package manager first, then use it to install all required libraries directly with 'uv pip install' commands. Document all installation paths and versions for future reference. Use uv for all Python package management to ensure reproducibility and faster installation times.\n\nThis task can be worked on independently and in parallel with others. The Installation Phase has no dependencies and is parallelizable (parallelizable: true).\n</info added on 2025-05-07T14:46:36.027Z>\n<info added on 2025-05-07T15:14:41.583Z>\nThe Installation Phase has been completed successfully. The following steps were taken to set up the development environment:\n\n1. Created a new Python virtual environment using uv:\n   ```sh\n   uv venv .venv\n   ```\n2. Activated the environment:\n   ```sh\n   source .venv/bin/activate\n   ```\n3. Installed core development dependencies:\n   ```sh\n   uv pip install black ruff mypy ipython requests\n   ```\n   (Additional project-specific packages can be added as needed)\n4. Generated requirements.txt for reproducibility:\n   ```sh\n   uv pip freeze > requirements.txt\n   ```\n5. Documented uv version:\n   ```sh\n   uv --version\n   ```\n\nAll installation paths and versions have been documented as required. The development environment is now fully set up and ready for use. Setup instructions have been added to the README for reference. The next phase (Configuration Phase) can now begin.\n</info added on 2025-05-07T15:14:41.583Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Configuration Phase",
          "description": "Configure all installed components to work together properly and set up authentication for external services",
          "dependencies": [
            1
          ],
          "details": "Set environment variables, configure IDE settings, establish database connections, set up version control repositories, configure build tools, and establish authentication for any external services or APIs required for development. Generate requirements.txt using 'uv pip freeze > requirements.txt' for documentation purposes.\n<info added on 2025-05-07T14:26:08.046Z>\nSet environment variables, configure IDE settings, establish database connections, set up version control repositories, configure build tools, and establish authentication for any external services or APIs required for development. Generate requirements.txt using 'uv pip freeze > requirements.txt' for documentation purposes.\n\nCreate a proper environment configuration by duplicating the .env.example file to .env:\n1. Copy the .env.example file to .env using the command: `cp .env.example .env` (Unix/Mac) or `copy .env.example .env` (Windows)\n2. Open the newly created .env file and fill in all required values\n3. Ensure all environment variables are properly set according to your local development environment\n4. Document any custom environment variables added to the project in the .env.example file with appropriate comments\n5. Verify that sensitive information (API keys, passwords, etc.) is not committed to version control\n6. Update the project documentation to include information about required environment variables\n</info added on 2025-05-07T14:26:08.046Z>\n\nNote: For Unsloth or GPU-based fine-tuning, use Google Colab or https://lightning.ai/lars/home. The local environment should only include packages compatible with Mac (Apple Silicon) and not require GPU or xformers.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Verification Phase",
          "description": "Test the complete development environment to ensure all components work together properly",
          "dependencies": [
            2
          ],
          "details": "Run test scripts to verify installations, validate configurations, test connections to external services, perform a sample build process, and document any issues encountered along with their resolutions. Test environment reproducibility by creating a new environment using the generated requirements.txt and uv.\n\nNote: Verify that only Mac (Apple Silicon) compatible packages are installed. GPU-dependent packages like Unsloth and xformers should not be included in the local environment. For GPU-accelerated workflows, document the process of using Google Colab or https://lightning.ai/lars/home instead.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Create and Maintain README.md for Developer Onboarding",
          "description": "Develop and maintain a comprehensive README.md file to help new developers set up the environment, understand the project structure, and follow best practices.",
          "details": "1. Write a clear project overview and purpose.\n2. Document the setup process, including using uv for environment management, installing dependencies, and configuring the .env file.\n3. Explain the directory structure and the role of key files (e.g., requirements.txt, .env, .env.example, notebooks, scripts).\n4. Provide instructions for running Jupyter notebooks and scripts.\n5. List common troubleshooting tips and FAQs.\n6. Include contribution guidelines and code style references.\n7. Update the README.md as the project evolves to ensure accuracy and completeness.\n8. Add a dedicated section explaining that the local environment is designed for Mac (Apple Silicon) compatibility and does not include GPU-dependent packages like Unsloth and xformers.\n9. Document how to use Google Colab or https://lightning.ai/lars/home for GPU-accelerated workflows and Unsloth-based fine-tuning.\n<info added on 2025-05-07T14:46:49.605Z>\n1. Write a clear project overview and purpose.\n2. Document the setup process, including using uv for environment management, installing dependencies, and configuring the .env file.\n3. Explain the directory structure and the role of key files (e.g., requirements.txt, .env, .env.example, notebooks, scripts).\n4. Provide instructions for running Jupyter notebooks and scripts.\n5. List common troubleshooting tips and FAQs.\n6. Include contribution guidelines and code style references.\n7. Update the README.md as the project evolves to ensure accuracy and completeness.\n\nNote: This task can be worked on independently and in parallel with others. It has no dependencies and is parallelizable: true.\n</info added on 2025-05-07T14:46:49.605Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 1
        },
        {
          "id": 5,
          "title": "Document Cloud-based GPU Environment Setup",
          "description": "Create documentation for setting up GPU-accelerated environments on Google Colab and Lightning.ai for Unsloth-based fine-tuning",
          "dependencies": [
            1
          ],
          "details": "1. Create a dedicated Jupyter notebook with setup instructions for Google Colab that includes:\n   - Installing Unsloth and other GPU-dependent packages\n   - Setting up authentication for W&B and Hugging Face\n   - Example code for fine-tuning with Unsloth\n2. Document the process for using Lightning.ai/lars for GPU-accelerated workflows\n3. Include instructions for transferring local work to cloud environments\n4. Document how to sync results and models back to the local environment\n5. Add troubleshooting tips specific to cloud GPU environments",
          "status": "done",
          "parentTaskId": 1
        }
      ]
    },
    {
      "id": 2,
      "title": "Token Extraction from GPT-2 Vocabulary",
      "description": "Extract all multi-character, letter-based tokens from the GPT-2 tokenizer vocabulary and save them to a JSON file.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "1. Load the GPT-2 tokenizer from Hugging Face\n2. Extract all tokens from the vocabulary\n3. Filter tokens to include only multi-character, letter-based tokens\n4. Save the filtered token list to a JSON file with the structure specified in the PRD\n5. Create a Jupyter notebook to verify token selection\n6. Analyze token frequency and length distribution\n\nThis task has been broken down into three parallelizable subtasks that can be worked on independently:\n- Script Writing: Implementing the token extraction logic\n- Validation & Testing: Ensuring the extracted tokens meet requirements\n- Documentation: Creating clear documentation for the process and results\n\nFile Organization:\n- Main token extraction script: `src/data/token_extractor.py`\n- Extracted tokens file: `data/processed/gpt2_letter_tokens.json`\n- Validation notebook: `notebooks/token_validation.ipynb`\n- Token analysis visualizations: `results/token_analysis/`\n- Documentation: `docs/token_extraction.md`\n\nImplementation:\n```python\nfrom transformers import GPT2Tokenizer\nimport json\nimport re\nimport os\n\ndef extract_tokens():\n    # Load tokenizer\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n    # Extract and filter tokens\n    filtered_tokens = []\n    for token_id, token_text in tokenizer.get_vocab().items():\n        # Remove special tokens and decode byte tokens\n        decoded_token = tokenizer.convert_tokens_to_string([token_text])\n\n        # Filter for multi-character letter-based tokens\n        if len(decoded_token) > 1 and re.match(r'^[a-zA-Z]+$', decoded_token):\n            filtered_tokens.append({\n                \"token\": decoded_token,\n                \"token_id\": token_id,\n                \"char_length\": len(decoded_token)\n            })\n\n    # Ensure directory exists\n    os.makedirs(\"data/processed\", exist_ok=True)\n    \n    # Save to file\n    with open(\"data/processed/gpt2_letter_tokens.json\", \"w\") as f:\n        json.dump({\"tokens\": filtered_tokens}, f, indent=2)\n\n    return filtered_tokens\n\ntokens = extract_tokens()\nprint(f\"Extracted {len(tokens)} multi-character letter-based tokens\")\n```\n\nResults:\n- Successfully extracted 46,789 tokens from the GPT-2 vocabulary\n- All tokens are multi-character and letter-based as required\n- Tokens saved to JSON file with proper structure",
      "testStrategy": "1. Verify JSON file is successfully created at `data/processed/gpt2_letter_tokens.json`\n2. Confirm file contains at least 5,000 tokens\n3. Randomly sample tokens to confirm they are multi-character and letter-based\n4. Verify no special tokens (like <|endoftext|>) are included\n5. Create visualizations of token length distribution and save to `results/token_analysis/`\n6. Commit token extraction script and results\n\nAll tests have been successfully completed. The extracted token set contains 46,789 tokens, which exceeds the minimum requirement of 5,000 tokens. Validation confirmed all tokens are multi-character and letter-based with no special tokens included.",
      "subtasks": [
        {
          "id": 2.1,
          "title": "Script Writing",
          "description": "Implement the token extraction logic from the GPT-2 vocabulary",
          "details": "1. Create the script at `src/data/token_extractor.py`\n2. Load the GPT-2 tokenizer from Hugging Face\n3. Extract all tokens from the vocabulary\n4. Filter tokens to include only multi-character, letter-based tokens\n5. Save the filtered token list to `data/processed/gpt2_letter_tokens.json` with the structure specified in the PRD\n6. Ensure all necessary directories are created if they don't exist\n\nThis task can be worked on independently and in parallel with others.\n\nparallelizable: true",
          "status": "completed"
        },
        {
          "id": 2.2,
          "title": "Validation & Testing",
          "description": "Ensure the extracted tokens meet requirements and create validation tools",
          "details": "1. Create a Jupyter notebook at `notebooks/token_validation.ipynb` to verify token selection\n2. Verify JSON file is successfully created at `data/processed/gpt2_letter_tokens.json`\n3. Confirm file contains at least 5,000 tokens\n4. Randomly sample tokens to confirm they are multi-character and letter-based\n5. Verify no special tokens (like <|endoftext|>) are included\n6. Create visualizations of token length distribution and save to `results/token_analysis/`\n\nThis task can be worked on independently and in parallel with others.\n\nparallelizable: true",
          "status": "completed"
        },
        {
          "id": 2.3,
          "title": "Documentation",
          "description": "Create clear documentation for the token extraction process and results",
          "details": "1. Create documentation file at `docs/token_extraction.md`\n2. Document the token extraction methodology\n3. Analyze token frequency and length distribution\n4. Create a README explaining how to use the extraction script\n5. Document any interesting patterns or observations in the token set\n6. Include references to file locations (`src/data/token_extractor.py`, `data/processed/gpt2_letter_tokens.json`, etc.)\n7. Prepare documentation for integration with other components\n\nThis task can be worked on independently and in parallel with others.\n\nparallelizable: true",
          "status": "completed"
        }
      ]
    },
    {
      "id": 3,
      "title": "Dataset Creation and Splitting",
      "description": "Create training, validation, and test datasets for spelling tasks, ensuring proper separation between training tokens and validation/test words to evaluate if training on spelling improves model performance on position and count question metrics.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "1. Download English word list from dwyl/english-words repository\n2. Create training set from tokenizer vocabulary (multi-character, letter-only tokens)\n3. Create validation/test sets from English dictionary words that:\n   - Split into multiple tokens by the tokenizer\n   - Have at least one token in the split that is multi-character and letter-only\n   - Do not appear in the training set\n4. Generate letter count questions (\"How many X's are in Y?\")\n5. Generate letter position questions (\"What is the Nth letter in Y?\")\n6. Split data based on source rather than percentage: training from tokenizer vocabulary, validation/test from filtered external word lists\n7. Format as a Hugging Face dataset with appropriate splits\n8. Create notebook to visualize dataset statistics\n9. Establish a Hugging Face benchmark with evaluation scripts and leaderboard integration\n\nNOTE: The dataset split is NOT based on percentage. The training set (universal set) comes from tokenizer vocabulary, while validation and test sets (hold-out sets) come from external word lists. This source-based split is essential for the experiment's purpose of determining if training on spelling improves model performance on position and count question metrics.\n\nFile Structure:\n- Raw word lists: `data/raw/word_lists/`\n- Processed word lists: `data/processed/word_lists/`\n- Training set: `data/splits/train.json`\n- Validation set: `data/splits/val.json`\n- Test set: `data/splits/test.json`\n- Question generation scripts: `src/data/question_generator.py` and `src/data/utils.py`\n- Dataset formatting scripts: `src/data/dataset_formatter.py` and `src/data/dataset_builder.py`\n- Documentation: `docs/dataset.md` and `docs/split_methodology.md`\n- Analysis notebooks: `notebooks/dataset_analysis.ipynb` and `notebooks/split_verification.ipynb`\n\nImplementation:\n```python\nimport json\nimport random\nimport string\nimport os\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nimport requests\n\n# Create directories if they don't exist\nos.makedirs(\"data/raw/word_lists\", exist_ok=True)\nos.makedirs(\"data/processed/word_lists\", exist_ok=True)\nos.makedirs(\"data/splits\", exist_ok=True)\n\n# Download English word list\nword_list_url = \"https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt\"\nresponse = requests.get(word_list_url)\nall_words = response.text.splitlines()\n\n# Save raw word list\nwith open(\"data/raw/word_lists/english_words.txt\", \"w\") as f:\n    f.write(\"\\n\".join(all_words))\n\n# Load tokenizer and filtered tokens\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nwith open(\"gpt2_letter_tokens.json\", \"r\") as f:\n    tokens_data = json.load(f)\n\ntokens = [t[\"token\"] for t in tokens_data[\"tokens\"]]\n\n# Save processed tokens\nwith open(\"data/processed/word_lists/tokenizer_vocabulary.json\", \"w\") as f:\n    json.dump({\"tokens\": tokens}, f, indent=2)\n\n# Find valid validation/test words\nvalid_words = []\nfor word in all_words:\n    if not word.isalpha():\n        continue\n    \n    # Tokenize the word\n    token_ids = tokenizer.encode(word, add_special_tokens=False)\n    tokens_in_word = tokenizer.convert_ids_to_tokens(token_ids)\n    \n    # Check if word splits into multiple tokens with at least one multi-character token\n    if len(tokens_in_word) > 1 and any(len(tokenizer.convert_tokens_to_string([t])) > 1 for t in tokens_in_word):\n        # Ensure word is not in training set\n        if word.lower() not in [t.lower() for t in tokens]:\n            valid_words.append(word)\n\n# Save processed valid words\nwith open(\"data/processed/word_lists/valid_external_words.json\", \"w\") as f:\n    json.dump({\"words\": valid_words}, f, indent=2)\n\n# Split valid words into validation and test sets\nval_words, test_words = train_test_split(valid_words, test_size=0.5, random_state=42)\n\n# Generate questions using question_generator.py\nfrom src.data.question_generator import generate_questions\n\n# Generate training, validation, and test questions\ntrain_questions = generate_questions(tokens, \"letter_count\") + generate_questions(tokens, \"letter_position\")\nval_questions = generate_questions(val_words, \"letter_count\") + generate_questions(val_words, \"letter_position\")\ntest_questions = generate_questions(test_words, \"letter_count\") + generate_questions(test_words, \"letter_position\")\n\n# Save splits to JSON files\nwith open(\"data/splits/train.json\", \"w\") as f:\n    json.dump({\"questions\": train_questions}, f, indent=2)\n\nwith open(\"data/splits/val.json\", \"w\") as f:\n    json.dump({\"questions\": val_questions}, f, indent=2)\n\nwith open(\"data/splits/test.json\", \"w\") as f:\n    json.dump({\"questions\": test_questions}, f, indent=2)\n\n# Create datasets using dataset_formatter.py and dataset_builder.py\nfrom src.data.dataset_formatter import format_dataset\nfrom src.data.dataset_builder import build_and_push_dataset\n\n# Format and build the dataset\ndatasets = format_dataset(train_questions, val_questions, test_questions)\ncombined_dataset = build_and_push_dataset(datasets, \"YOUR-USERNAME/llm-spelling-dataset\")\n```",
      "testStrategy": "1. Verify dataset successfully generates 2,000+ questions\n2. Confirm questions are grammatically correct\n3. Verify train/validation/test splits come from appropriate sources (training from tokenizer vocabulary, validation/test from filtered external words)\n4. Manually check 20 random samples to ensure answers correctly match questions\n5. Confirm dataset is successfully pushed to Hugging Face\n6. Verify local JSON files are created for each split in the correct locations:\n   - `data/splits/train.json`\n   - `data/splits/val.json`\n   - `data/splits/test.json`\n7. Create and review notebook `notebooks/dataset_analysis.ipynb` exploring dataset statistics\n8. Test evaluation scripts to ensure they correctly measure performance on position and count question metrics\n9. Verify benchmark integration with Hugging Face leaderboard\n10. Use `notebooks/split_verification.ipynb` to verify there is no overlap between the universal set (training) and hold-out sets (validation/test) to ensure valid measurement of model performance improvements\n11. Check that all documentation files (`docs/dataset.md` and `docs/split_methodology.md`) are complete and accurate",
      "subtasks": [
        {
          "id": 1,
          "title": "Word List Acquisition",
          "description": "Gather a comprehensive and diverse word list from reliable sources, ensuring coverage of the desired vocabulary scope.",
          "dependencies": [],
          "details": "Implementation involves sourcing words from open datasets, dictionaries, or APIs. Validation criteria include checking for duplicates, ensuring language consistency, and verifying word authenticity. Challenges include handling noisy data, inconsistent formats, and ensuring the list is representative of the target domain.\n<info added on 2025-05-07T14:45:31.121Z>\nImplementation involves sourcing words from open datasets, dictionaries, or APIs. Validation criteria include checking for duplicates, ensuring language consistency, and verifying word authenticity. Challenges include handling noisy data, inconsistent formats, and ensuring the list is representative of the target domain.\n\nThis task is broken down into three subtasks that can be executed in parallel:\n\n1. Sourcing: Identify and collect words from multiple reliable sources such as open datasets, dictionaries, APIs, and academic word lists. Focus on gathering a comprehensive set that covers the desired vocabulary scope. This task can be worked on independently and in parallel with others. (parallelizable: true)\n\n2. Cleaning: Process the collected words to remove duplicates, standardize formats, handle special characters, and ensure consistent casing. Address any encoding issues and normalize variations of the same word. This task can be worked on independently and in parallel with others. (parallelizable: true)\n\n3. Validation: Verify the authenticity and appropriateness of words in the list. Check for language consistency, filter out inappropriate content, and ensure the words meet the project's requirements. This task can be worked on independently and in parallel with others. (parallelizable: true)\n\nThe overall Word List Acquisition task is parallelizable, with team members able to work on different subtasks simultaneously to improve efficiency.\n</info added on 2025-05-07T14:45:31.121Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Training/Validation/Test Set Creation with Filtering",
          "description": "Create distinct datasets from different sources: training set from tokenizer vocabulary and validation/test sets from filtered external word lists to establish a true holdout set for testing.",
          "dependencies": [
            1
          ],
          "details": "Implementation requires extracting tokenizer vocabulary for training and applying strict filtering criteria to external word lists for validation/test sets. Ensure no overlap between training tokens and validation/test words. Validation involves statistical checks for distribution balance and manual spot checks for leakage. Challenges include maintaining diversity across splits and implementing robust filtering logic.\n\nFile Structure:\n- Raw word lists stored in: `data/raw/word_lists/`\n- Processed word lists stored in: `data/processed/word_lists/`\n- Training set saved to: `data/splits/train.json`\n- Validation set saved to: `data/splits/val.json`\n- Test set saved to: `data/splits/test.json`\n\nVerification of splits should be documented in `notebooks/split_verification.ipynb`.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Question Generation for Each Type",
          "description": "Automatically generate letter count and letter position questions for each word in the dataset to establish metrics for evaluating model performance.",
          "dependencies": [
            2
          ],
          "details": "Implementation uses templates to generate questions per word for both letter count and letter position types. Validation includes checking for grammatical correctness, relevance, and uniqueness of questions. Challenges involve ensuring variety in question phrasing and scaling generation efficiently across the universal set and holdout set.\n\nImplementation should be in:\n- Main script: `src/data/question_generator.py`\n- Utility functions: `src/data/utils.py`\n\nThe generated questions will be stored in the split files:\n- `data/splits/train.json`\n- `data/splits/val.json`\n- `data/splits/test.json`",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Dataset Formatting and Splitting",
          "description": "Format the generated data according to Hugging Face requirements, ensuring proper structure and metadata for each split based on their distinct sources.",
          "dependencies": [
            3
          ],
          "details": "Implementation involves structuring data as JSON, CSV, or other required formats, with clear fields for input, output, and metadata. Validation checks include schema compliance, correct split assignments, and tokenization compatibility. Challenges include handling edge cases in formatting and ensuring compatibility with downstream tools. Note that splits are based on source (not percentage): training uses tokenizer vocabulary while validation/test use external word lists.\n\nImplementation should use:\n- Main formatting script: `src/data/dataset_formatter.py`\n- HuggingFace dataset script: `src/data/dataset_builder.py`\n\nThe formatted datasets should be saved to:\n- `data/splits/train.json`\n- `data/splits/val.json`\n- `data/splits/test.json`",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Dataset Publishing and Benchmark Creation",
          "description": "Publish the finalized dataset to Hugging Face and establish a benchmark with evaluation scripts and leaderboard integration.",
          "dependencies": [
            4
          ],
          "details": "Implementation includes uploading dataset files, creating evaluation scripts that measure performance on position and count question metrics, integrating with Hugging Face leaderboard, and writing detailed documentation. Validation involves verifying downloadability, documentation clarity, and reproducibility. Challenges include ensuring evaluation scripts accurately reflect the experiment's purpose of determining if training on spelling improves model performance.\n\nDocumentation should be created in:\n- `docs/dataset.md` - General dataset documentation\n- `docs/split_methodology.md` - Detailed explanation of the split methodology\n\nDataset analysis should be performed in:\n- `notebooks/dataset_analysis.ipynb`",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Universal Set and Holdout Set Verification",
          "description": "Verify that the training set (universal set) and test set (true holdout set) are properly separated to enable valid measurement of model performance improvements.",
          "dependencies": [
            2
          ],
          "details": "Implementation involves comprehensive checks to ensure no overlap between training tokens and test words. Create verification scripts to confirm the integrity of the splits. Validation includes statistical analysis of word distributions and characteristics across splits. Challenges include defining appropriate metrics to verify the splits serve the experiment's purpose of determining if training on spelling improves model performance on position and count question metrics.\n\nVerification should be performed and documented in:\n- `notebooks/split_verification.ipynb`\n\nThis notebook should analyze the splits stored in:\n- `data/splits/train.json`\n- `data/splits/val.json`\n- `data/splits/test.json`",
          "status": "done"
        },
        {
          "id": 7,
          "title": "Source-Based Split Documentation",
          "description": "Document the source-based split approach and its importance for the experiment's validity.",
          "dependencies": [
            2,
            4
          ],
          "details": "Create clear documentation explaining why the dataset uses a source-based split (training from tokenizer vocabulary, validation/test from external word lists) rather than a percentage-based split. Explain how this approach creates a true universal set and hold-out set, which is essential for validly measuring if training on spelling improves model performance on position and count question metrics.\n\nDocumentation should be created in:\n- `docs/split_methodology.md` - Detailed explanation of the split methodology\n- `docs/dataset.md` - General dataset documentation with references to the split methodology\n\nThis documentation should also be included in the dataset card when publishing to Hugging Face.",
          "status": "done"
        }
      ]
    },
    {
      "id": 4,
      "title": "Training Data Formatting with Template Variations",
      "description": "Format the training data using various template formats for spelling examples to maximize LLM generalization and token-awareness.",
      "status": "done",
      "dependencies": [
        3
      ],
      "priority": "medium",
      "details": "1. Create a script to format the training data for fine-tuning\n2. Implement the template variations specified in the PRD, including:\n   - Simple variations (spelling first)\n   - Narrative/playful versions (spelling first)\n   - Educational/formal tone (spelling first)\n   - Spoken word/emphatic style (spelling first)\n   - Simple variations (word first)\n   - Narrative/playful versions (word first)\n   - Educational/formal tone (word first)\n   - Spoken word/emphatic style (word first)\n   - LLM-friendly structured training format (no \"spell\")\n3. Include additional variations for token separation:\n   - No separator between tokens\n   - Arrows between tokens\n   - Various punctuation and formatting\n4. Create Python scripts for analysis and visualization\n5. Implement efficient DataLoader with proper batching\n\nFile Structure:\n- Template definitions: `configs/templates/`\n- Template categories: `configs/templates/categories.json`\n- Token separation: `src/data/token_separator.py`\n- Template processor: `src/data/template_processor.py`\n- Example generator: `src/data/example_generator.py`\n- Data loader: `src/data/data_loader.py`\n- Formatted training data: `data/processed/training_data/`\n- Template variations: `data/processed/template_variations/`\n- Analysis scripts: `src/analysis/template_analysis.py`\n- Performance analysis: `src/analysis/template_performance.py`\n- Visualization utilities: `src/analysis/visualization_utils.py`\n- Report generator: `src/analysis/report_generator.py`\n- Results output: `results/token_analysis/`\n- Template documentation: `docs/templates.md`\n- Data format specification: `docs/data_format.md`\n\nImplementation:\n```python\ndef format_training_examples(dataset):\n    formatted_examples = []\n    \n    # Template categories\n    templates = {\n        \"spelling_first_simple\": [\n            \"s t r a w — that spells '{word}.'\\n\",\n            \"The letters s, t, r, a, w spell the word '{word}.'\\n\",\n            \"s-t-r-a-w makes the word '{word}.'\\n\",\n            \"Put together, s t r a w spells {word}.\\n\",\n            \"When you combine s, t, r, a, and w, you get {word}.\\n\"\n        ],\n        \"spelling_first_playful\": [\n            \"Say it with me: s...t...r...a...w — {word}!\\n\",\n            \"Five little letters — s, t, r, a, w — team up to make '{word}.'\\n\",\n            \"You line up s, t, r, a, and w, and what do you get? {word}!\\n\",\n            \"It starts with an 's' and ends with a 'w' — that's '{word}.'\\n\",\n            \"One letter at a time: s, t, r, a, w. Together? {word}.\\n\"\n        ],\n        # Add all other template categories from the PRD\n    }\n    \n    # Token separation styles\n    separators = [\n        \"\", # No separator\n        \" \", # Space\n        \", \", # Comma and space\n        \"-\", # Dash\n        \"...\", # Triple dots\n        \" -> \" # Arrow\n    ]\n    \n    for example in dataset:\n        word = example[\"word\"]\n        letters = list(word)\n        \n        # Randomly select template category and template\n        category = random.choice(list(templates.keys()))\n        template = random.choice(templates[category])\n        \n        # Randomly select separator\n        separator = random.choice(separators)\n        \n        # Format the letters with the chosen separator\n        spelled_letters = separator.join(letters)\n        \n        # Format the example using the template\n        formatted_text = template.format(word=word, letters=spelled_letters)\n        \n        formatted_examples.append({\n            \"input\": formatted_text,\n            \"output\": word,\n            \"template_category\": category,\n            \"separator\": separator\n        })\n    \n    return formatted_examples\n\n# Create custom collation function for efficient batching\ndef custom_collate_fn(batch):\n    input_ids = [item[\"input_ids\"] for item in batch]\n    attention_mask = [item[\"attention_mask\"] for item in batch]\n\n    # Pad sequences to the maximum length in the batch\n    max_length = max(len(ids) for ids in input_ids)\n\n    # Pad input_ids and attention_mask\n    input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n    attention_mask = [mask + [0] * (max_length - len(mask)) for mask in attention_mask]\n\n    # Convert to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_mask = torch.tensor(attention_mask)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask\n    }\n```",
      "testStrategy": "1. Verify script runs without errors\n2. Confirm dataset contains all template variations specified in the PRD\n3. Check that examples use a mix of punctuation and formatting\n4. Ensure no template is over-represented\n5. Test analysis scripts with sample data\n6. Test DataLoader with custom collation function\n7. Verify efficient batching with varied text lengths\n8. Validate that all files are created in the correct locations:\n   - Check template files in `configs/templates/`\n   - Verify processed data in `data/processed/training_data/`\n   - Ensure analysis scripts produce expected outputs in `results/token_analysis/`\n9. Test the complete pipeline from template processing to data loading\n10. Verify HTML reports are generated correctly\n11. Test command-line interfaces for analysis scripts",
      "subtasks": [
        {
          "id": 1,
          "title": "Template Design and Categorization",
          "description": "Create and categorize various template formats for training data based on different use cases and model requirements",
          "dependencies": [],
          "details": "Develop a comprehensive template system that supports various data types (text, images, audio, video). Create templates for different ML tasks and ensure they follow best practices for data formatting. Categorize templates based on complexity, use case, and required model architecture. Quality metrics should include template coverage, flexibility, and adherence to formatting standards. Test by validating templates with sample data across different domains.\n<info added on 2025-05-07T20:23:50.045Z>\nDevelop a comprehensive template system that supports various data types (text, images, audio, video). Create templates for different ML tasks and ensure they follow best practices for data formatting. Categorize templates based on complexity, use case, and required model architecture. Quality metrics should include template coverage, flexibility, and adherence to formatting standards. Test by validating templates with sample data across different domains.\n\nThe template design and categorization has been completed with the following structure:\n\n1. Template Categories:\n   - Spelling-first templates with variations: simple, playful, educational, and emphatic styles\n   - Word-first templates with variations: simple, playful, educational, and emphatic styles\n   - Structured templates: token-based and JSON-like formats\n\n2. Documentation:\n   - templates.md: Provides a comprehensive overview of the template system, categories, and usage guidelines\n   - data_format.md: Contains detailed specifications for data formats and processing guidelines\n\n3. Template Implementation Details:\n   - Multiple formatting styles implemented for each category\n   - Various token separation methods defined (to be implemented in next subtask)\n   - Structured formats designed specifically for machine learning applications\n   - Consistent variable substitution patterns established across all templates\n\n4. Project Organization:\n   - Template configurations stored in configs/templates/ directory\n   - Categories defined in configs/templates/categories.json\n   - Documentation placed in docs/ directory\n   - Clear file structure established for implementation phase\n\nThe template system is now fully designed and categorized, providing a solid foundation for the token separation strategy implementation in the next subtask.\n</info added on 2025-05-07T20:23:50.045Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Token Separation Strategy Implementation",
          "description": "Develop and implement various token separation strategies for different data types and model requirements",
          "dependencies": [
            1
          ],
          "details": "Research and implement multiple token separation approaches (whitespace, subword, character-level, etc.). Create a configurable system that allows switching between strategies based on language or data type. Develop custom tokenization rules for domain-specific data. Quality metrics should include tokenization accuracy, processing speed, and vocabulary coverage. Test with diverse multilingual datasets and measure impact on model performance. Implement in `src/data/token_separator.py`.\n<info added on 2025-05-07T20:26:12.086Z>\nResearch and implement multiple token separation approaches (whitespace, subword, character-level, etc.). Create a configurable system that allows switching between strategies based on language or data type. Develop custom tokenization rules for domain-specific data. Quality metrics should include tokenization accuracy, processing speed, and vocabulary coverage. Test with diverse multilingual datasets and measure impact on model performance. Implement in `src/data/token_separator.py`.\n\nThe TokenSeparator class has been successfully implemented in src/data/token_separator.py with the following features:\n\n1. Multiple built-in separator styles:\n   - none: tokens without separators\n   - space: tokens separated by spaces\n   - comma: tokens separated by commas\n   - dash: tokens separated by dashes\n   - dots: tokens separated by dots\n   - arrow: tokens separated by arrows\n\n2. A flexible SeparatorConfig dataclass that provides configuration options:\n   - Style selection from predefined styles\n   - Support for custom separator strings\n   - Control over spacing around separators\n   - Token capitalization options\n\n3. Utility functions to enhance usability:\n   - get_all_separator_examples(): Generates examples using all available styles\n   - create_custom(): Creates separators with custom configuration\n   - get_random_separator(): Selects a random style for variety in outputs\n\n4. A comprehensive test script (scripts/test_token_separator.py) that demonstrates:\n   - All built-in separator styles in action\n   - How to use custom separators\n   - Random style selection functionality\n   - Proper token processing workflow\n\n5. Testing with sample tokens confirms:\n   - All separator styles function as expected\n   - Proper spacing and formatting is maintained\n   - Custom separator functionality works correctly\n   - Random style selection provides appropriate variation\n\nThe implementation is now ready for integration with the template processor in the next subtask (Dynamic Example Generation System).\n</info added on 2025-05-07T20:26:12.086Z>",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Dynamic Example Generation System",
          "description": "Build a system that can dynamically generate training examples with appropriate variations and augmentations",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement data augmentation techniques for different data types (text rotation, image transformation, etc.). Create a pipeline for generating variations of training examples to prevent overfitting. Develop rules for maintaining data balance across classes. Quality metrics should include variation diversity, generation speed, and class distribution balance. Test by measuring model performance improvements with augmented data versus baseline. Implement in `src/data/example_generator.py` and store outputs in `data/processed/template_variations/`.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Efficient Data Loading and Batching",
          "description": "Optimize data loading and batching processes for improved training efficiency",
          "dependencies": [
            2,
            3
          ],
          "details": "Implement efficient data loading mechanisms that minimize memory usage and processing time. Develop smart batching strategies that group similar-length sequences together. Create data splitting functionality for training, validation, and testing sets. Quality metrics should include loading speed, memory efficiency, and training throughput. Test by benchmarking different loading approaches and measuring impact on training time. Implement in `src/data/data_loader.py`.",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Template Variation Analysis and Visualization",
          "description": "Analyze and visualize the effectiveness of different template variations on model performance",
          "dependencies": [
            1,
            3,
            4
          ],
          "details": "Develop Python scripts to analyze and visualize how different template designs affect model training. Create metrics to quantify template effectiveness across different data types and tasks. Implement automated analysis to recommend optimal template configurations. Quality metrics should include visualization clarity, analysis accuracy, and recommendation relevance. Test by comparing model performance across different template variations and validating analysis results.\n\nImplement the following scripts:\n- `src/analysis/template_analysis.py`: Main analysis script with command-line interface\n- `src/analysis/template_performance.py`: Performance analysis across template variations\n- `src/analysis/visualization_utils.py`: Shared plotting utilities for consistent visualization\n- `src/analysis/report_generator.py`: HTML report generation for easy sharing of results\n\nOutput structure:\n- `results/token_analysis/figures/`: All PNG/PDF visualizations\n- `results/token_analysis/reports/`: HTML reports\n- `results/token_analysis/data/`: Processed CSV/JSON data\n\nEnsure all scripts have proper command-line interfaces, documentation, and error handling.",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Documentation and File Structure Setup",
          "description": "Create and organize the file structure and documentation for template variations and data formatting",
          "dependencies": [],
          "details": "Set up the required directory structure for template files, implementation files, output files, and analysis files. Create comprehensive documentation in `docs/templates.md` and `docs/data_format.md` explaining the template system, data formats, and usage guidelines. Ensure all file paths are correctly referenced in the implementation code. Test by verifying that all directories exist and documentation is complete and accurate.",
          "status": "done"
        },
        {
          "id": 7,
          "title": "Analysis Scripts and Results Structure Setup",
          "description": "Set up the Python script-based analysis system and results directory structure",
          "dependencies": [
            6
          ],
          "details": "Create the necessary directory structure for analysis scripts and results output:\n\n1. Create the following directories:\n   - `src/analysis/` for all analysis scripts\n   - `results/token_analysis/figures/` for visualizations\n   - `results/token_analysis/reports/` for HTML reports\n   - `results/token_analysis/data/` for processed analysis data\n\n2. Set up script templates with proper imports, documentation, and command-line interfaces:\n   - `src/analysis/template_analysis.py`\n   - `src/analysis/template_performance.py`\n   - `src/analysis/visualization_utils.py`\n   - `src/analysis/report_generator.py`\n\n3. Implement basic functionality in each script:\n   - Command-line argument parsing\n   - Configuration loading\n   - Logging setup\n   - Error handling\n   - Basic documentation\n\n4. Create unit tests for each script to verify basic functionality\n\n5. Update documentation to reflect the new script-based analysis approach",
          "status": "done"
        }
      ]
    },
    {
      "id": 5,
      "title": "Baseline Model Evaluation",
      "description": "Evaluate the Qwen3-4B model on letter count and position tasks to establish a performance baseline for transfer learning effectiveness, leveraging Lightning.AI Studios for efficient computation and experiment tracking.",
      "status": "pending",
      "dependencies": [
        3
      ],
      "priority": "medium",
      "details": "1. Set up DsPy for multi-shot prompting\n2. Create Python scripts to evaluate the Qwen3-4B model\n3. Test on both primary metrics (letter count, letter position) using multi-token words\n4. Ensure output format is correct (single integer for character count or single letter for character position)\n5. Document the baseline performance for comparison\n\nQwen3-4B Configuration:\n- Implement both thinking and non-thinking modes for evaluation\n- Configure sampling parameters: Temperature=0.6, TopP=0.95, TopK=20, MinP=0 for thinking mode\n- Focus evaluation on English-only token subset\n- Compare performance between thinking and non-thinking modes\n\nLightning.AI Studios Integration:\n- Create a dedicated evaluation Studio following the \"one Studio, one task\" principle\n- Utilize GPU switching feature (CPU → T4 → A100) for cost-effective evaluation\n- Ensure sufficient memory allocation for Qwen3-4B model requirements\n- Leverage Lightning.AI plugins for experiment tracking and visualization\n- Use shared filesystem for accessing models and datasets\n\nData Sources:\n- Training data: Spelling variations with multicharacter tokens\n- Evaluation data: Multi-token words for position and character count questions\n\nEvaluation Approach:\n- Focus on measuring transfer learning effectiveness from spelling training to position/count tasks\n- No traditional train/val/test split since evaluation uses different data\n- Separate evaluation pipeline for position and count metrics\n- Implement evaluation metrics using TorchMetrics for optimized distributed evaluation\n\nFile Structure:\n- Main framework: `src/evaluation/framework.py`\n- Metrics definitions: `src/evaluation/metrics.py`\n- Evaluation config: `configs/evaluation/base_config.yaml`\n- Lightning.AI Studio config: `configs/evaluation/lightning_studio.yaml`\n- Letter count evaluator: `src/evaluation/letter_count.py`\n- Letter position evaluator: `src/evaluation/letter_position.py`\n- Common utilities: `src/evaluation/utils.py`\n- Visualization utilities: `src/evaluation/visualization.py`\n- Report generation: `src/evaluation/report.py`\n- Results directory: `results/evaluation/`\n- Visualizations: `results/evaluation/figures/`\n- HTML reports: `results/evaluation/reports/`\n- Processed data: `results/evaluation/data/`\n- Raw metrics: `results/evaluation/data/metrics.json`\n- Detailed analysis: `results/evaluation/data/analysis.json`\n\nImplementation:\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport dspy\nimport wandb\nimport argparse\nimport os\nimport json\nimport pandas as pd\nimport torchmetrics\nfrom lightning.pytorch import loggers as pl_loggers\nfrom lightning.app import LightningApp, LightningWork\nfrom lightning.app.components import TracerPythonScript\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Evaluate Qwen3-4B model on spelling tasks\")\n    parser.add_argument(\"--model\", type=str, default=\"Qwen/Qwen3-4B\", help=\"Model to evaluate\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"results/evaluation\", help=\"Directory to save results\")\n    parser.add_argument(\"--log_wandb\", action=\"store_true\", help=\"Whether to log results to W&B\")\n    parser.add_argument(\"--use_lightning\", action=\"store_true\", help=\"Whether to use Lightning.AI for distributed evaluation\")\n    parser.add_argument(\"--gpu_tier\", type=str, default=\"T4\", choices=[\"CPU\", \"T4\", \"A100\"], help=\"GPU tier to use for evaluation\")\n    parser.add_argument(\"--thinking_mode\", action=\"store_true\", help=\"Whether to use thinking mode for Qwen3\")\n    return parser.parse_args()\n\nclass EvaluationWork(LightningWork):\n    def __init__(self, model_name, output_dir, log_wandb=False, gpu_tier=\"T4\", thinking_mode=False):\n        super().__init__(cloud_compute={\"gpu_type\": gpu_tier.lower() if gpu_tier != \"CPU\" else None})\n        self.model_name = model_name\n        self.output_dir = output_dir\n        self.log_wandb = log_wandb\n        self.thinking_mode = thinking_mode\n        \n    def run(self):\n        # Create output directories\n        os.makedirs(f\"{self.output_dir}/data\", exist_ok=True)\n        os.makedirs(f\"{self.output_dir}/figures\", exist_ok=True)\n        os.makedirs(f\"{self.output_dir}/reports\", exist_ok=True)\n        \n        # Initialize W&B if requested\n        if self.log_wandb:\n            wandb.init(project=\"llm-spelling-finetuning\", name=\"qwen3-baseline-evaluation\")\n        \n        # Load Qwen3-4B model and tokenizer\n        model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map=\"auto\", trust_remote_code=True)\n        model.eval()\n        tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n        \n        # Load evaluation dataset with multi-token words\n        from datasets import load_dataset\n        eval_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\", split=\"evaluation\")\n        \n        # Initialize TorchMetrics for evaluation\n        letter_count_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=20)  # Assuming max 20 characters\n        letter_position_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=26)  # A-Z positions\n        \n        # Define generation function with Qwen3-specific parameters\n        def generate_answer(model, tokenizer, question, max_length=10):\n            inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n            \n            # Configure generation parameters based on thinking mode\n            generation_config = {\n                \"max_new_tokens\": max_length,\n                \"do_sample\": True if self.thinking_mode else False,\n                \"pad_token_id\": tokenizer.pad_token_id,\n            }\n            \n            # Add thinking mode specific parameters\n            if self.thinking_mode:\n                generation_config.update({\n                    \"temperature\": 0.6,\n                    \"top_p\": 0.95,\n                    \"top_k\": 20,\n                    \"min_p\": 0,\n                })\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    inputs.input_ids,\n                    **generation_config\n                )\n            \n            response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n            \n            # Extract just the first token/character for letter position or first number for letter count\n            if \"How many\" in question:\n                # Extract first number\n                import re\n                numbers = re.findall(r'\\d+', response)\n                return numbers[0] if numbers else response.strip()\n            else:\n                # Extract first character\n                return response.strip()[0] if response.strip() else \"\"\n        \n        # Evaluate on letter count questions\n        def evaluate_letter_count(model, tokenizer, dataset):\n            correct = 0\n            total = 0\n            results = []\n            \n            for item in dataset:\n                if item[\"question_type\"] != \"letter_count\":\n                    continue\n                    \n                prediction = generate_answer(model, tokenizer, item[\"question\"])\n                is_correct = prediction == item[\"answer\"]\n                \n                results.append({\n                    \"question\": item[\"question\"],\n                    \"expected\": item[\"answer\"],\n                    \"prediction\": prediction,\n                    \"correct\": is_correct\n                })\n                \n                correct += int(is_correct)\n                total += 1\n            \n            accuracy = correct / total if total > 0 else 0\n            print(f\"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})\")\n            \n            return accuracy, results\n        \n        # Evaluate on letter position questions\n        def evaluate_letter_position(model, tokenizer, dataset):\n            correct = 0\n            total = 0\n            results = []\n            \n            for item in dataset:\n                if item[\"question_type\"] != \"letter_position\":\n                    continue\n                    \n                prediction = generate_answer(model, tokenizer, item[\"question\"])\n                is_correct = prediction.lower() == item[\"answer\"].lower()\n                \n                results.append({\n                    \"question\": item[\"question\"],\n                    \"expected\": item[\"answer\"],\n                    \"prediction\": prediction,\n                    \"correct\": is_correct\n                })\n                \n                correct += int(is_correct)\n                total += 1\n            \n            accuracy = correct / total if total > 0 else 0\n            print(f\"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})\")\n            \n            return accuracy, results\n        \n        # Run evaluation\n        count_accuracy, count_results = evaluate_letter_count(model, tokenizer, eval_dataset)\n        position_accuracy, position_results = evaluate_letter_position(model, tokenizer, eval_dataset)\n        \n        # Log results to W&B if requested\n        if self.log_wandb:\n            wandb.log({\n                \"letter_count_accuracy\": count_accuracy,\n                \"letter_position_accuracy\": position_accuracy,\n                \"thinking_mode\": self.thinking_mode,\n                \"count_examples\": wandb.Table(dataframe=pd.DataFrame(count_results[:20])),\n                \"position_examples\": wandb.Table(dataframe=pd.DataFrame(position_results[:20]))\n            })\n        \n        # Save results locally\n        metrics_data = {\n            \"model\": self.model_name,\n            \"thinking_mode\": self.thinking_mode,\n            \"letter_count_accuracy\": count_accuracy,\n            \"letter_position_accuracy\": position_accuracy,\n            \"count_results\": count_results,\n            \"position_results\": position_results\n        }\n        \n        # Create a filename that includes the thinking mode information\n        mode_suffix = \"thinking\" if self.thinking_mode else \"standard\"\n        metrics_filename = f\"metrics_{mode_suffix}.json\"\n        \n        with open(f\"{self.output_dir}/data/{metrics_filename}\", \"w\") as f:\n            json.dump(metrics_data, f, indent=2)\n        \n        # Generate visualizations\n        from src.evaluation.visualization import create_accuracy_chart, create_error_analysis\n        create_accuracy_chart(metrics_data, f\"{self.output_dir}/figures/accuracy_{mode_suffix}.png\")\n        create_error_analysis(metrics_data, f\"{self.output_dir}/figures/error_analysis_{mode_suffix}.png\")\n        \n        # Generate HTML report\n        from src.evaluation.report import generate_html_report\n        generate_html_report(metrics_data, f\"{self.output_dir}/reports/baseline_report_{mode_suffix}.html\")\n        \n        print(f\"Evaluation complete. Results saved to {self.output_dir}\")\n\ndef main():\n    args = parse_args()\n    \n    if args.use_lightning:\n        # Use Lightning.AI for distributed evaluation\n        eval_work = EvaluationWork(\n            model_name=args.model,\n            output_dir=args.output_dir,\n            log_wandb=args.log_wandb,\n            gpu_tier=args.gpu_tier,\n            thinking_mode=args.thinking_mode\n        )\n        app = LightningApp(eval_work)\n        app.run()\n    else:\n        # Create output directories\n        os.makedirs(f\"{args.output_dir}/data\", exist_ok=True)\n        os.makedirs(f\"{args.output_dir}/figures\", exist_ok=True)\n        os.makedirs(f\"{args.output_dir}/reports\", exist_ok=True)\n        \n        # Initialize W&B if requested\n        if args.log_wandb:\n            wandb.init(project=\"llm-spelling-finetuning\", name=\"qwen3-baseline-evaluation\")\n        \n        # Load Qwen3-4B model and tokenizer\n        model_name = args.model\n        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n        model.eval()\n        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        \n        # Load evaluation dataset with multi-token words\n        from datasets import load_dataset\n        eval_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\", split=\"evaluation\")\n        \n        # Define generation function with Qwen3-specific parameters\n        def generate_answer(model, tokenizer, question, max_length=10):\n            inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n            \n            # Configure generation parameters based on thinking mode\n            generation_config = {\n                \"max_new_tokens\": max_length,\n                \"do_sample\": True if args.thinking_mode else False,\n                \"pad_token_id\": tokenizer.pad_token_id,\n            }\n            \n            # Add thinking mode specific parameters\n            if args.thinking_mode:\n                generation_config.update({\n                    \"temperature\": 0.6,\n                    \"top_p\": 0.95,\n                    \"top_k\": 20,\n                    \"min_p\": 0,\n                })\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    inputs.input_ids,\n                    **generation_config\n                )\n            \n            response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n            \n            # Extract just the first token/character for letter position or first number for letter count\n            if \"How many\" in question:\n                # Extract first number\n                import re\n                numbers = re.findall(r'\\d+', response)\n                return numbers[0] if numbers else response.strip()\n            else:\n                # Extract first character\n                return response.strip()[0] if response.strip() else \"\"\n        \n        # Evaluate on letter count questions\n        def evaluate_letter_count(model, tokenizer, dataset):\n            correct = 0\n            total = 0\n            results = []\n            \n            for item in dataset:\n                if item[\"question_type\"] != \"letter_count\":\n                    continue\n                    \n                prediction = generate_answer(model, tokenizer, item[\"question\"])\n                is_correct = prediction == item[\"answer\"]\n                \n                results.append({\n                    \"question\": item[\"question\"],\n                    \"expected\": item[\"answer\"],\n                    \"prediction\": prediction,\n                    \"correct\": is_correct\n                })\n                \n                correct += int(is_correct)\n                total += 1\n            \n            accuracy = correct / total if total > 0 else 0\n            print(f\"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})\")\n            \n            return accuracy, results\n        \n        # Evaluate on letter position questions\n        def evaluate_letter_position(model, tokenizer, dataset):\n            correct = 0\n            total = 0\n            results = []\n            \n            for item in dataset:\n                if item[\"question_type\"] != \"letter_position\":\n                    continue\n                    \n                prediction = generate_answer(model, tokenizer, item[\"question\"])\n                is_correct = prediction.lower() == item[\"answer\"].lower()\n                \n                results.append({\n                    \"question\": item[\"question\"],\n                    \"expected\": item[\"answer\"],\n                    \"prediction\": prediction,\n                    \"correct\": is_correct\n                })\n                \n                correct += int(is_correct)\n                total += 1\n            \n            accuracy = correct / total if total > 0 else 0\n            print(f\"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})\")\n            \n            return accuracy, results\n        \n        # Run evaluation\n        count_accuracy, count_results = evaluate_letter_count(model, tokenizer, eval_dataset)\n        position_accuracy, position_results = evaluate_letter_position(model, tokenizer, eval_dataset)\n        \n        # Log results to W&B if requested\n        if args.log_wandb:\n            wandb.log({\n                \"letter_count_accuracy\": count_accuracy,\n                \"letter_position_accuracy\": position_accuracy,\n                \"thinking_mode\": args.thinking_mode,\n                \"count_examples\": wandb.Table(dataframe=pd.DataFrame(count_results[:20])),\n                \"position_examples\": wandb.Table(dataframe=pd.DataFrame(position_results[:20]))\n            })\n        \n        # Save results locally\n        metrics_data = {\n            \"model\": model_name,\n            \"thinking_mode\": args.thinking_mode,\n            \"letter_count_accuracy\": count_accuracy,\n            \"letter_position_accuracy\": position_accuracy,\n            \"count_results\": count_results,\n            \"position_results\": position_results\n        }\n        \n        # Create a filename that includes the thinking mode information\n        mode_suffix = \"thinking\" if args.thinking_mode else \"standard\"\n        metrics_filename = f\"metrics_{mode_suffix}.json\"\n        \n        with open(f\"{args.output_dir}/data/{metrics_filename}\", \"w\") as f:\n            json.dump(metrics_data, f, indent=2)\n        \n        # Generate visualizations\n        from src.evaluation.visualization import create_accuracy_chart, create_error_analysis\n        create_accuracy_chart(metrics_data, f\"{args.output_dir}/figures/accuracy_{mode_suffix}.png\")\n        create_error_analysis(metrics_data, f\"{args.output_dir}/figures/error_analysis_{mode_suffix}.png\")\n        \n        # Generate HTML report\n        from src.evaluation.report import generate_html_report\n        generate_html_report(metrics_data, f\"{args.output_dir}/reports/baseline_report_{mode_suffix}.html\")\n        \n        print(f\"Evaluation complete. Results saved to {args.output_dir}\")\n\nif __name__ == \"__main__\":\n    main()\n```",
      "testStrategy": "1. Verify the evaluation scripts run without errors with Qwen3-4B model\n2. Test command-line arguments for flexibility, including Lightning.AI specific arguments and thinking mode toggle\n3. Confirm output format is correct (single integer for count, single letter for position)\n4. Check that results are properly logged to W&B when specified, including thinking mode parameter\n5. Verify baseline performance metrics are saved to `results/evaluation/data/metrics_thinking.json` and `results/evaluation/data/metrics_standard.json`\n6. Ensure visualizations are correctly generated in `results/evaluation/figures/` for both thinking and standard modes\n7. Validate HTML reports are generated in `results/evaluation/reports/` for both thinking and standard modes\n8. Test error handling for edge cases\n9. Analyze error patterns in Qwen3-4B model predictions and document in `results/evaluation/data/analysis.json`\n10. Ensure documentation is complete in `docs/evaluation.md`, `docs/metrics.md`, `docs/baseline_results.md`, and `docs/lightning_studio_setup.md`\n11. Verify that the Python scripts can be imported and used as modules by other components\n12. Confirm the evaluation correctly uses multi-token words for position and count tasks\n13. Validate that the evaluation framework properly measures transfer learning effectiveness\n14. Test for correlation analysis between spelling training and position/count task performance\n15. Verify Lightning.AI Studio setup and configuration works correctly with Qwen3-4B's memory requirements\n16. Test GPU switching functionality (CPU → T4 → A100) for cost optimization\n17. Validate TorchMetrics integration for distributed evaluation\n18. Test automated job submission for batch evaluations with both thinking and non-thinking modes\n19. Verify environment isolation and dependency management in Lightning.AI Studio\n20. Compare performance between thinking and non-thinking modes to determine optimal configuration\n21. Test English-only token subset focus in evaluation",
      "subtasks": [
        {
          "id": 1,
          "title": "Evaluation Framework Setup",
          "description": "Establish the hierarchical evaluation framework structure for NLP model assessment",
          "dependencies": [],
          "details": "Create a modular evaluation framework that supports both automated and human evaluation components. Implement a transfer learning evaluation approach using multi-token words for position and count tasks. Ensure the framework can handle diverse linguistic structures and edge cases. Set up configuration files for evaluation parameters and thresholds.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Metrics Definition and Implementation",
          "description": "Define and implement comprehensive evaluation metrics for model assessment",
          "dependencies": [
            1
          ],
          "details": "Implement position accuracy and count accuracy metrics for multi-token words. Add specialized metrics for transfer learning effectiveness evaluation. Create a metrics registry system that allows for easy addition of new metrics. Ensure all metrics are properly documented with mathematical formulations. Implement analysis of error patterns and potential correlation with spelling training.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Letter Count Evaluator Implementation",
          "description": "Develop specialized evaluator for letter count assessment",
          "dependencies": [
            2
          ],
          "details": "Implement a dedicated evaluator that analyzes the model's ability to count letters in multi-token words. Create test cases with varying complexity levels. Implement error tolerance thresholds. Design the evaluator to track performance across different text lengths. Include detailed logging of evaluation results for later analysis of transfer learning effectiveness.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Position Evaluator Implementation",
          "description": "Develop specialized evaluator for letter position assessment",
          "dependencies": [
            2
          ],
          "details": "Create an evaluator that tests the model's ability to identify letter positions within multi-token words. Implement position-based metrics including absolute and relative position accuracy. Design test cases with varying complexity. Include support for different character sets. Implement detailed error tracking for position-based mistakes to analyze transfer learning effectiveness.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Results Visualization System",
          "description": "Develop comprehensive visualization tools for evaluation results",
          "dependencies": [
            2,
            3,
            4
          ],
          "details": "Create interactive dashboards showing performance across all metrics. Implement comparison visualizations between model versions. Design visualizations to show transfer learning effectiveness from spelling training to position/count tasks. Include error distribution visualizations. Ensure all visualizations are exportable in multiple formats (PNG, PDF, interactive HTML).",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Error Analysis Framework",
          "description": "Implement systematic error analysis capabilities",
          "dependencies": [
            3,
            4,
            5
          ],
          "details": "Develop tools to categorize and analyze error patterns. Create clustering algorithms to group similar errors. Implement analysis of correlation between spelling training and position/count task performance. Design interfaces for domain experts to review and annotate errors. Include recommendation generation for model improvements based on error patterns.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Documentation and Reporting",
          "description": "Create comprehensive documentation and automated reporting",
          "dependencies": [
            1,
            2,
            5,
            6
          ],
          "details": "Document the entire evaluation framework architecture with focus on transfer learning approach. Create user guides for running evaluations. Implement automated report generation with executive summaries and detailed technical appendices. Include benchmark comparisons against industry standards. Design templates for different stakeholder audiences (technical, management, etc.).",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Integration with External Components",
          "description": "Ensure seamless integration with other system components",
          "dependencies": [
            1,
            2,
            5,
            7
          ],
          "details": "Develop APIs for integration with model training pipelines. Implement webhooks for continuous evaluation triggers. Create data exchange formats for evaluation results. Design integration with CI/CD pipelines for automated testing. Implement monitoring capabilities to track transfer learning effectiveness over time.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Transfer Learning Effectiveness Analysis",
          "description": "Implement analysis of transfer learning from spelling training to position/count tasks",
          "dependencies": [
            3,
            4,
            6
          ],
          "details": "Develop metrics to quantify transfer learning effectiveness. Create visualization tools to show correlation between spelling training and position/count task performance. Implement statistical analysis to identify significant patterns. Design experiments to test different transfer learning hypotheses. Document findings in comprehensive reports.",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "Lightning.AI Studio Setup",
          "description": "Configure dedicated Lightning.AI Studio for evaluation tasks",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated evaluation Studio following the \"one Studio, one task\" principle. Configure proper environment isolation and dependency management. Set up shared filesystem access for models and datasets. Implement GPU switching functionality (CPU → T4 → A100) for cost optimization. Create configuration files for Lightning.AI Studio setup.",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "TorchMetrics Integration",
          "description": "Implement evaluation metrics using TorchMetrics for optimized distributed evaluation",
          "dependencies": [
            2,
            10
          ],
          "details": "Refactor existing metrics to use TorchMetrics for better performance in distributed environments. Implement custom TorchMetrics classes for specialized evaluation needs. Ensure metrics are properly synchronized across distributed processes. Add support for metric serialization and deserialization for result persistence.",
          "status": "pending"
        },
        {
          "id": 12,
          "title": "Automated Job Submission",
          "description": "Set up automated job submission for batch evaluations",
          "dependencies": [
            10,
            11
          ],
          "details": "Implement batch job submission system for running multiple evaluations. Create job templates for different evaluation scenarios. Set up job scheduling and prioritization. Implement notification system for job completion. Design job monitoring dashboard for tracking evaluation progress.",
          "status": "pending"
        },
        {
          "id": 13,
          "title": "Lightning.AI Documentation",
          "description": "Create comprehensive documentation for Lightning.AI Studio setup and usage",
          "dependencies": [
            10,
            11,
            12
          ],
          "details": "Document Lightning.AI Studio setup process. Create tutorials for running evaluations in Lightning.AI. Document GPU switching functionality and cost optimization strategies. Include troubleshooting guides for common issues. Create reference documentation for all Lightning.AI specific configuration options.",
          "status": "pending"
        },
        {
          "id": 14,
          "title": "Qwen3-4B Thinking Mode Implementation",
          "description": "Implement and evaluate Qwen3-4B's thinking mode capabilities",
          "dependencies": [
            1,
            2
          ],
          "details": "Configure Qwen3-4B's thinking mode with specified parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0). Implement comparison framework to evaluate performance differences between thinking and non-thinking modes. Create visualization tools to highlight performance differences. Document best practices for using thinking mode in different evaluation scenarios.",
          "status": "pending"
        },
        {
          "id": 15,
          "title": "English-only Token Subset Focus",
          "description": "Configure evaluation to focus on English-only token subset",
          "dependencies": [
            1,
            14
          ],
          "details": "Implement filtering mechanisms to focus evaluation on English-only token subset. Create analysis tools to measure performance differences between full vocabulary and English-only subset. Document impact of token subset focus on evaluation results. Implement visualization tools to highlight performance differences across token subsets.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 6,
      "title": "Hyperparameter Tuning Infrastructure",
      "description": "Create a configuration system for hyperparameter experiments focused on spelling task performance and transfer learning, and set up experiment tracking with Weights & Biases using Python scripts instead of notebooks, leveraging Lightning.AI Studios for efficient training and experimentation.",
      "status": "pending",
      "dependencies": [
        1,
        5
      ],
      "priority": "medium",
      "details": "1. Create a configuration system for hyperparameter experiments with focus on spelling tasks and transfer learning\n2. Set up Python scripts for experiment tracking with separate metrics for spelling and transfer learning\n3. Create a script that can run training with different hyperparameters optimized for transfer learning\n4. Set up W&B for experiment tracking with correlation analysis between spelling and transfer learning\n5. Define a clear set of metrics for comparing experiments across both direct and transfer performance\n6. Create a dedicated Lightning.AI Studio for hyperparameter tuning following the \"one Studio, one task\" principle\n7. Implement GPU switching for efficient resource usage (CPU → T4 → A100)\n8. Leverage Lightning.AI's job system for managing multiple training runs\n9. Add support for Qwen3-4B tokenizer analysis, focusing on English-only token subset\n10. Implement analysis of multi-token word behavior in Qwen3-4B\n\nFile Structure:\n- Base configs: `configs/hyperparameters/`\n- Model configs: `configs/hyperparameters/models/`\n- Training configs: `configs/hyperparameters/training/`\n- Evaluation configs: `configs/hyperparameters/evaluation/`\n- Search space definitions: `configs/hyperparameters/search_spaces/`\n- Lightning.AI configs: `configs/hyperparameters/lightning/`\n- Token analysis configs: `configs/hyperparameters/token_analysis/`\n\nPython Module Structure:\n- Config manager: `src/tuning/config.py`\n- W&B integration: `src/tuning/wandb_integration.py`\n- Lightning.AI integration: `src/tuning/lightning_integration.py`\n- Grid search: `src/tuning/grid.py`\n- Experiment executor: `src/tuning/executor.py`\n- Visualization tools: `src/tuning/visualization.py`\n- Report generation: `src/tuning/report.py`\n- Transfer learning analysis: `src/tuning/transfer_analysis.py`\n- Lightning.AI job manager: `src/tuning/lightning_jobs.py`\n- Token analysis: `src/tuning/token_analysis.py`\n- Qwen3 tokenizer utilities: `src/tuning/qwen3_tokenizer.py`\n\nResults Structure:\n- Experiment results: `results/tuning/data/`\n- Best configurations: `results/tuning/configs/`\n- Performance plots: `results/tuning/figures/`\n- Transfer learning analysis: `results/tuning/transfer/`\n- HTML reports: `results/tuning/reports/`\n- Token analysis: `results/tuning/token_analysis/`\n- English token subset: `results/tuning/token_analysis/english_tokens/`\n- Documentation: `docs/hyperparameter_tuning.md`, `docs/config_system.md`, `docs/tuning_results.md`, `docs/transfer_learning.md`, `docs/lightning_studio_setup.md`, `docs/qwen3_token_analysis.md`\n\nImplementation:\n```python\nimport yaml\nimport os\nfrom datetime import datetime\nimport wandb\nimport argparse\nimport pytorch_lightning as pl\nfrom lightning_app import LightningApp, LightningFlow, LightningWork\nfrom lightning_app.storage import Path\nfrom transformers import AutoTokenizer\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef create_experiment_config(\n    exp_name,\n    lora_r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    learning_rate=2e-4,\n    batch_size=8,\n    grad_accum_steps=4,\n    max_steps=1000,\n    warmup_steps=100,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    # Spelling-specific parameters\n    spelling_data_ratio=0.7,\n    spelling_augmentation=True,\n    spelling_difficulty=\"medium\",\n    # Transfer learning parameters\n    transfer_eval_frequency=100,\n    position_task_weight=0.5,\n    count_task_weight=0.5,\n    # Lightning.AI parameters\n    gpu_tier=\"cpu\",  # Options: \"cpu\", \"t4\", \"a100\"\n    auto_scale=True,\n    sleep_when_idle=True,\n    # Qwen3 tokenizer parameters\n    use_english_only_tokens=False,\n    analyze_thinking_mode=False,\n):\n    \"\"\"Create and save an experiment configuration with spelling and transfer learning focus.\"\"\"\n    config = {\n        \"experiment_name\": exp_name,\n        \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n        \"lora_config\": {\n            \"r\": lora_r,\n            \"alpha\": lora_alpha,\n            \"dropout\": lora_dropout,\n            \"target_modules\": target_modules,\n        },\n        \"training_config\": {\n            \"learning_rate\": learning_rate,\n            \"per_device_train_batch_size\": batch_size,\n            \"gradient_accumulation_steps\": grad_accum_steps,\n            \"max_steps\": max_steps,\n            \"warmup_steps\": warmup_steps,\n        },\n        \"spelling_config\": {\n            \"data_ratio\": spelling_data_ratio,\n            \"augmentation\": spelling_augmentation,\n            \"difficulty\": spelling_difficulty,\n        },\n        \"transfer_config\": {\n            \"eval_frequency\": transfer_eval_frequency,\n            \"position_task_weight\": position_task_weight,\n            \"count_task_weight\": count_task_weight,\n        },\n        \"lightning_config\": {\n            \"gpu_tier\": gpu_tier,\n            \"auto_scale\": auto_scale,\n            \"sleep_when_idle\": sleep_when_idle,\n        },\n        \"tokenizer_config\": {\n            \"model_name\": \"Qwen/Qwen3-4B\",\n            \"use_english_only_tokens\": use_english_only_tokens,\n            \"analyze_thinking_mode\": analyze_thinking_mode,\n        }\n    }\n\n    # Create experiments directory if it doesn't exist\n    os.makedirs(\"configs/hyperparameters/\", exist_ok=True)\n\n    # Save config to file\n    config_path = f\"configs/hyperparameters/{exp_name}_{config['timestamp']}.yaml\"\n    with open(config_path, \"w\") as f:\n        yaml.dump(config, f)\n\n    print(f\"Created experiment config: {config_path}\")\n    return config_path\n\n# Define hyperparameter grid with spelling and transfer learning focus\ndef create_hyperparameter_grid():\n    grid = {\n        # LoRA parameters\n        \"lora_r\": [4, 8, 16, 32],\n        \"lora_alpha\": [8, 16, 32, 64],\n        # Training parameters\n        \"learning_rate\": [1e-4, 2e-4, 5e-4, 1e-3],\n        \"batch_size\": [4, 8, 16, 32],\n        \"grad_accum_steps\": [1, 2, 4, 8],\n        \"max_steps\": [500, 1000, 2000, 5000],\n        # Spelling-specific parameters\n        \"spelling_data_ratio\": [0.5, 0.7, 0.9],\n        \"spelling_difficulty\": [\"easy\", \"medium\", \"hard\"],\n        # Transfer learning parameters\n        \"position_task_weight\": [0.3, 0.5, 0.7],\n        \"count_task_weight\": [0.3, 0.5, 0.7],\n        # Lightning.AI parameters\n        \"gpu_tier\": [\"cpu\", \"t4\", \"a100\"],\n        # Qwen3 tokenizer parameters\n        \"use_english_only_tokens\": [False, True],\n        \"analyze_thinking_mode\": [False, True],\n    }\n    return grid\n\n# Create experiment configs for grid search\ndef create_grid_search_configs(base_name=\"spelling_transfer_exp\"):\n    grid = create_hyperparameter_grid()\n    configs = []\n    \n    # Start with default configuration\n    configs.append(create_experiment_config(f\"{base_name}_default\"))\n    \n    # Create configs for each hyperparameter variation\n    for param, values in grid.items():\n        for value in values:\n            # Skip the default value\n            if param == \"lora_r\" and value == 16: continue\n            if param == \"lora_alpha\" and value == 32: continue\n            if param == \"learning_rate\" and value == 2e-4: continue\n            if param == \"batch_size\" and value == 8: continue\n            if param == \"grad_accum_steps\" and value == 4: continue\n            if param == \"max_steps\" and value == 1000: continue\n            if param == \"spelling_data_ratio\" and value == 0.7: continue\n            if param == \"spelling_difficulty\" and value == \"medium\": continue\n            if param == \"position_task_weight\" and value == 0.5: continue\n            if param == \"count_task_weight\" and value == 0.5: continue\n            if param == \"gpu_tier\" and value == \"cpu\": continue\n            if param == \"use_english_only_tokens\" and value == False: continue\n            if param == \"analyze_thinking_mode\" and value == False: continue\n                \n            kwargs = {param: value}\n            config_path = create_experiment_config(f\"{base_name}_{param}_{value}\", **kwargs)\n            configs.append(config_path)\n    \n    return configs\n\n# Initialize W&B sweep with spelling and transfer learning metrics\ndef create_wandb_sweep():\n    sweep_config = {\n        \"method\": \"grid\",\n        \"metric\": {\n            \"name\": \"transfer_learning_score\",  # Combined metric for transfer learning effectiveness\n            \"goal\": \"maximize\"\n        },\n        \"parameters\": {\n            # LoRA parameters\n            \"lora_r\": {\"values\": [4, 8, 16, 32]},\n            \"lora_alpha\": {\"values\": [8, 16, 32, 64]},\n            # Training parameters\n            \"learning_rate\": {\"values\": [1e-4, 2e-4, 5e-4, 1e-3]},\n            \"batch_size\": {\"values\": [4, 8, 16, 32]},\n            \"grad_accum_steps\": {\"values\": [1, 2, 4, 8]},\n            \"max_steps\": {\"values\": [500, 1000, 2000, 5000]},\n            # Spelling-specific parameters\n            \"spelling_data_ratio\": {\"values\": [0.5, 0.7, 0.9]},\n            \"spelling_difficulty\": {\"values\": [\"easy\", \"medium\", \"hard\"]},\n            # Transfer learning parameters\n            \"position_task_weight\": {\"values\": [0.3, 0.5, 0.7]},\n            \"count_task_weight\": {\"values\": [0.3, 0.5, 0.7]},\n            # Lightning.AI parameters\n            \"gpu_tier\": {\"values\": [\"cpu\", \"t4\", \"a100\"]},\n            # Qwen3 tokenizer parameters\n            \"use_english_only_tokens\": {\"values\": [False, True]},\n            \"analyze_thinking_mode\": {\"values\": [False, True]},\n        }\n    }\n    \n    sweep_id = wandb.sweep(sweep_config, project=\"llm-spelling-transfer-learning\")\n    return sweep_id\n\n# Calculate transfer learning score from spelling and transfer metrics\ndef calculate_transfer_score(spelling_accuracy, position_accuracy, count_accuracy, position_weight=0.5, count_weight=0.5):\n    \"\"\"Calculate a combined score that measures transfer learning effectiveness.\"\"\"\n    transfer_score = position_weight * position_accuracy + count_weight * count_accuracy\n    # Correlation bonus: reward configurations where spelling improvement correlates with transfer task improvement\n    correlation_bonus = min(spelling_accuracy, (position_accuracy + count_accuracy) / 2) * 0.2\n    return transfer_score + correlation_bonus\n\n# Qwen3 tokenizer analysis functions\ndef analyze_qwen3_tokenizer(use_english_only=False, analyze_thinking_mode=False):\n    \"\"\"Analyze the Qwen3-4B tokenizer with focus on English tokens and thinking/non-thinking modes.\"\"\"\n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")\n    vocab = tokenizer.get_vocab()\n    \n    # Create results directory\n    os.makedirs(\"results/tuning/token_analysis\", exist_ok=True)\n    \n    # Basic tokenizer statistics\n    stats = {\n        \"vocab_size\": len(vocab),\n        \"special_tokens\": len([t for t in vocab.keys() if t.startswith(\"<\") and t.endswith(\">\")])  \n    }\n    \n    # Analyze multi-token words\n    english_words = [\"apple\", \"banana\", \"computer\", \"algorithm\", \"intelligence\", \"understanding\", \n                    \"knowledge\", \"learning\", \"development\", \"hyperparameter\"]\n    \n    word_tokens = {}\n    for word in english_words:\n        tokens = tokenizer.tokenize(word)\n        word_tokens[word] = {\n            \"tokens\": tokens,\n            \"token_count\": len(tokens)\n        }\n    \n    # Identify English tokens\n    english_tokens = {}\n    if use_english_only:\n        # Simple heuristic: tokens that only contain ASCII characters are likely English\n        for token, id in vocab.items():\n            if all(ord(c) < 128 for c in token) and not token.startswith(\"<\"):\n                english_tokens[token] = id\n        \n        # Save English token subset\n        os.makedirs(\"results/tuning/token_analysis/english_tokens\", exist_ok=True)\n        with open(\"results/tuning/token_analysis/english_tokens/english_vocab.json\", \"w\") as f:\n            json.dump(english_tokens, f, indent=2)\n            \n        stats[\"english_token_count\"] = len(english_tokens)\n        stats[\"english_token_percentage\"] = len(english_tokens) / len(vocab) * 100\n    \n    # Analyze thinking vs non-thinking mode token usage\n    if analyze_thinking_mode:\n        thinking_samples = [\n            \"Let me think about this step by step.\",\n            \"I need to analyze this carefully.\",\n            \"Let's break this down systematically.\"\n        ]\n        \n        non_thinking_samples = [\n            \"The answer is 42.\",\n            \"Yes, that's correct.\",\n            \"No, that's not right.\"\n        ]\n        \n        thinking_tokens = {}\n        non_thinking_tokens = {}\n        \n        for sample in thinking_samples:\n            tokens = tokenizer.tokenize(sample)\n            for token in tokens:\n                thinking_tokens[token] = thinking_tokens.get(token, 0) + 1\n                \n        for sample in non_thinking_samples:\n            tokens = tokenizer.tokenize(sample)\n            for token in tokens:\n                non_thinking_tokens[token] = non_thinking_tokens.get(token, 0) + 1\n        \n        stats[\"thinking_mode\"] = {\n            \"unique_tokens\": len(thinking_tokens),\n            \"token_distribution\": thinking_tokens\n        }\n        \n        stats[\"non_thinking_mode\"] = {\n            \"unique_tokens\": len(non_thinking_tokens),\n            \"token_distribution\": non_thinking_tokens\n        }\n    \n    # Save analysis results\n    with open(\"results/tuning/token_analysis/tokenizer_stats.json\", \"w\") as f:\n        json.dump(stats, f, indent=2)\n        \n    with open(\"results/tuning/token_analysis/multi_token_words.json\", \"w\") as f:\n        json.dump(word_tokens, f, indent=2)\n    \n    # Create visualizations\n    if use_english_only:\n        # Plot token distribution comparison\n        labels = ['Full Vocabulary', 'English-only Subset']\n        sizes = [len(vocab) - len(english_tokens), len(english_tokens)]\n        \n        plt.figure(figsize=(10, 6))\n        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n        plt.axis('equal')\n        plt.title('Qwen3-4B Token Distribution')\n        plt.savefig(\"results/tuning/token_analysis/token_distribution.png\")\n    \n    return stats\n\n# Lightning.AI Work class for hyperparameter tuning\nclass HyperparameterTuningWork(LightningWork):\n    def __init__(self, config_path, gpu_tier=\"cpu\"):\n        super().__init__(cloud_compute=gpu_tier)\n        self.config_path = config_path\n        self.results = None\n        \n    def run(self):\n        # Load configuration\n        with open(self.config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n            \n        # Set up PyTorch Lightning trainer\n        trainer = pl.Trainer(\n            max_steps=config[\"training_config\"][\"max_steps\"],\n            accelerator=\"auto\",\n            devices=1,\n            precision=16,\n            logger=True,\n        )\n        \n        # Run training and evaluation\n        # ... training code here ...\n        \n        # Calculate transfer learning score\n        spelling_accuracy = 0.85  # Example value\n        position_accuracy = 0.75  # Example value\n        count_accuracy = 0.70     # Example value\n        transfer_score = calculate_transfer_score(\n            spelling_accuracy, \n            position_accuracy, \n            count_accuracy,\n            position_weight=config[\"transfer_config\"][\"position_task_weight\"],\n            count_weight=config[\"transfer_config\"][\"count_task_weight\"]\n        )\n        \n        # Store results\n        self.results = {\n            \"spelling_accuracy\": spelling_accuracy,\n            \"position_accuracy\": position_accuracy,\n            \"count_accuracy\": count_accuracy,\n            \"transfer_score\": transfer_score,\n        }\n        \n        # Save results to shared storage\n        results_dir = os.path.join(\"results/tuning/data\", config[\"experiment_name\"])\n        os.makedirs(results_dir, exist_ok=True)\n        with open(os.path.join(results_dir, \"results.yaml\"), \"w\") as f:\n            yaml.dump(self.results, f)\n\n# Lightning.AI Flow class to manage hyperparameter tuning jobs\nclass HyperparameterTuningFlow(LightningFlow):\n    def __init__(self):\n        super().__init__()\n        self.tuning_jobs = {}\n        self.completed_jobs = {}\n        \n    def run(self):\n        # Check status of running jobs\n        for job_id, job in list(self.tuning_jobs.items()):\n            if job.status.ready:\n                self.completed_jobs[job_id] = job.results\n                del self.tuning_jobs[job_id]\n                \n        # Report progress\n        if self.tuning_jobs or self.completed_jobs:\n            print(f\"Running jobs: {len(self.tuning_jobs)}, Completed jobs: {len(self.completed_jobs)}\")\n            \n    def add_job(self, config_path, job_id=None):\n        if job_id is None:\n            job_id = f\"job_{len(self.tuning_jobs) + len(self.completed_jobs)}\"\n            \n        # Load configuration to get GPU tier\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n            gpu_tier = config.get(\"lightning_config\", {}).get(\"gpu_tier\", \"cpu\")\n            \n        # Create and add job\n        job = HyperparameterTuningWork(config_path=config_path, gpu_tier=gpu_tier)\n        self.tuning_jobs[job_id] = job\n        return job_id\n\n# Command-line interface for experiment execution\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run hyperparameter tuning experiments for spelling and transfer learning\")\n    parser.add_argument(\"--mode\", choices=[\"grid\", \"sweep\", \"single\", \"lightning\", \"token-analysis\"], default=\"single\",\n                        help=\"Experiment mode: grid search, W&B sweep, single experiment, Lightning.AI jobs, or token analysis\")\n    parser.add_argument(\"--name\", type=str, default=\"spelling_transfer_exp\",\n                        help=\"Base name for the experiment\")\n    parser.add_argument(\"--config\", type=str, help=\"Path to a specific config file (for single mode)\")\n    parser.add_argument(\"--focus\", choices=[\"spelling\", \"transfer\", \"balanced\"], default=\"balanced\",\n                        help=\"Focus of the experiment: spelling performance, transfer learning, or balanced\")\n    parser.add_argument(\"--gpu-tier\", choices=[\"cpu\", \"t4\", \"a100\"], default=\"cpu\",\n                        help=\"GPU tier to use for Lightning.AI jobs\")\n    parser.add_argument(\"--english-only\", action=\"store_true\", help=\"Use only English tokens from Qwen3 tokenizer\")\n    parser.add_argument(\"--analyze-thinking\", action=\"store_true\", help=\"Analyze thinking vs non-thinking mode token usage\")\n    \n    args = parser.parse_args()\n    \n    # Adjust weights based on experiment focus\n    position_weight = 0.5\n    count_weight = 0.5\n    if args.focus == \"spelling\":\n        position_weight = 0.3\n        count_weight = 0.3\n    elif args.focus == \"transfer\":\n        position_weight = 0.6\n        count_weight = 0.6\n    \n    if args.mode == \"token-analysis\":\n        print(\"Running Qwen3-4B tokenizer analysis...\")\n        stats = analyze_qwen3_tokenizer(use_english_only=args.english_only, analyze_thinking_mode=args.analyze_thinking)\n        print(f\"Analysis complete. Results saved to results/tuning/token_analysis/\")\n        print(f\"Vocabulary size: {stats['vocab_size']}\")\n        if 'english_token_count' in stats:\n            print(f\"English tokens: {stats['english_token_count']} ({stats['english_token_percentage']:.2f}%)\")\n    elif args.mode == \"grid\":\n        configs = create_grid_search_configs(args.name)\n        print(f\"Created {len(configs)} configurations for grid search with {args.focus} focus\")\n    elif args.mode == \"sweep\":\n        sweep_id = create_wandb_sweep()\n        print(f\"Created W&B sweep with ID: {sweep_id} and {args.focus} focus\")\n    elif args.mode == \"lightning\":\n        # Set up Lightning.AI app for hyperparameter tuning\n        flow = HyperparameterTuningFlow()\n        app = LightningApp(flow)\n        \n        # Create configurations\n        configs = create_grid_search_configs(args.name)\n        \n        # Add jobs to the flow\n        for config in configs:\n            flow.add_job(config)\n            \n        # Run the app\n        app.run()\n    elif args.mode == \"single\":\n        if args.config:\n            print(f\"Using provided config: {args.config} with {args.focus} focus\")\n        else:\n            config_path = create_experiment_config(\n                args.name, \n                position_task_weight=position_weight,\n                count_task_weight=count_weight,\n                gpu_tier=args.gpu_tier,\n                use_english_only_tokens=args.english_only,\n                analyze_thinking_mode=args.analyze_thinking,\n            )\n            print(f\"Created single experiment config: {config_path} with {args.focus} focus\")\n\nif __name__ == \"__main__\":\n    main()\n```",
      "testStrategy": "1. Verify configuration system creates valid YAML files with spelling and transfer learning parameters in the correct directories (`configs/hyperparameters/`)\n2. Confirm W&B experiment tracking properly separates spelling metrics from transfer learning metrics\n3. Test that the hyperparameter grid generates the expected number of configurations including spelling-specific parameters\n4. Verify W&B sweep configuration includes both spelling and transfer learning metrics\n5. Test the command-line interfaces for all Python scripts, especially the new `--focus`, `--gpu-tier`, `--english-only`, and `--analyze-thinking` parameters\n6. Ensure metrics for comparing experiments clearly separate direct spelling performance from transfer learning effectiveness\n7. Verify that results are properly saved to `results/tuning/` directories including the new transfer learning analysis\n8. Test the experiment executor to ensure it correctly tracks both spelling performance and transfer learning metrics\n9. Verify HTML report generation produces valid reports that show correlations between spelling improvement and transfer learning\n10. Test visualization tools to ensure they generate figures showing relationships between spelling training and transfer learning performance\n11. Verify the transfer learning analysis module correctly calculates combined scores and identifies optimal training patterns\n12. Test Lightning.AI Studio creation and configuration with proper environment isolation\n13. Verify GPU switching functionality works correctly (CPU → T4 → A100) based on experiment phase\n14. Test Lightning.AI job system integration for managing multiple training runs\n15. Verify shared filesystem access for datasets and checkpoint saving\n16. Test cost optimization features including GPU sleep settings and resource usage efficiency\n17. Verify PyTorch Lightning integration works correctly with the Lightning.AI platform\n18. Test Qwen3-4B tokenizer analysis functionality with focus on English-only token subset\n19. Verify multi-token word analysis correctly identifies tokenization patterns in Qwen3-4B\n20. Test the thinking/non-thinking mode analysis to ensure it captures differences in token usage patterns\n21. Verify the English token filtering process produces a valid subset of the full vocabulary\n22. Test token distribution visualization to ensure it correctly shows the proportion of English tokens\n23. Verify that experiments can be run with both full vocabulary and English-only token subset",
      "subtasks": [
        {
          "id": 1,
          "title": "Configuration System Design and Implementation",
          "description": "Design and implement a flexible configuration system for hyperparameter management",
          "dependencies": [],
          "details": "Create a configuration framework that supports defining, validating, and loading hyperparameter configurations. Implement serialization/deserialization of configurations to JSON/YAML formats. Design a hierarchical configuration structure that allows for inheritance and overrides. Include validation mechanisms to ensure hyperparameter values fall within acceptable ranges. Support both discrete values (HPARAM_CANDIDATES) and continuous ranges (HPARAM_RANGE) for different hyperparameter types.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Experiment Tracking Setup with W&B Integration",
          "description": "Implement experiment tracking infrastructure with Weights & Biases integration focused on spelling and transfer learning metrics",
          "dependencies": [
            1
          ],
          "details": "Set up W&B project structure for hyperparameter experiments with separate tracking for spelling and transfer learning metrics. Implement logging mechanisms for spelling training metrics, transfer learning metrics, and model artifacts in `src/tuning/wandb_integration.py`. Create utilities for experiment initialization, updating, and finalization. Design a consistent naming convention for experiments. Implement automatic synchronization between local experiment state and W&B. Add support for experiment grouping and comparison within the W&B interface. Create visualizations that show correlations between spelling improvement and position/count task performance.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Hyperparameter Grid Definition and Validation",
          "description": "Create a system for defining and validating hyperparameter search spaces for spelling and transfer learning optimization",
          "dependencies": [
            1
          ],
          "details": "Implement a framework for defining hyperparameter search spaces including random, grid, and Bayesian optimization strategies in `src/tuning/grid.py`. Create validation mechanisms to ensure search spaces are properly defined. Support both continuous ranges and discrete value sets for different hyperparameter types. Implement utilities for sampling from defined search spaces. Add functionality to estimate the total number of trials based on the search space definition. Create interfaces for custom search space definitions. Store search space definitions in `configs/hyperparameters/search_spaces/`. Include spelling-specific parameters (data ratio, augmentation, difficulty) and transfer learning parameters (task weights, evaluation frequency) in the search space.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Experiment Execution Framework",
          "description": "Build a framework for executing hyperparameter tuning experiments optimized for transfer learning effectiveness",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Implement a job scheduler in `src/tuning/executor.py` for running multiple trials with different hyperparameter configurations. Create mechanisms for early stopping of underperforming trials based on both spelling and transfer metrics. Design parallel execution capabilities to utilize available computational resources efficiently. Implement checkpointing and resumption of interrupted experiments. Add support for distributed training across multiple machines. Create a monitoring system for active experiments with real-time status updates for both spelling and transfer learning performance. Save experiment results to `results/tuning/data/` with best configurations in `results/tuning/configs/`. Implement command-line interfaces for flexible experiment execution with options to focus on spelling performance, transfer learning, or a balanced approach.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Results Visualization and Reporting System",
          "description": "Develop tools for visualizing and reporting hyperparameter tuning results with focus on transfer learning effectiveness",
          "dependencies": [
            2,
            4
          ],
          "details": "Create visualization tools in `src/tuning/visualization.py` for comparing metrics across different hyperparameter configurations, showing relationships between spelling performance and transfer learning. Implement automated analysis to identify the most influential hyperparameters for both spelling and transfer tasks. Design HTML report generation in `src/tuning/report.py` for exploring the hyperparameter search space and transfer learning patterns. Add functionality to export comparison reports to `results/tuning/reports/`. Implement statistical analysis tools to evaluate the significance of performance differences and correlations between spelling and transfer metrics. Create recommendation system for suggesting optimal hyperparameter configurations for future experiments based on transfer learning goals. Generate performance plots in `results/tuning/figures/` showing relationships between spelling training and transfer learning outcomes.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Documentation and User Guides",
          "description": "Create comprehensive documentation for the hyperparameter tuning system with focus on transfer learning",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Develop detailed documentation covering the hyperparameter tuning system in `docs/hyperparameter_tuning.md`. Create a configuration guide explaining the structure and usage of the configuration system in `docs/config_system.md`. Write a results analysis guide detailing how to interpret and utilize tuning results in `docs/tuning_results.md`. Create a transfer learning guide explaining how to optimize spelling training for better transfer to position/count tasks in `docs/transfer_learning.md`. Include examples, best practices, and troubleshooting information in all documentation. Document command-line interfaces and provide usage examples for all scripts, including the new focus options.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Python Package Structure and Testing",
          "description": "Implement proper Python packaging and testing for the tuning infrastructure",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Organize the tuning code as a proper Python package with appropriate imports and dependencies. Create unit tests for each component of the tuning infrastructure. Implement integration tests to verify the end-to-end workflow. Set up continuous integration for automated testing. Create a requirements.txt or setup.py file to manage dependencies. Ensure compatibility with the rest of the codebase. Add type hints and docstrings for better code documentation.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Transfer Learning Analysis Module",
          "description": "Develop a module for analyzing transfer learning effectiveness from spelling to position/count tasks",
          "dependencies": [
            2,
            4,
            5
          ],
          "details": "Create a dedicated module `src/tuning/transfer_analysis.py` for analyzing the relationship between spelling training and transfer learning performance. Implement metrics that quantify transfer learning effectiveness across different hyperparameter configurations. Design visualization tools specifically for transfer learning analysis. Create correlation analysis between spelling performance improvements and position/count task improvements. Implement functions to identify which training patterns lead to better transfer learning. Add support for calculating combined scores that balance direct spelling performance with transfer learning effectiveness. Save transfer learning analysis results to `results/tuning/transfer/`.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Lightning.AI Studio Integration",
          "description": "Set up a dedicated Lightning.AI Studio for hyperparameter tuning experiments",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Create a dedicated Lightning.AI Studio for hyperparameter tuning following the \"one Studio, one task\" principle. Configure proper environment isolation with all required dependencies. Set up shared filesystem access for datasets and model checkpoints. Implement GPU switching functionality (CPU for development → T4 for testing → A100 for full training). Create configuration files for Lightning.AI Studio in `configs/hyperparameters/lightning/`. Implement cost optimization through GPU sleep settings and efficient resource usage. Document the Lightning.AI Studio setup process in `docs/lightning_studio_setup.md`. Create scripts for launching and monitoring experiments in the Lightning.AI environment.",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "PyTorch Lightning Integration",
          "description": "Implement training using PyTorch Lightning for better integration with Lightning.AI platform",
          "dependencies": [
            1,
            4,
            9
          ],
          "details": "Refactor training code to use PyTorch Lightning for better integration with the Lightning.AI platform. Implement Lightning Modules for spelling and transfer learning models. Create custom callbacks for tracking transfer learning metrics. Set up Lightning DataModules for efficient data loading. Implement checkpointing and model saving compatible with Lightning.AI's shared filesystem. Create Lightning CLI interfaces for experiment configuration. Implement distributed training support using Lightning's built-in capabilities. Add support for mixed precision training to improve performance on GPUs.",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "Lightning.AI Job System Integration",
          "description": "Leverage Lightning.AI's job system for managing multiple training runs",
          "dependencies": [
            9,
            10
          ],
          "details": "Implement integration with Lightning.AI's job system in `src/tuning/lightning_jobs.py`. Create job templates for different experiment types (grid search, single run, etc.). Implement job scheduling based on resource availability. Add support for job dependencies and sequential execution. Create monitoring tools for tracking job status and performance. Implement automatic resource scaling based on experiment requirements. Add support for job prioritization based on expected impact. Create utilities for job result collection and aggregation. Implement cost tracking and optimization for Lightning.AI resources.",
          "status": "pending"
        },
        {
          "id": 12,
          "title": "Qwen3-4B Tokenizer Analysis Implementation",
          "description": "Implement analysis tools for Qwen3-4B tokenizer with focus on English-only token subset",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated module `src/tuning/token_analysis.py` for analyzing the Qwen3-4B tokenizer. Implement functions to identify and extract English-only tokens from the full vocabulary. Create analysis tools for multi-token word behavior in Qwen3-4B. Implement visualization of token distribution between full vocabulary and English-only subset. Add support for analyzing thinking vs. non-thinking modes and their impact on token usage patterns. Create utilities for filtering and using the English-only token subset in experiments. Save analysis results and English token subset to `results/tuning/token_analysis/`. Document the tokenizer analysis process and findings in `docs/qwen3_token_analysis.md`.",
          "status": "pending"
        },
        {
          "id": 13,
          "title": "English Token Subset Integration",
          "description": "Integrate English-only token subset option into the hyperparameter tuning framework",
          "dependencies": [
            1,
            12
          ],
          "details": "Extend the configuration system to support using English-only token subset from Qwen3-4B. Implement mechanisms to filter and use only English tokens during training and evaluation. Create comparison experiments between full vocabulary and English-only subset. Add metrics to measure the impact of token subset on spelling performance and transfer learning. Implement visualization tools to compare results between full vocabulary and English-only experiments. Document the English token subset approach and its effects in the hyperparameter tuning documentation.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 7,
      "title": "Unsloth Integration for Optimized Fine-tuning",
      "description": "Set up Unsloth for optimized LoRA fine-tuning of the Qwen3-4B model with memory efficiency optimizations in a cloud GPU environment (Google Colab or Lightning.ai), using Python scripts instead of notebooks for better maintainability and version control. Configure the system to handle separate training (spelling variations) and evaluation (position/count) datasets, with special attention to Qwen3's tokenizer and English-only token subset.",
      "status": "pending",
      "dependencies": [
        6
      ],
      "priority": "high",
      "details": "**NOTE: This task requires a cloud GPU environment. Do not attempt on local Mac.**\n\n1. Install and configure Unsloth for optimized fine-tuning in Google Colab or Lightning.ai\n2. Set up Unsloth-specific environment requirements in the cloud environment\n3. Configure memory-efficient QLoRA training\n4. Set up Flash Attention 2 if available on cloud GPU hardware\n5. Implement proper tokenization for instruction fine-tuning with Qwen3-4B's tokenizer\n6. Configure GPU memory optimizations\n7. Set up separate handling for spelling training data and position/count evaluation data\n8. Implement efficient evaluation of position/count tasks during training\n9. Leverage Lightning.AI Studios for data preparation and processing\n10. Filter and validate data based on Qwen3's English-only token subset\n11. Handle multi-token words appropriately in Qwen3's tokenization patterns\n12. Prepare data for both thinking and non-thinking modes\n\nFile Structure:\n- Environment setup: `src/unsloth/environment.py`\n- Model loading and configuration: `src/unsloth/model.py`\n- Dataset preparation: `src/unsloth/dataset.py`\n- Lightning DataModules: `src/unsloth/datamodules.py`\n- Training setup: `src/unsloth/trainer.py`\n- Training monitoring: `src/unsloth/monitor.py`\n- HTML report generation: `src/unsloth/report.py`\n- Evaluation utilities: `src/unsloth/evaluation.py`\n- Data validation: `src/unsloth/data_validation.py`\n- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`\n- Token filtering utilities: `src/unsloth/token_filter.py`\n\nOutput Structure:\n- `results/unsloth/figures/` (All PNG/PDF visualizations)\n- `results/unsloth/reports/` (HTML reports)\n- `results/unsloth/data/` (Training metrics)\n- `results/unsloth/configs/` (Model configurations)\n- `results/unsloth/evaluation/` (Position/count evaluation results)\n- `results/unsloth/data_versions/` (Data version tracking)\n- `results/unsloth/token_analysis/` (Qwen3 token usage analysis)\n\nImplementation:\n```python\n# Install Unsloth in Google Colab or Lightning.ai environment\n!pip install unsloth\n\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\n\n# Optimize GPU memory\ndef optimize_gpu_memory():\n    if torch.cuda.is_available():\n        # Set GPU memory allocation strategy\n        torch.cuda.set_per_process_memory_fraction(0.9)  # Reserve 10% for system\n        # Enable memory caching for faster allocation\n        torch.backends.cudnn.benchmark = True\n        # Use TF32 precision on Ampere GPUs or later for faster computation\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n# Load model with Unsloth optimizations\ndef load_unsloth_model(config):\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"Qwen/Qwen3-4B\",  # Updated to Qwen3-4B\n        max_seq_length=512,\n        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n        load_in_4bit=True,  # Use 4-bit quantization to reduce memory usage\n        token=None,  # Add your HF token for private models\n    )\n\n    # Add LoRA adapters with Unsloth-specific optimizations\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=config[\"lora_config\"][\"r\"],\n        target_modules=config[\"lora_config\"][\"target_modules\"],\n        lora_alpha=config[\"lora_config\"][\"alpha\"],\n        lora_dropout=config[\"lora_config\"][\"dropout\"],\n        bias=\"none\",  # Unsloth-specific - sets which modules receive adapters\n        use_gradient_checkpointing=True,  # Unsloth-specific - saves memory\n        random_state=42,  # For reproducibility\n        use_rslora=False,  # Set to True for rank-stabilized LoRA (optional)\n        loftq_config=None,  # Optional LoftQ configuration\n    )\n    \n    return model, tokenizer\n\n# Get English-only token subset for Qwen3\ndef get_english_token_subset(tokenizer):\n    # This function identifies the English-only token subset in Qwen3's vocabulary\n    # Implementation depends on Qwen3's specific tokenization patterns\n    english_tokens = []\n    for token_id in range(len(tokenizer)):\n        token = tokenizer.decode([token_id])\n        # Apply filtering logic to identify English-only tokens\n        # This is a simplified example - actual implementation would be more complex\n        if all(c.isascii() for c in token):\n            english_tokens.append(token_id)\n    \n    return set(english_tokens)\n\n# Analyze multi-token word handling in Qwen3\ndef analyze_multi_token_words(tokenizer, common_words):\n    results = {}\n    for word in common_words:\n        tokens = tokenizer.encode(word, add_special_tokens=False)\n        results[word] = {\n            \"token_count\": len(tokens),\n            \"tokens\": [tokenizer.decode([t]) for t in tokens]\n        }\n    return results\n\n# Prepare spelling dataset for Unsloth with Qwen3 tokenizer\ndef prepare_spelling_dataset(dataset, tokenizer, config):\n    # Get English token subset if filtering is enabled\n    english_tokens = get_english_token_subset(tokenizer) if config[\"use_english_only\"] else None\n    \n    def formatting_prompts_func(examples):\n        questions = examples[\"question\"]\n        answers = examples[\"answer\"]\n        \n        # Format based on thinking mode configuration\n        if config[\"use_thinking_mode\"]:\n            # Special Qwen3 prompt format with thinking mode\n            prompts = [\n                f\"<human>: {question}\\n<assistant>: <thinking>\\nLet me analyze this spelling question carefully.\\n</thinking>\\n\"\n                for question in questions\n            ]\n        else:\n            # Standard prompt format without thinking mode\n            prompts = [\n                f\"<human>: {question}\\n<assistant>: \"\n                for question in questions\n            ]\n\n        # Format responses with EOS token\n        formatted_responses = [\n            f\"{answer}{tokenizer.eos_token}\"\n            for answer in answers\n        ]\n        \n        # Validate tokens if English-only filtering is enabled\n        if english_tokens:\n            valid_examples = []\n            valid_responses = []\n            \n            for prompt, response in zip(prompts, formatted_responses):\n                prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n                response_tokens = tokenizer.encode(response, add_special_tokens=False)\n                \n                # Check if all tokens are in the English subset\n                if all(t in english_tokens for t in prompt_tokens + response_tokens):\n                    valid_examples.append(prompt)\n                    valid_responses.append(response)\n                    \n            prompts = valid_examples\n            formatted_responses = valid_responses\n\n        return {\n            \"prompt\": prompts,\n            \"completion\": formatted_responses,\n        }\n    \n    formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n    \n    # Analyze multi-token word handling\n    if config[\"analyze_tokenization\"]:\n        common_words = [example[\"answer\"] for example in dataset[:100]]\n        tokenization_analysis = analyze_multi_token_words(tokenizer, common_words)\n        \n        # Save analysis to file\n        import json\n        with open(\"results/unsloth/token_analysis/multi_token_analysis.json\", \"w\") as f:\n            json.dump(tokenization_analysis, f, indent=2)\n    \n    return formatted_dataset\n\n# Prepare position/count dataset for evaluation with Qwen3 tokenizer\ndef prepare_position_count_dataset(dataset, tokenizer, config):\n    # Get English token subset if filtering is enabled\n    english_tokens = get_english_token_subset(tokenizer) if config[\"use_english_only\"] else None\n    \n    def formatting_prompts_func(examples):\n        questions = examples[\"question\"]\n        answers = examples[\"answer\"]\n\n        # Format based on thinking mode configuration\n        if config[\"use_thinking_mode\"]:\n            # Special Qwen3 prompt format with thinking mode for position/count tasks\n            prompts = [\n                f\"<human>: {question}\\n<assistant>: <thinking>\\nI need to carefully analyze the positions and counts in this question.\\n</thinking>\\n\"\n                for question in questions\n            ]\n        else:\n            # Standard prompt format without thinking mode\n            prompts = [\n                f\"<human>: {question}\\n<assistant>: \"\n                for question in questions\n            ]\n\n        # Format responses with EOS token\n        formatted_responses = [\n            f\"{answer}{tokenizer.eos_token}\"\n            for answer in answers\n        ]\n        \n        # Validate tokens if English-only filtering is enabled\n        if english_tokens:\n            valid_examples = []\n            valid_responses = []\n            valid_task_types = []\n            \n            for prompt, response, task_type in zip(prompts, formatted_responses, \n                                                 examples.get(\"task_type\", [\"position_count\"] * len(questions))):\n                prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n                response_tokens = tokenizer.encode(response, add_special_tokens=False)\n                \n                # Check if all tokens are in the English subset\n                if all(t in english_tokens for t in prompt_tokens + response_tokens):\n                    valid_examples.append(prompt)\n                    valid_responses.append(response)\n                    valid_task_types.append(task_type)\n                    \n            prompts = valid_examples\n            formatted_responses = valid_responses\n            task_types = valid_task_types\n        else:\n            task_types = examples.get(\"task_type\", [\"position_count\"] * len(questions))\n\n        return {\n            \"prompt\": prompts,\n            \"completion\": formatted_responses,\n            \"task_type\": task_types\n        }\n    \n    formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n    return formatted_dataset\n\n# Set up Unsloth trainer with dual dataset support\ndef create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, eval_dataset, config):\n    trainer = FastLanguageModel.get_trainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,  # Spelling validation dataset\n        args=FastLanguageModel.get_train_args(\n            output_dir=f\"./spelling-lora-{config['experiment_name']}\",\n            per_device_train_batch_size=config[\"training_config\"][\"per_device_train_batch_size\"],\n            gradient_accumulation_steps=config[\"training_config\"][\"gradient_accumulation_steps\"],\n            warmup_steps=config[\"training_config\"][\"warmup_steps\"],\n            max_steps=config[\"training_config\"][\"max_steps\"],\n            learning_rate=config[\"training_config\"][\"learning_rate\"],\n            fp16=not torch.cuda.is_bf16_supported(),\n            bf16=torch.cuda.is_bf16_supported(),\n            logging_steps=10,\n            evaluation_strategy=\"steps\",\n            eval_steps=100,\n            save_strategy=\"steps\",\n            save_steps=200,\n            optim=\"adamw_torch\",  # Unsloth recommends adamw_torch over paged_adamw_8bit\n            max_grad_norm=0.3,    # Gradient clipping - Unsloth recommended value\n            report_to=\"wandb\",\n        ),\n        data_collator=FastLanguageModel.get_data_collator(tokenizer=tokenizer),\n    )\n    \n    # Add position/count evaluation dataset as a custom attribute\n    trainer.position_count_dataset = eval_dataset\n    \n    # Add custom evaluation callback for position/count tasks\n    class PositionCountEvaluationCallback(TrainerCallback):\n        def on_evaluate(self, args, state, control, **kwargs):\n            # Run evaluation on position/count dataset\n            metrics = evaluate_position_count(trainer.model, trainer.tokenizer, \n                                             trainer.position_count_dataset, \n                                             config[\"evaluation_config\"])\n            # Log metrics to wandb\n            wandb.log({f\"position_count_{k}\": v for k, v in metrics.items()}, \n                      step=state.global_step)\n    \n    trainer.add_callback(PositionCountEvaluationCallback())\n    \n    return trainer\n\n# Evaluate model on position/count tasks\ndef evaluate_position_count(model, tokenizer, dataset, config):\n    # Set up metrics\n    metrics = {\n        \"position_accuracy\": 0.0,\n        \"count_accuracy\": 0.0,\n        \"overall_accuracy\": 0.0\n    }\n    \n    # Implement evaluation logic for position/count tasks\n    # This would generate predictions and compare against ground truth\n    \n    return metrics\n\n# Main training function\ndef train_with_unsloth(config_path):\n    # Load config\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    \n    # Initialize W&B\n    wandb.init(project=\"llm-spelling-finetuning\", name=config[\"experiment_name\"], config=config)\n    \n    # Optimize GPU memory\n    optimize_gpu_memory()\n    \n    # Load model and tokenizer\n    model, tokenizer = load_unsloth_model(config)\n    \n    # Load datasets\n    spelling_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\")\n    position_count_dataset = load_dataset(\"YOUR-USERNAME/llm-position-count-dataset\")\n    \n    # Prepare datasets for Unsloth with Qwen3 tokenizer\n    train_dataset = prepare_spelling_dataset(spelling_dataset[\"train\"], tokenizer, config)\n    val_dataset = prepare_spelling_dataset(spelling_dataset[\"validation\"], tokenizer, config)\n    eval_dataset = prepare_position_count_dataset(position_count_dataset[\"validation\"], tokenizer, config)\n    \n    # Create trainer\n    trainer = create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, eval_dataset, config)\n    \n    # Train model\n    trainer.train()\n    \n    # Save model\n    trainer.save_model()\n    \n    # Final evaluation on both datasets\n    spelling_eval_results = trainer.evaluate()\n    position_count_eval_results = evaluate_position_count(model, tokenizer, eval_dataset, config)\n    \n    # Log final results\n    wandb.log({\n        **spelling_eval_results,\n        **{f\"final_position_count_{k}\": v for k, v in position_count_eval_results.items()}\n    })\n    \n    # Generate comprehensive report\n    generate_evaluation_report(spelling_eval_results, position_count_eval_results, config)\n    \n    # Close W&B\n    wandb.finish()\n    \n    return {\n        \"spelling\": spelling_eval_results,\n        \"position_count\": position_count_eval_results\n    }\n\n# Generate comprehensive evaluation report\ndef generate_evaluation_report(spelling_results, position_count_results, config):\n    # Create HTML report with visualizations for both tasks\n    # Save to results/unsloth/reports/\n    pass\n```",
      "testStrategy": "1. Verify Unsloth installs and imports correctly in Google Colab or Lightning.ai\n2. Confirm memory usage is optimized compared to standard fine-tuning\n3. Test that 4-bit quantization is working correctly on cloud GPU\n4. Measure training speed improvement over baseline implementation\n5. Verify all Unsloth-specific optimizations are configured\n6. Test with a small dataset to ensure the training loop works in cloud environment\n7. Monitor GPU memory usage during training\n8. Verify that the implementation does not contain any local-only dependencies\n9. Test command-line interfaces for all scripts\n10. Verify HTML report generation functionality\n11. Test the integration between all Python modules\n12. Validate output directory structure and file generation\n13. Ensure proper error handling and logging in scripts\n14. Test loading and processing of both spelling and position/count datasets\n15. Verify that evaluation metrics for both tasks are correctly calculated and logged\n16. Test the transfer performance from spelling training to position/count evaluation\n17. Validate that memory usage remains optimized when handling both datasets\n18. Test the custom evaluation callback for position/count tasks\n19. Verify Lightning.AI Studio setup and configuration for data preparation\n20. Test data validation and quality check mechanisms\n21. Validate data versioning and tracking functionality\n22. Test Lightning DataModules integration with Unsloth training pipeline\n23. Verify efficient data streaming and caching mechanisms in Lightning.AI environment\n24. Test Qwen3-4B tokenizer handling for multi-token words\n25. Verify English-only token subset filtering functionality\n26. Test data preparation in both thinking and non-thinking modes\n27. Validate token analysis reporting for Qwen3 tokenization patterns\n28. Test compatibility of prepared data with Qwen3's tokenization requirements\n29. Verify that filtered datasets maintain sufficient size for effective training\n30. Test the impact of English-only token filtering on model performance",
      "subtasks": [
        {
          "id": 1,
          "title": "Environment Setup with Optimizations",
          "description": "Configure the cloud GPU environment with Unsloth and necessary dependencies for optimized LLM fine-tuning",
          "dependencies": [],
          "details": "Install Unsloth library and compatible dependencies (bitsandbytes, transformers, PEFT). Configure GPU memory settings for optimal performance. Set up quantization libraries. Verify hardware compatibility (CUDA, ROCm, or Metal backend). Test environment with basic model loading to ensure all components work together.\n<info added on 2025-05-07T14:48:05.432Z>\nInstall Unsloth library and compatible dependencies (bitsandbytes, transformers, PEFT). Configure GPU memory settings for optimal performance. Set up quantization libraries. Verify hardware compatibility (CUDA, ROCm, or Metal backend). Test environment with basic model loading to ensure all components work together.\n\nThis task can be worked on independently and in parallel with others. The environment setup has no dependencies and is parallelizable (parallelizable: true), allowing team members to begin this work immediately while other tasks are being planned or executed.\n</info added on 2025-05-07T14:48:05.432Z>\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Model Loading with Unsloth-specific Configurations",
          "description": "Implement efficient model loading using Unsloth's FastLanguageModel with proper quantization and LoRA setup in cloud GPU environment",
          "dependencies": [
            1
          ],
          "details": "Use FastLanguageModel.from_pretrained() to load base models with quantization in Google Colab or Lightning.ai. Configure LoRA adapters with get_peft_model() using appropriate rank and target modules. Implement proper quantization settings (4-bit, 8-bit) based on available cloud GPU VRAM. Set up gradient checkpointing with 'unsloth' option. Validate model loading with memory profiling.\n\nFile location:\n- Model loading and configuration: `src/unsloth/model.py`\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Dataset Preparation for Unsloth",
          "description": "Prepare and optimize training datasets for efficient processing with Unsloth using Lightning.AI Studios, with special handling for Qwen3-4B's tokenizer and English-only token subset",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated Lightning.AI Studio for data preparation following the \"one Studio, one task\" principle. Format dataset according to Unsloth and Qwen3-4B requirements using Lightning.AI's shared filesystem for efficient data storage and access. Implement Lightning DataModules for better integration with the training pipeline. Set up automated data validation and quality checks to ensure data integrity. Configure proper environment isolation and dependency management in the Studio. Use CPU-optimized instances for data processing tasks to optimize costs. Implement efficient data streaming and caching mechanisms. Set up automated data versioning and tracking for reproducibility.\n\nSpecific Qwen3-4B requirements:\n1. Implement functions to identify and filter for English-only token subset\n2. Create analysis tools for multi-token word handling in Qwen3's tokenization patterns\n3. Set up data preparation for both thinking and non-thinking modes\n4. Implement validation to ensure data compatibility with Qwen3's tokenization requirements\n5. Create reporting tools to analyze token usage patterns in the dataset\n\nFile locations:\n- Dataset preparation: `src/unsloth/dataset.py`\n- Lightning DataModules: `src/unsloth/datamodules.py`\n- Data validation: `src/unsloth/data_validation.py`\n- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`\n- Token filtering utilities: `src/unsloth/token_filter.py`\n\nOutput locations:\n- `results/unsloth/data_versions/` (Data version tracking)\n- `results/unsloth/token_analysis/` (Qwen3 token usage analysis)\n\n**NOTE: This task requires a Lightning.AI Studio environment. Documentation should include detailed setup instructions.**",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Trainer Setup with Memory Optimizations",
          "description": "Configure SFTTrainer with Unsloth-optimized parameters for efficient fine-tuning in cloud GPU environment",
          "dependencies": [
            2,
            3
          ],
          "details": "Set up SFTTrainer with optimized batch size and gradient accumulation in Google Colab or Lightning.ai. Configure learning rate and scheduler based on training duration. Implement proper precision settings (bf16/fp16) based on cloud GPU hardware support. Set up memory-efficient optimizers (adamw_8bit). Configure logging and checkpointing. Validate trainer setup with memory usage monitoring during initial training steps.\n\nFile location:\n- Training setup: `src/unsloth/trainer.py`\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Monitoring and Reporting System",
          "description": "Create comprehensive monitoring and reporting system for Unsloth training",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Implement training monitoring system with real-time metrics tracking. Create HTML report generation functionality to summarize training results. Develop visualization utilities for training metrics. Set up proper logging and error handling. Implement command-line interfaces for all scripts.\n\nFile locations:\n- Training monitoring: `src/unsloth/monitor.py`\n- HTML report generation: `src/unsloth/report.py`\n\nOutput locations:\n- `results/unsloth/figures/` (All PNG/PDF visualizations)\n- `results/unsloth/reports/` (HTML reports)\n- `results/unsloth/data/` (Training metrics)\n- `results/unsloth/configs/` (Model configurations)\n\n**NOTE: While development can be done locally, testing should be performed in a cloud GPU environment.**",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Command-line Interface and Integration",
          "description": "Develop command-line interfaces for all Unsloth scripts and ensure proper integration",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Create command-line interfaces for all Unsloth scripts to enable flexible usage. Implement proper argument parsing with sensible defaults. Ensure proper integration between all modules. Set up configuration file handling. Implement proper error handling and user feedback. Create comprehensive documentation for CLI usage.\n\nFile locations:\n- All Python scripts in `src/unsloth/`\n- Main CLI entry point: `src/unsloth/__main__.py`\n\n**NOTE: While development can be done locally, testing should be performed in a cloud GPU environment.**",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Dual Dataset Handling Implementation",
          "description": "Implement efficient handling of both spelling training data and position/count evaluation data with Qwen3-4B tokenizer support",
          "dependencies": [
            3
          ],
          "details": "Create separate data processing pipelines for spelling and position/count datasets. Implement efficient data loading and caching mechanisms for both datasets. Configure memory-efficient data handling during training and evaluation phases. Implement dataset-specific tokenization and formatting for Qwen3-4B. Validate dual dataset handling with performance metrics.\n\nQwen3-4B specific requirements:\n1. Implement English-only token subset filtering for both datasets\n2. Handle multi-token words appropriately in both datasets\n3. Support both thinking and non-thinking modes in data preparation\n4. Create analysis tools to validate token usage patterns\n5. Implement efficient caching of tokenized data to improve performance\n\nFile location:\n- Dataset preparation: `src/unsloth/dataset.py`\n- Dual dataset handler: `src/unsloth/dual_dataset.py`\n- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Position/Count Task Evaluation System",
          "description": "Develop evaluation system for position/count tasks during spelling variation training",
          "dependencies": [
            4,
            7
          ],
          "details": "Implement custom evaluation callback for position/count tasks. Create metrics calculation for position and count accuracy. Develop efficient evaluation pipeline that runs during training. Set up proper logging of transfer metrics. Implement visualization utilities for transfer performance. Create comprehensive reporting for both spelling and position/count performance.\n\nFile locations:\n- Evaluation utilities: `src/unsloth/evaluation.py`\n- Training monitoring: `src/unsloth/monitor.py`\n\nOutput locations:\n- `results/unsloth/evaluation/` (Position/count evaluation results)\n- `results/unsloth/figures/` (Transfer performance visualizations)\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Lightning.AI Studio Documentation",
          "description": "Create comprehensive documentation for Lightning.AI Studio setup and configuration",
          "dependencies": [
            3
          ],
          "details": "Document the complete setup process for Lightning.AI Studios for data preparation. Include detailed instructions for environment configuration, dependency management, and resource allocation. Document the shared filesystem usage and data versioning approach. Create step-by-step guides for setting up CPU-optimized instances for data processing. Document the integration between Lightning DataModules and Unsloth training pipeline. Include troubleshooting guides and best practices for efficient data processing in Lightning.AI environment.\n\nFile location:\n- `docs/lightning_studio_setup.md`\n- `docs/data_processing_guide.md`\n\n**NOTE: This documentation should be comprehensive enough for new team members to set up and use Lightning.AI Studios for data preparation.**",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "Qwen3 Tokenizer Analysis and Optimization",
          "description": "Analyze and optimize data processing for Qwen3-4B's tokenizer patterns",
          "dependencies": [
            3
          ],
          "details": "Create comprehensive analysis tools for Qwen3-4B's tokenization patterns. Implement visualization utilities for token usage patterns. Develop optimization strategies for multi-token words. Create efficient filtering mechanisms for English-only token subset. Implement validation tools to ensure data compatibility with Qwen3's tokenization requirements.\n\nSpecific tasks:\n1. Create token frequency analysis tools for Qwen3-4B vocabulary\n2. Implement visualization utilities for token distribution in datasets\n3. Develop optimization strategies for handling multi-token words\n4. Create efficient filtering mechanisms for English-only token subset\n5. Implement validation tools to ensure data compatibility with Qwen3's tokenization\n6. Analyze impact of thinking vs. non-thinking modes on token usage\n\nFile locations:\n- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`\n- Token analysis tools: `src/unsloth/token_analysis.py`\n- Visualization utilities: `src/unsloth/token_visualization.py`\n\nOutput locations:\n- `results/unsloth/token_analysis/` (Qwen3 token usage analysis)\n- `results/unsloth/figures/token_distribution/` (Token distribution visualizations)\n\n**NOTE: While analysis can be done locally, validation should be performed with the actual Qwen3-4B tokenizer in a cloud environment.**",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "Thinking Mode Implementation",
          "description": "Implement support for both thinking and non-thinking modes in data preparation",
          "dependencies": [
            3,
            7
          ],
          "details": "Create flexible data preparation pipelines that support both thinking and non-thinking modes. Implement configuration options for switching between modes. Develop analysis tools to compare performance between modes. Create visualization utilities for token usage patterns in different modes.\n\nSpecific tasks:\n1. Implement thinking mode formatting with <thinking> tags for Qwen3-4B\n2. Create non-thinking mode formatting for direct response generation\n3. Develop configuration options for switching between modes\n4. Implement analysis tools to compare performance between modes\n5. Create visualization utilities for token usage patterns in different modes\n\nFile locations:\n- Dataset preparation: `src/unsloth/dataset.py`\n- Thinking mode utilities: `src/unsloth/thinking_mode.py`\n- Configuration handling: `src/unsloth/config.py`\n\nOutput locations:\n- `results/unsloth/thinking_analysis/` (Thinking mode analysis)\n- `results/unsloth/figures/thinking_comparison/` (Thinking mode comparison visualizations)\n\n**NOTE: This task requires testing in a cloud GPU environment with the actual Qwen3-4B model.**",
          "status": "pending"
        }
      ]
    },
    {
      "id": 8,
      "title": "Model Fine-tuning and Experimentation",
      "description": "Implement the training loop and run experiments with different hyperparameters to find the optimal configuration for effective transfer learning using Qwen3-4B model in cloud GPU environments.",
      "status": "pending",
      "dependencies": [
        4,
        6,
        7
      ],
      "priority": "high",
      "details": "**IMPORTANT NOTE: This task requires a cloud GPU environment for Qwen3-4B fine-tuning. Do not attempt on local Mac.**\n\n1. Create a reusable training script that accepts hyperparameter configs (`src/training/train.py`)\n2. Implement the training loop using Unsloth with Qwen3-4B on Google Colab or https://lightning.ai/lars/home\n3. Set up checkpoint saving and loading system (`src/training/checkpointing.py`)\n4. Implement early stopping based on validation metrics\n5. Configure Qwen3-4B specific features:\n   - Support for thinking/non-thinking modes\n   - Sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n   - English-only token subset handling during training\n   - Adaptation to Qwen3's tokenizer patterns\n6. Run experiments with different hyperparameters focused on transfer learning effectiveness:\n   - LoRA rank (r): [4, 8, 16, 32]\n   - LoRA alpha: [8, 16, 32, 64]\n   - Learning rate: [1e-4, 2e-4, 5e-4, 1e-3]\n   - Batch size: [4, 8, 16, 32]\n   - Gradient accumulation steps: [1, 2, 4, 8]\n   - Training steps: [500, 1000, 2000, 5000]\n   - Thinking mode: [enabled, disabled]\n7. Track all experiments in W&B with both spelling metrics and transfer metrics\n8. Implement evaluation metrics suitable for both thinking and non-thinking modes\n9. Analyze correlation between spelling improvement and transfer performance\n10. Identify training patterns that lead to better transfer learning\n\n**Transfer Learning Focus:**\n- Primary training on spelling variation tasks\n- Monitor transfer learning effectiveness to position/count tasks\n- Track correlation between spelling performance and transfer capabilities\n- Identify which training approaches generalize better across task types\n- Compare performance between thinking and non-thinking modes\n\n**File Structure:**\n- Training Infrastructure:\n  - Training script: `src/training/train.py`\n  - Training utilities: `src/training/utils.py`\n  - Data loaders: `src/training/data_loaders.py`\n  - Model checkpointing: `src/training/checkpointing.py`\n  - Transfer metrics: `src/training/transfer_metrics.py`\n  - Qwen3 utilities: `src/training/qwen3_utils.py`\n\n- Model Components:\n  - Model architecture: `src/models/spelling_model.py`\n  - Loss functions: `src/models/losses.py`\n  - Metrics tracking: `src/models/metrics.py`\n  - Model utilities: `src/models/utils.py`\n  - Transfer evaluation: `src/models/transfer_eval.py`\n  - Thinking mode handler: `src/models/thinking_mode.py`\n\n- Deployment Components:\n  - Model export: `src/deployment/model_export.py`\n  - Lightning.AI Studio setup: `src/deployment/lightning_studio.py`\n  - API implementation: `src/deployment/api.py`\n  - Performance monitoring: `src/deployment/monitoring.py`\n  - Load testing: `src/deployment/benchmark.py`\n  - Performance visualization: `src/deployment/visualization.py`\n  - Report generation: `src/deployment/report.py`\n  - Transfer analysis: `src/deployment/transfer_analysis.py`\n  - Auto-scaling config: `src/deployment/scaling_config.py`\n\n- Configurations:\n  - Training config: `configs/training/config.yaml`\n  - Model config: `configs/models/model_config.yaml`\n  - Optimizer config: `configs/training/optimizer.yaml`\n  - Scheduler config: `configs/training/scheduler.yaml`\n  - Transfer config: `configs/training/transfer_config.yaml`\n  - Qwen3 config: `configs/models/qwen3_config.yaml`\n  - Lightning.AI deployment config: `configs/deployment/lightning_config.yaml`\n\n- Results and Checkpoints:\n  - Model checkpoints: `checkpoints/`\n  - Training logs: `results/training_logs/`\n  - Performance metrics: `results/metrics/`\n  - Error analysis: `results/error_analysis/`\n  - Transfer analysis: `results/transfer_analysis/`\n  - Deployment results: `results/deployment/`\n    - Figures: `results/deployment/figures/`\n    - Reports: `results/deployment/reports/`\n    - Data: `results/deployment/data/`\n    - Models: `results/deployment/models/`\n\n- Documentation:\n  - Training guide: `docs/training.md`\n  - Model architecture: `docs/model.md`\n  - Results analysis: `docs/results.md`\n  - Transfer learning analysis: `docs/transfer_learning.md`\n  - Lightning.AI deployment guide: `docs/lightning_deployment.md`\n  - Qwen3 specific guide: `docs/qwen3_guide.md`\n\nImplementation:\n```python\nimport os\nimport yaml\nimport torch\nimport wandb\nimport numpy as np\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Main experiment runner\ndef run_experiment(config_path):\n    # Load config\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    \n    # Initialize W&B\n    run = wandb.init(\n        project=\"llm-spelling-finetuning\",\n        name=config[\"experiment_name\"],\n        config=config,\n        reinit=True\n    )\n    \n    # Load model and tokenizer with Unsloth\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"Qwen/Qwen3-4B\",\n        max_seq_length=512,\n        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n        load_in_4bit=True\n    )\n    \n    # Configure Qwen3-specific sampling parameters\n    generation_config = model.generation_config\n    generation_config.temperature = 0.6\n    generation_config.top_p = 0.95\n    generation_config.top_k = 20\n    generation_config.min_p = 0.0\n    \n    # Add LoRA adapters\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=config[\"lora_config\"][\"r\"],\n        target_modules=config[\"lora_config\"][\"target_modules\"],\n        lora_alpha=config[\"lora_config\"][\"alpha\"],\n        lora_dropout=config[\"lora_config\"][\"dropout\"],\n        bias=\"none\",\n        use_gradient_checkpointing=True,\n        random_state=42\n    )\n    \n    # Load datasets - both spelling and transfer task datasets\n    spelling_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\")\n    position_dataset = load_dataset(\"YOUR-USERNAME/llm-position-dataset\")\n    count_dataset = load_dataset(\"YOUR-USERNAME/llm-count-dataset\")\n    \n    # Format dataset for instruction fine-tuning with Qwen3's format\n    def formatting_func(examples, thinking_mode=False):\n        questions = examples[\"question\"]\n        answers = examples[\"answer\"]\n        \n        if thinking_mode:\n            # Format with thinking mode enabled\n            prompts = [f\"<|im_start|>user\\n{q}\\n<|im_end|>\\n<|im_start|>assistant\\n<|thinking|>\" for q in questions]\n            completions = [f\"Let me solve this step by step...\\n{a}\\n<|endthinking|>\\n{a}<|im_end|>\" for a in answers]\n        else:\n            # Format with standard mode (no thinking)\n            prompts = [f\"<|im_start|>user\\n{q}\\n<|im_end|>\\n<|im_start|>assistant\\n\" for q in questions]\n            completions = [f\"{a}<|im_end|>\" for a in answers]\n        \n        return {\"prompt\": prompts, \"completion\": completions}\n    \n    # Apply formatting based on thinking mode configuration\n    thinking_mode = config.get(\"thinking_mode\", False)\n    train_dataset = spelling_dataset[\"train\"].map(lambda x: formatting_func(x, thinking_mode), batched=True)\n    eval_dataset = spelling_dataset[\"validation\"].map(lambda x: formatting_func(x, thinking_mode), batched=True)\n    \n    # Format transfer task datasets for evaluation\n    position_eval_dataset = position_dataset[\"validation\"].map(lambda x: formatting_func(x, thinking_mode), batched=True)\n    count_eval_dataset = count_dataset[\"validation\"].map(lambda x: formatting_func(x, thinking_mode), batched=True)\n    \n    # Set up output directory\n    output_dir = f\"./results/{config['experiment_name']}_{config['timestamp']}\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create training arguments\n    training_args = FastLanguageModel.get_train_args(\n        output_dir=output_dir,\n        per_device_train_batch_size=config[\"training_config\"][\"per_device_train_batch_size\"],\n        gradient_accumulation_steps=config[\"training_config\"][\"gradient_accumulation_steps\"],\n        warmup_steps=config[\"training_config\"][\"warmup_steps\"],\n        max_steps=config[\"training_config\"][\"max_steps\"],\n        learning_rate=config[\"training_config\"][\"learning_rate\"],\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=10,\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        save_strategy=\"steps\",\n        save_steps=200,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        optim=\"adamw_torch\",\n        max_grad_norm=0.3,\n        report_to=\"wandb\"\n    )\n    \n    # Define compute metrics function for transfer learning evaluation\n    def compute_metrics(eval_pred):\n        predictions, labels = eval_pred\n        predictions = np.argmax(predictions, axis=2)\n        \n        # Only consider non-padding tokens\n        mask = labels != -100\n        labels = labels[mask]\n        predictions = predictions[mask]\n        \n        accuracy = accuracy_score(labels, predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n        \n        # Add thinking mode specific metrics if applicable\n        metrics = {\n            \"accuracy\": accuracy,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        }\n        \n        if thinking_mode:\n            # Add thinking-specific metrics\n            # This would analyze the quality of the thinking process\n            # Implementation depends on specific requirements\n            pass\n        \n        return metrics\n    \n    # Create trainer with early stopping\n    trainer = FastLanguageModel.get_trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=FastLanguageModel.get_data_collator(tokenizer),\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n    )\n    \n    # Train model\n    trainer.train()\n    \n    # Save final model\n    trainer.save_model(f\"{output_dir}/final\")\n    \n    # Evaluate on spelling validation set\n    spelling_eval_results = trainer.evaluate()\n    \n    # Evaluate on transfer task datasets\n    position_eval_results = trainer.evaluate(eval_dataset=position_eval_dataset)\n    count_eval_results = trainer.evaluate(eval_dataset=count_eval_dataset)\n    \n    # Calculate transfer metrics\n    transfer_metrics = {\n        \"position_transfer_score\": position_eval_results[\"eval_accuracy\"],\n        \"count_transfer_score\": count_eval_results[\"eval_accuracy\"],\n        \"avg_transfer_score\": (position_eval_results[\"eval_accuracy\"] + count_eval_results[\"eval_accuracy\"]) / 2,\n        \"spelling_to_position_ratio\": spelling_eval_results[\"eval_accuracy\"] / position_eval_results[\"eval_accuracy\"] if position_eval_results[\"eval_accuracy\"] > 0 else 0,\n        \"spelling_to_count_ratio\": spelling_eval_results[\"eval_accuracy\"] / count_eval_results[\"eval_accuracy\"] if count_eval_results[\"eval_accuracy\"] > 0 else 0\n    }\n    \n    # Log all results\n    wandb.log({\n        **spelling_eval_results,\n        \"position_eval_results\": position_eval_results,\n        \"count_eval_results\": count_eval_results,\n        **transfer_metrics,\n        \"thinking_mode\": thinking_mode\n    })\n    \n    # Save evaluation results\n    all_results = {\n        \"spelling_eval_results\": spelling_eval_results,\n        \"position_eval_results\": position_eval_results,\n        \"count_eval_results\": count_eval_results,\n        \"transfer_metrics\": transfer_metrics,\n        \"thinking_mode\": thinking_mode\n    }\n    \n    with open(f\"{output_dir}/eval_results.yaml\", \"w\") as f:\n        yaml.dump(all_results, f)\n    \n    # Close wandb run\n    wandb.finish()\n    \n    return output_dir, all_results\n\n# Run multiple experiments\ndef run_experiments(config_paths):\n    results = {}\n    for config_path in config_paths:\n        print(f\"Running experiment with config: {config_path}\")\n        output_dir, eval_results = run_experiment(config_path)\n        \n        # Extract config name\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n        \n        results[config[\"experiment_name\"]] = {\n            \"output_dir\": output_dir,\n            \"eval_results\": eval_results\n        }\n    \n    # Analyze transfer learning effectiveness across experiments\n    analyze_transfer_effectiveness(results)\n    \n    # Save all results\n    with open(\"experiment_results_summary.yaml\", \"w\") as f:\n        yaml.dump(results, f)\n    \n    return results\n\n# Analyze transfer learning effectiveness\ndef analyze_transfer_effectiveness(results):\n    \"\"\"Analyze which training patterns lead to better transfer learning\"\"\"\n    # Extract key metrics for analysis\n    experiment_metrics = []\n    for exp_name, exp_data in results.items():\n        with open(f\"{exp_data['output_dir']}/config.yaml\", \"r\") as f:\n            config = yaml.safe_load(f)\n        \n        metrics = {\n            \"experiment_name\": exp_name,\n            \"lora_rank\": config[\"lora_config\"][\"r\"],\n            \"lora_alpha\": config[\"lora_config\"][\"alpha\"],\n            \"learning_rate\": config[\"training_config\"][\"learning_rate\"],\n            \"batch_size\": config[\"training_config\"][\"per_device_train_batch_size\"],\n            \"grad_accum_steps\": config[\"training_config\"][\"gradient_accumulation_steps\"],\n            \"training_steps\": config[\"training_config\"][\"max_steps\"],\n            \"thinking_mode\": config.get(\"thinking_mode\", False),\n            \"spelling_accuracy\": exp_data[\"eval_results\"][\"spelling_eval_results\"][\"eval_accuracy\"],\n            \"position_accuracy\": exp_data[\"eval_results\"][\"position_eval_results\"][\"eval_accuracy\"],\n            \"count_accuracy\": exp_data[\"eval_results\"][\"count_eval_results\"][\"eval_accuracy\"],\n            \"avg_transfer_score\": exp_data[\"eval_results\"][\"transfer_metrics\"][\"avg_transfer_score\"]\n        }\n        experiment_metrics.append(metrics)\n    \n    # Calculate correlations between spelling performance and transfer tasks\n    import pandas as pd\n    df = pd.DataFrame(experiment_metrics)\n    correlation = df[[\"spelling_accuracy\", \"position_accuracy\", \"count_accuracy\", \"avg_transfer_score\"]].corr()\n    \n    # Analyze impact of thinking mode on transfer learning\n    thinking_vs_standard = df.groupby(\"thinking_mode\")[[\"spelling_accuracy\", \"position_accuracy\", \n                                                      \"count_accuracy\", \"avg_transfer_score\"]].mean()\n    \n    # Identify top performing configurations for transfer learning\n    df_sorted = df.sort_values(by=\"avg_transfer_score\", ascending=False)\n    top_configs = df_sorted.head(5)\n    \n    # Save analysis results\n    os.makedirs(\"results/transfer_analysis\", exist_ok=True)\n    correlation.to_csv(\"results/transfer_analysis/metric_correlations.csv\")\n    thinking_vs_standard.to_csv(\"results/transfer_analysis/thinking_mode_impact.csv\")\n    top_configs.to_csv(\"results/transfer_analysis/top_transfer_configs.csv\")\n    \n    # Generate visualization of transfer learning effectiveness\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df[\"spelling_accuracy\"], df[\"avg_transfer_score\"], \n                c=df[\"thinking_mode\"].map({True: 'red', False: 'blue'}))\n    plt.xlabel(\"Spelling Task Accuracy\")\n    plt.ylabel(\"Average Transfer Task Accuracy\")\n    plt.title(\"Correlation between Spelling Performance and Transfer Learning\")\n    plt.legend([\"Thinking Mode\", \"Standard Mode\"])\n    plt.savefig(\"results/transfer_analysis/spelling_vs_transfer.png\")\n    \n    # Log findings to wandb\n    wandb.init(project=\"llm-spelling-finetuning\", name=\"transfer_analysis\", reinit=True)\n    wandb.log({\n        \"correlation_matrix\": wandb.Table(dataframe=correlation),\n        \"thinking_vs_standard\": wandb.Table(dataframe=thinking_vs_standard),\n        \"top_transfer_configs\": wandb.Table(dataframe=top_configs),\n        \"spelling_vs_transfer_plot\": wandb.Image(\"results/transfer_analysis/spelling_vs_transfer.png\")\n    })\n    wandb.finish()\n```",
      "testStrategy": "1. Verify training script (`src/training/train.py`) runs without errors on Google Colab or lightning.ai with Qwen3-4B\n2. Test both thinking and non-thinking modes to ensure proper formatting and processing\n3. Confirm experiments are properly tracked in W&B with both spelling and transfer metrics\n4. Verify Qwen3-specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) are correctly applied\n5. Test English-only token subset handling during training\n6. Check that checkpoints are saved correctly to `checkpoints/` directory and can be loaded properly\n7. Verify early stopping works as expected\n8. Test that the best model is loaded at the end of training\n9. Compare performance across different hyperparameter configurations using Python scripts\n10. Specifically test the impact of thinking mode vs. non-thinking mode on performance\n11. Ensure all experiment results are properly saved to `results/metrics/` and `results/training_logs/`\n12. Verify the environment is properly set up with GPU access before starting experiments\n13. Validate that all configuration files in `configs/` directory are properly loaded and applied\n14. Test transfer learning evaluation on position and count tasks with both thinking and non-thinking modes\n15. Verify correlation analysis between spelling performance and transfer capabilities\n16. Test the transfer analysis visualization generation, including thinking mode comparisons\n17. Validate the identification of optimal training patterns for transfer learning\n18. Test deployment scripts in `src/deployment/` directory with Qwen3-4B models:\n    - Verify model export functionality in `model_export.py`\n    - Test Lightning.AI Studio setup in `lightning_studio.py`\n    - Test API implementation in `api.py`\n    - Validate monitoring capabilities in `monitoring.py`\n    - Check benchmark functionality in `benchmark.py`\n    - Test visualization generation in `visualization.py`\n    - Verify HTML report generation in `report.py`\n    - Test transfer analysis in `transfer_analysis.py`\n    - Validate auto-scaling configuration in `scaling_config.py`\n19. Ensure all deployment and analysis results are correctly saved to the appropriate directories\n20. Test Lightning.AI deployment with Qwen3-4B models:\n    - Verify dedicated deployment Studio creation\n    - Test environment isolation and dependency management\n    - Validate auto-scaling and load balancing configuration\n    - Check monitoring and logging setup\n    - Test cost optimization mechanisms\n    - Verify automated testing and validation pipeline",
      "subtasks": [
        {
          "id": 1,
          "title": "Training Script Implementation",
          "description": "Develop a robust training script that handles the fine-tuning process for pre-trained models",
          "dependencies": [],
          "details": "Create a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.\n<info added on 2025-05-07T14:48:16.314Z>\nCreate a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.\n\nThis task can be worked on independently and in parallel with others. The training script implementation has no dependencies and is parallelizable (parallelizable: true).\n</info added on 2025-05-07T14:48:16.314Z>",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Checkpoint Management System",
          "description": "Implement a comprehensive checkpoint system to save and restore model states",
          "dependencies": [
            1
          ],
          "details": "Design a checkpoint manager in `src/training/checkpointing.py` that: saves model weights at configurable intervals, stores optimizer states, implements versioning for checkpoints, provides functionality to resume training from any checkpoint, includes cleanup mechanisms for old checkpoints, and ensures compatibility across different hardware configurations. All checkpoint operations should be compatible with Google Colab or lightning.ai cloud environments. Checkpoints should be saved to the `checkpoints/` directory with appropriate naming conventions.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Early Stopping Mechanism",
          "description": "Develop an early stopping system to prevent overfitting and optimize training time",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement a configurable early stopping mechanism that: monitors validation metrics (loss, accuracy), applies patience parameters to allow for fluctuations, saves best model states when improvements occur, provides restoration of best model after training, includes visualization of stopping point, and allows for custom stopping criteria definition. Ensure the implementation works reliably in cloud GPU environments like Google Colab or lightning.ai. The early stopping configuration should be defined in `configs/training/config.yaml` and the implementation should be integrated with the checkpoint system in `src/training/checkpointing.py`.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Hyperparameter Experimentation Framework",
          "description": "Create a framework for systematic hyperparameter tuning and experimentation",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Develop a hyperparameter experimentation system that: supports grid search and random search methods, enables parallel experiment execution, provides configuration management for experiments, implements parameter scheduling (learning rate decay), integrates with checkpoint system, and includes mechanisms to handle failed experiments gracefully. Design the framework to work efficiently in cloud GPU environments (Google Colab or lightning.ai) and to handle potential session timeouts or disconnections. Configuration files should be stored in the `configs/` directory with appropriate organization. Implement utilities in `src/training/utils.py` to support experiment management.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Results Tracking and Analysis System",
          "description": "Build a comprehensive system to track, visualize and compare experiment results",
          "dependencies": [
            4
          ],
          "details": "Implement a results management system that: stores metrics for all experiments in `results/metrics/`, generates comparative visualizations using Python scripts, calculates statistical significance of improvements, exports results in standard formats, provides filtering and sorting capabilities, and integrates with external visualization tools if needed. Ensure all results are properly saved to persistent storage accessible after cloud GPU sessions end. Implement error analysis functionality in `src/deployment/visualization.py` and `src/deployment/report.py` to help understand model performance and limitations. Use the deployment scripts to generate HTML reports and visualizations that will be saved to `results/deployment/reports/` and `results/deployment/figures/` respectively.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Cloud Environment Setup Guide",
          "description": "Create documentation for setting up the required cloud GPU environment",
          "dependencies": [],
          "details": "Develop a comprehensive guide in `docs/training.md` for setting up the training environment on Google Colab or lightning.ai, including: step-by-step instructions for accessing GPU resources, installing Unsloth and other dependencies, configuring W&B integration, handling file storage and persistence, and troubleshooting common issues. Include examples of notebook configurations that work well for this specific fine-tuning task. Create additional documentation in `docs/model.md` for model architecture details and in `docs/results.md` for analyzing training results.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Deployment Scripts Implementation",
          "description": "Develop Python scripts for model deployment and performance analysis",
          "dependencies": [
            5
          ],
          "details": "Create a suite of Python scripts in the `src/deployment/` directory to handle all aspects of model deployment and analysis using Lightning.AI Studios:\n\n1. `model_export.py`: Implement model export and conversion functionality with command-line interface\n2. `lightning_studio.py`: Create dedicated deployment Studio setup following the \"one Studio, one task\" principle\n3. `api.py`: Create an API implementation using Lightning.AI's serving engine\n4. `monitoring.py`: Develop performance monitoring and logging capabilities\n5. `benchmark.py`: Implement load testing functionality\n6. `visualization.py`: Create performance visualization tools\n7. `report.py`: Develop HTML report generation\n8. `scaling_config.py`: Configure auto-scaling and load balancing for the deployment\n\nEnsure all scripts have proper command-line interfaces for flexibility and can be run independently. Implement proper Python packaging with clear separation of concerns. Configure environment isolation and dependency management for the Lightning.AI Studio. Implement cost optimization through efficient resource usage. Set up automated testing and validation in the deployment pipeline.\n\nAll output from these scripts should be saved to the appropriate directories under `results/deployment/`:\n- Figures: `results/deployment/figures/`\n- Reports: `results/deployment/reports/`\n- Performance data: `results/deployment/data/`\n- Exported models: `results/deployment/models/`\n\nCreate comprehensive deployment documentation in `docs/lightning_deployment.md` including Lightning.AI-specific setup instructions.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Transfer Learning Evaluation System",
          "description": "Implement a system to evaluate transfer learning effectiveness across different tasks",
          "dependencies": [
            1,
            4
          ],
          "details": "Develop a transfer learning evaluation system in `src/training/transfer_metrics.py` and `src/models/transfer_eval.py` that: evaluates models trained on spelling tasks against position and count tasks, calculates transfer metrics and correlation scores, identifies which training patterns lead to better transfer, visualizes the relationship between spelling performance and transfer capabilities, and generates comprehensive reports on transfer learning effectiveness. The system should integrate with the existing experimentation framework and results tracking system. All transfer analysis results should be saved to `results/transfer_analysis/` directory.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Transfer Learning Documentation",
          "description": "Create comprehensive documentation on transfer learning analysis and findings",
          "dependencies": [
            8
          ],
          "details": "Develop detailed documentation in `docs/transfer_learning.md` that explains: the transfer learning evaluation methodology, metrics used to assess transfer effectiveness, analysis of correlation between spelling performance and transfer capabilities, identification of optimal training patterns for transfer learning, visualization of key findings, and recommendations for maximizing transfer learning effectiveness. Include examples and case studies from the experiments to illustrate important concepts and findings.",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "Qwen3-4B Thinking Mode Implementation",
          "description": "Implement support for Qwen3-4B's thinking and non-thinking modes",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated module in `src/models/thinking_mode.py` that handles Qwen3-4B's thinking and non-thinking modes. Implement functionality to format prompts correctly for each mode, process model outputs appropriately, and evaluate performance differences. Develop utilities in `src/training/qwen3_utils.py` to support Qwen3-specific features including handling of the English-only token subset during training and adapting to Qwen3's tokenizer patterns. Create configuration options in `configs/models/qwen3_config.yaml` to control thinking mode behavior and other Qwen3-specific parameters. Document the implementation details and usage guidelines in `docs/qwen3_guide.md`.",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "Qwen3-4B Sampling Parameters Configuration",
          "description": "Implement configuration for Qwen3-4B's specific sampling parameters",
          "dependencies": [
            1,
            10
          ],
          "details": "Create a configuration system for Qwen3-4B's sampling parameters in `configs/models/qwen3_config.yaml` that includes Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 as default values. Implement utilities in `src/training/qwen3_utils.py` to apply these parameters during model initialization and inference. Add functionality to experiment with different sampling parameter combinations and analyze their impact on model performance. Document the sampling parameters and their effects in `docs/qwen3_guide.md`.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 9,
      "title": "Comprehensive Model Evaluation",
      "description": "Evaluate the fine-tuned models on the test set using multiple metrics and perform detailed analysis of transfer learning effectiveness between spelling and position/count tasks, with specific focus on Qwen3-4B's thinking and non-thinking modes, English token subset, and sampling parameters, leveraging Lightning.AI Studios for efficient GPU-based evaluation.",
      "status": "pending",
      "dependencies": [
        8
      ],
      "priority": "medium",
      "details": "1. Evaluate the best model on the test set with separate pipelines for spelling and position/count tasks\n2. Implement comprehensive evaluation metrics:\n   - Direct Spelling Performance Metrics\n   - Letter Count Accuracy\n   - Letter Position Accuracy\n   - Character-Level Accuracy\n   - Levenshtein Distance\n   - Token-Level Perplexity\n   - Transfer Learning Effectiveness Metrics\n   - Correlation Analysis between Spelling and Position/Count Performance\n3. Evaluate both thinking and non-thinking modes of Qwen3-4B\n4. Assess performance with the English-only token subset\n5. Analyze the impact of Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n6. Compare performance between modes for both spelling and transfer tasks\n7. Perform detailed error analysis for both task types and modes\n8. Investigate transfer learning patterns and identify successful/unsuccessful transfer cases\n9. Analyze model's generalization capabilities\n10. Create visualizations of the results with mode-specific comparisons\n11. Compare performance between base and fine-tuned models\n12. Evaluate the effectiveness of the English token filtering approach\n13. Analyze how thinking mode affects transfer learning capabilities\n14. Implement mode-specific evaluation metrics and visualization tools\n15. Ensure proper handling of Qwen3's tokenizer patterns in evaluation\n\nNOTE: Use Lightning.AI Studios for all model evaluation, leveraging their GPU switching feature (CPU → T4 → A100) for cost-effective evaluation.\n\nFile Structure:\n- Main evaluator: `src/evaluation/evaluator.py`\n- Metrics calculator: `src/evaluation/metrics.py`\n- Error analyzer: `src/evaluation/error_analysis.py`\n- Transfer learning analyzer: `src/evaluation/transfer_analysis.py`\n- Visualization utils: `src/evaluation/visualization.py`\n- Qwen3 mode evaluator: `src/evaluation/qwen3_mode_evaluator.py`\n- English token analyzer: `src/evaluation/token_analysis.py`\n- Test data: `data/splits/test.json`\n- Challenge sets: `data/splits/challenge_sets/`\n- Edge cases: `data/splits/edge_cases/`\n- Error categories: `data/splits/error_categories/`\n- Evaluation results: `results/evaluation/`\n- Performance metrics: `results/evaluation/metrics/`\n- Error analysis: `results/evaluation/error_analysis/`\n- Transfer learning analysis: `results/evaluation/transfer_analysis/`\n- Mode comparison: `results/evaluation/mode_comparison/`\n- Token subset analysis: `results/evaluation/token_analysis/`\n- Visualizations: `results/evaluation/plots/`\n\nImplementation:\n```python\nimport torch\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport Levenshtein\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nimport wandb\nfrom scipy.stats import pearsonr, spearmanr\n\n# Load models for comparison\ndef load_models(base_model_name, finetuned_model_path):\n    # Load base model\n    base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n    base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n    base_tokenizer.pad_token = base_tokenizer.eos_token\n    \n    # Load fine-tuned model\n    finetuned_model = AutoModelForCausalLM.from_pretrained(finetuned_model_path)\n    finetuned_tokenizer = AutoTokenizer.from_pretrained(base_model_name)  # Use same tokenizer\n    finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token\n    \n    return {\n        \"base\": (base_model, base_tokenizer),\n        \"finetuned\": (finetuned_model, finetuned_tokenizer)\n    }\n\n# Generate answer from model with support for Qwen3 thinking/non-thinking modes\ndef generate_answer(model, tokenizer, question, max_length=10, thinking_mode=False, \n                   temperature=0.6, top_p=0.95, top_k=20, min_p=0):\n    inputs = tokenizer(question, return_tensors=\"pt\")\n    \n    # Configure generation parameters based on Qwen3 specifications\n    generation_config = {\n        \"max_length\": len(inputs.input_ids[0]) + max_length,\n        \"pad_token_id\": tokenizer.eos_token_id,\n        \"do_sample\": temperature > 0,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"top_k\": top_k,\n    }\n    \n    # Add min_p if supported by the model\n    if hasattr(model.config, \"min_p\"):\n        generation_config[\"min_p\"] = min_p\n    \n    # Add thinking mode configuration for Qwen3\n    if \"qwen\" in tokenizer.name_or_path.lower():\n        if thinking_mode:\n            # Enable thinking mode\n            generation_config[\"thinking\"] = True\n        else:\n            # Disable thinking mode\n            generation_config[\"thinking\"] = False\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs.input_ids,\n            **generation_config\n        )\n    \n    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n    \n    # Extract just the first token/character for letter position or first number for letter count\n    if \"How many\" in question:\n        # Extract first number\n        import re\n        numbers = re.findall(r'\\d+', response)\n        return numbers[0] if numbers else response.strip()\n    elif \"What is the letter\" in question:\n        # Extract first character\n        return response.strip()[0] if response.strip() else \"\"\n    else:\n        # For spelling tasks, return the full response\n        return response.strip()\n\n# Filter to English-only tokens for Qwen3\ndef filter_english_tokens(tokenizer, input_ids):\n    # This is a simplified example - actual implementation would depend on Qwen3's token structure\n    english_token_ids = [id for id in range(tokenizer.vocab_size) if is_english_token(tokenizer, id)]\n    english_token_set = set(english_token_ids)\n    \n    # Filter logits to only allow English tokens\n    def filter_fn(input_ids, scores):\n        for batch_idx in range(scores.shape[0]):\n            for token_idx in range(scores.shape[1]):\n                if token_idx not in english_token_set:\n                    scores[batch_idx, token_idx] = -float('inf')\n        return scores\n    \n    return filter_fn\n\n# Helper function to determine if a token is English\ndef is_english_token(tokenizer, token_id):\n    token = tokenizer.convert_ids_to_tokens(token_id)\n    # Simple heuristic - can be improved with more sophisticated analysis\n    return all(c.isascii() and (c.isalnum() or c.isspace() or c in string.punctuation) for c in token)\n\n# Calculate letter count accuracy\ndef calc_letter_count_accuracy(model, tokenizer, test_dataset, thinking_mode=False, english_only=False):\n    correct = 0\n    total = 0\n    results = []\n    \n    for item in test_dataset:\n        if item[\"question_type\"] != \"letter_count\":\n            continue\n            \n        prediction = generate_answer(model, tokenizer, item[\"question\"], thinking_mode=thinking_mode)\n        is_correct = prediction == item[\"answer\"]\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"correct\": is_correct,\n            \"word\": item[\"word\"],\n            \"thinking_mode\": thinking_mode,\n            \"english_only\": english_only\n        })\n        \n        correct += int(is_correct)\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"Letter Count Accuracy (Thinking: {thinking_mode}, English-only: {english_only}): {accuracy:.4f} ({correct}/{total})\")\n    \n    return accuracy, results\n\n# Calculate letter position accuracy\ndef calc_letter_position_accuracy(model, tokenizer, test_dataset, thinking_mode=False, english_only=False):\n    correct = 0\n    total = 0\n    results = []\n    \n    for item in test_dataset:\n        if item[\"question_type\"] != \"letter_position\":\n            continue\n            \n        prediction = generate_answer(model, tokenizer, item[\"question\"], thinking_mode=thinking_mode)\n        is_correct = prediction.lower() == item[\"answer\"].lower()\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"correct\": is_correct,\n            \"word\": item[\"word\"],\n            \"thinking_mode\": thinking_mode,\n            \"english_only\": english_only\n        })\n        \n        correct += int(is_correct)\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"Letter Position Accuracy (Thinking: {thinking_mode}, English-only: {english_only}): {accuracy:.4f} ({correct}/{total})\")\n    \n    return accuracy, results\n\n# Calculate spelling accuracy\ndef calc_spelling_accuracy(model, tokenizer, test_dataset, thinking_mode=False, english_only=False):\n    correct = 0\n    total = 0\n    results = []\n    \n    for item in test_dataset:\n        if item[\"question_type\"] != \"spelling\":\n            continue\n            \n        prediction = generate_answer(model, tokenizer, item[\"question\"], thinking_mode=thinking_mode)\n        is_correct = prediction.lower() == item[\"answer\"].lower()\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"correct\": is_correct,\n            \"word\": item[\"word\"],\n            \"thinking_mode\": thinking_mode,\n            \"english_only\": english_only\n        })\n        \n        correct += int(is_correct)\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"Spelling Accuracy (Thinking: {thinking_mode}, English-only: {english_only}): {accuracy:.4f} ({correct}/{total})\")\n    \n    return accuracy, results\n\n# Calculate character-level accuracy\ndef calc_character_level_accuracy(model, tokenizer, test_dataset, thinking_mode=False, english_only=False):\n    total_char_accuracy = 0\n    total_samples = 0\n    results = []\n\n    for item in test_dataset:\n        if item[\"question_type\"] not in [\"letter_position\", \"letter_count\", \"spelling\"]:\n            continue\n\n        prediction = generate_answer(model, tokenizer, item[\"question\"], thinking_mode=thinking_mode)\n        pred_chars = prediction.strip().lower().replace(\" \", \"\")\n        true_chars = item[\"answer\"].lower()\n\n        # Calculate character-by-character accuracy\n        correct_chars = 0\n        for i, char in enumerate(true_chars):\n            if i < len(pred_chars) and pred_chars[i] == char:\n                correct_chars += 1\n\n        char_accuracy = correct_chars / len(true_chars) if len(true_chars) > 0 else 0\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"char_accuracy\": char_accuracy,\n            \"word\": item[\"word\"],\n            \"question_type\": item[\"question_type\"],\n            \"thinking_mode\": thinking_mode,\n            \"english_only\": english_only\n        })\n        \n        total_char_accuracy += char_accuracy\n        total_samples += 1\n\n    avg_char_accuracy = total_char_accuracy / total_samples if total_samples > 0 else 0\n    print(f\"Character-Level Accuracy (Thinking: {thinking_mode}, English-only: {english_only}): {avg_char_accuracy:.4f}\")\n    \n    return avg_char_accuracy, results\n\n# Calculate Levenshtein distance\ndef calc_levenshtein_metrics(model, tokenizer, test_dataset, thinking_mode=False, english_only=False):\n    total_distances = 0\n    total_samples = 0\n    results = []\n\n    for item in test_dataset:\n        if item[\"question_type\"] not in [\"letter_position\", \"letter_count\", \"spelling\"]:\n            continue\n\n        prediction = generate_answer(model, tokenizer, item[\"question\"], thinking_mode=thinking_mode)\n        pred_text = prediction.strip().lower()\n        true_text = item[\"answer\"].lower()\n\n        # Calculate Levenshtein distance\n        distance = Levenshtein.distance(pred_text, true_text)\n        normalized_distance = distance / max(len(pred_text), len(true_text)) if max(len(pred_text), len(true_text)) > 0 else 0\n\n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": true_text,\n            \"prediction\": pred_text,\n            \"levenshtein_distance\": distance,\n            \"normalized_distance\": normalized_distance,\n            \"word\": item[\"word\"],\n            \"question_type\": item[\"question_type\"],\n            \"thinking_mode\": thinking_mode,\n            \"english_only\": english_only\n        })\n        \n        total_distances += normalized_distance\n        total_samples += 1\n\n    avg_distance = total_distances / total_samples if total_samples > 0 else 0\n    print(f\"Average Normalized Levenshtein Distance (Thinking: {thinking_mode}, English-only: {english_only}): {avg_distance:.4f}\")\n    \n    return avg_distance, results\n\n# Analyze transfer learning effectiveness with mode comparison\ndef analyze_transfer_learning(spelling_results, position_count_results):\n    # Create a dictionary to map words to their spelling performance\n    word_spelling_performance = {}\n    for result in spelling_results:\n        word = result[\"word\"]\n        thinking_mode = result.get(\"thinking_mode\", False)\n        key = (word, thinking_mode)\n        if key not in word_spelling_performance:\n            word_spelling_performance[key] = []\n        word_spelling_performance[key].append(result[\"correct\"])\n    \n    # Calculate average spelling performance for each word and mode\n    for key in word_spelling_performance:\n        word_spelling_performance[key] = sum(word_spelling_performance[key]) / len(word_spelling_performance[key])\n    \n    # Map position/count performance to corresponding spelling performance\n    transfer_data = []\n    for result in position_count_results:\n        word = result[\"word\"]\n        thinking_mode = result.get(\"thinking_mode\", False)\n        key = (word, thinking_mode)\n        if key in word_spelling_performance:\n            transfer_data.append({\n                \"word\": word,\n                \"spelling_performance\": word_spelling_performance[key],\n                \"task_performance\": 1 if result[\"correct\"] else 0,\n                \"question_type\": result[\"question_type\"],\n                \"thinking_mode\": thinking_mode\n            })\n    \n    # Separate data by thinking mode\n    thinking_data = [item for item in transfer_data if item[\"thinking_mode\"]]\n    non_thinking_data = [item for item in transfer_data if not item[\"thinking_mode\"]]\n    \n    # Calculate correlation for each mode\n    transfer_analysis = {}\n    \n    # Process each mode\n    for mode_name, mode_data in [(\"thinking\", thinking_data), (\"non_thinking\", non_thinking_data)]:\n        if not mode_data:\n            transfer_analysis[mode_name] = {\"error\": \"No data available for this mode\"}\n            continue\n            \n        spelling_scores = [item[\"spelling_performance\"] for item in mode_data]\n        task_scores = [item[\"task_performance\"] for item in mode_data]\n        \n        if len(spelling_scores) > 1:  # Need at least 2 points for correlation\n            pearson_corr, pearson_p = pearsonr(spelling_scores, task_scores)\n            spearman_corr, spearman_p = spearmanr(spelling_scores, task_scores)\n        else:\n            pearson_corr, pearson_p = 0, 1\n            spearman_corr, spearman_p = 0, 1\n        \n        # Separate by question type\n        position_data = [item for item in mode_data if item[\"question_type\"] == \"letter_position\"]\n        count_data = [item for item in mode_data if item[\"question_type\"] == \"letter_count\"]\n        \n        # Calculate type-specific correlations\n        position_spelling = [item[\"spelling_performance\"] for item in position_data]\n        position_task = [item[\"task_performance\"] for item in position_data]\n        \n        count_spelling = [item[\"spelling_performance\"] for item in count_data]\n        count_task = [item[\"task_performance\"] for item in count_data]\n        \n        if len(position_spelling) > 1:\n            position_pearson, position_p = pearsonr(position_spelling, position_task)\n        else:\n            position_pearson, position_p = 0, 1\n            \n        if len(count_spelling) > 1:\n            count_pearson, count_p = pearsonr(count_spelling, count_task)\n        else:\n            count_pearson, count_p = 0, 1\n        \n        # Identify successful and unsuccessful transfer cases\n        successful_transfer = [item for item in mode_data if item[\"spelling_performance\"] > 0.5 and item[\"task_performance\"] == 1]\n        unsuccessful_transfer = [item for item in mode_data if item[\"spelling_performance\"] > 0.5 and item[\"task_performance\"] == 0]\n        \n        # Group words by spelling patterns\n        pattern_performance = {}\n        for item in mode_data:\n            word = item[\"word\"]\n            # Identify patterns (this is a simplified example - expand as needed)\n            patterns = []\n            if 'ie' in word or 'ei' in word:\n                patterns.append('ie_ei_rule')\n            if word.endswith('e') and any(word.endswith(f'{c}e') for c in 'aeiou'):\n                patterns.append('silent_e')\n            if any(c*2 in word for c in 'abcdefghijklmnopqrstuvwxyz'):\n                patterns.append('double_letter')\n            \n            for pattern in patterns:\n                if pattern not in pattern_performance:\n                    pattern_performance[pattern] = {\n                        \"spelling\": [], \n                        \"position\": [],\n                        \"count\": []\n                    }\n                \n                pattern_performance[pattern][\"spelling\"].append(item[\"spelling_performance\"])\n                \n                if item[\"question_type\"] == \"letter_position\":\n                    pattern_performance[pattern][\"position\"].append(item[\"task_performance\"])\n                elif item[\"question_type\"] == \"letter_count\":\n                    pattern_performance[pattern][\"count\"].append(item[\"task_performance\"])\n        \n        # Calculate average performance by pattern\n        for pattern in pattern_performance:\n            if pattern_performance[pattern][\"spelling\"]:\n                pattern_performance[pattern][\"avg_spelling\"] = sum(pattern_performance[pattern][\"spelling\"]) / len(pattern_performance[pattern][\"spelling\"])\n            else:\n                pattern_performance[pattern][\"avg_spelling\"] = 0\n                \n            if pattern_performance[pattern][\"position\"]:\n                pattern_performance[pattern][\"avg_position\"] = sum(pattern_performance[pattern][\"position\"]) / len(pattern_performance[pattern][\"position\"])\n            else:\n                pattern_performance[pattern][\"avg_position\"] = 0\n                \n            if pattern_performance[pattern][\"count\"]:\n                pattern_performance[pattern][\"avg_count\"] = sum(pattern_performance[pattern][\"count\"]) / len(pattern_performance[pattern][\"count\"])\n            else:\n                pattern_performance[pattern][\"avg_count\"] = 0\n        \n        transfer_analysis[mode_name] = {\n            \"overall_correlation\": {\n                \"pearson\": pearson_corr,\n                \"pearson_p_value\": pearson_p,\n                \"spearman\": spearman_corr,\n                \"spearman_p_value\": spearman_p\n            },\n            \"position_correlation\": {\n                \"pearson\": position_pearson,\n                \"p_value\": position_p\n            },\n            \"count_correlation\": {\n                \"pearson\": count_pearson,\n                \"p_value\": count_p\n            },\n            \"successful_transfer\": {\n                \"count\": len(successful_transfer),\n                \"examples\": successful_transfer[:10]  # Limit to 10 examples\n            },\n            \"unsuccessful_transfer\": {\n                \"count\": len(unsuccessful_transfer),\n                \"examples\": unsuccessful_transfer[:10]  # Limit to 10 examples\n            },\n            \"pattern_performance\": pattern_performance\n        }\n    \n    # Compare thinking vs non-thinking mode transfer effectiveness\n    if \"thinking\" in transfer_analysis and \"non_thinking\" in transfer_analysis:\n        thinking_corr = transfer_analysis[\"thinking\"][\"overall_correlation\"][\"pearson\"]\n        non_thinking_corr = transfer_analysis[\"non_thinking\"][\"overall_correlation\"][\"pearson\"]\n        \n        transfer_analysis[\"mode_comparison\"] = {\n            \"correlation_difference\": thinking_corr - non_thinking_corr,\n            \"better_mode\": \"thinking\" if thinking_corr > non_thinking_corr else \"non_thinking\"\n        }\n    \n    return transfer_analysis, transfer_data\n\n# Perform error analysis with mode comparison\ndef perform_error_analysis(results_dict):\n    error_analysis = {}\n    \n    # Analyze by thinking mode\n    for mode in [True, False]:\n        mode_name = \"thinking\" if mode else \"non_thinking\"\n        mode_results = {}\n        \n        for task_type, results in results_dict.items():\n            mode_results[task_type] = [r for r in results if r.get(\"thinking_mode\", False) == mode]\n        \n        # Analyze letter count errors\n        count_errors = [r for r in mode_results[\"letter_count\"] if not r[\"correct\"]]\n        \n        # Categorize errors\n        error_types = {\n            \"off_by_one\": 0,\n            \"completely_wrong\": 0,\n            \"no_number\": 0,\n            \"other\": 0\n        }\n        \n        for error in count_errors:\n            try:\n                pred = int(error[\"prediction\"])\n                true = int(error[\"expected\"])\n                \n                if abs(pred - true) == 1:\n                    error_types[\"off_by_one\"] += 1\n                else:\n                    error_types[\"completely_wrong\"] += 1\n            except ValueError:\n                if not error[\"prediction\"].strip():\n                    error_types[\"no_number\"] += 1\n                else:\n                    error_types[\"other\"] += 1\n        \n        # Analyze letter position errors\n        position_errors = [r for r in mode_results[\"letter_position\"] if not r[\"correct\"]]\n        \n        # Categorize position errors\n        position_error_types = {\n            \"adjacent_letter\": 0,\n            \"wrong_case\": 0,\n            \"no_response\": 0,\n            \"other\": 0\n        }\n        \n        for error in position_errors:\n            if not error[\"prediction\"].strip():\n                position_error_types[\"no_response\"] += 1\n            elif error[\"prediction\"].lower() == error[\"expected\"].lower():\n                position_error_types[\"wrong_case\"] += 1\n            elif error[\"word\"] and error[\"prediction\"] in error[\"word\"]:\n                position_error_types[\"adjacent_letter\"] += 1\n            else:\n                position_error_types[\"other\"] += 1\n        \n        # Analyze spelling errors\n        spelling_errors = [r for r in mode_results[\"spelling\"] if not r[\"correct\"]]\n        \n        # Categorize spelling errors\n        spelling_error_types = {\n            \"single_character_diff\": 0,\n            \"multiple_character_diff\": 0,\n            \"completely_wrong\": 0,\n            \"no_response\": 0\n        }\n        \n        for error in spelling_errors:\n            if not error[\"prediction\"].strip():\n                spelling_error_types[\"no_response\"] += 1\n            else:\n                distance = Levenshtein.distance(error[\"prediction\"].lower(), error[\"expected\"].lower())\n                if distance == 1:\n                    spelling_error_types[\"single_character_diff\"] += 1\n                elif distance <= 3:\n                    spelling_error_types[\"multiple_character_diff\"] += 1\n                else:\n                    spelling_error_types[\"completely_wrong\"] += 1\n        \n        # Analyze by word length\n        word_length_performance = {}\n        \n        for result in mode_results[\"letter_count\"] + mode_results[\"letter_position\"] + mode_results[\"spelling\"]:\n            word = result[\"word\"]\n            length = len(word)\n            task_type = \"position_count\" if result in mode_results[\"letter_count\"] + mode_results[\"letter_position\"] else \"spelling\"\n            \n            if length not in word_length_performance:\n                word_length_performance[length] = {\n                    \"spelling\": {\"correct\": 0, \"total\": 0},\n                    \"position_count\": {\"correct\": 0, \"total\": 0}\n                }\n            \n            word_length_performance[length][task_type][\"total\"] += 1\n            if result[\"correct\"]:\n                word_length_performance[length][task_type][\"correct\"] += 1\n        \n        # Calculate accuracy by word length\n        for length, stats in word_length_performance.items():\n            for task_type in [\"spelling\", \"position_count\"]:\n                stats[task_type][\"accuracy\"] = stats[task_type][\"correct\"] / stats[task_type][\"total\"] if stats[task_type][\"total\"] > 0 else 0\n        \n        error_analysis[mode_name] = {\n            \"letter_count_errors\": error_types,\n            \"letter_position_errors\": position_error_types,\n            \"spelling_errors\": spelling_error_types,\n            \"word_length_performance\": word_length_performance\n        }\n    \n    # Compare error patterns between thinking and non-thinking modes\n    if \"thinking\" in error_analysis and \"non_thinking\" in error_analysis:\n        error_analysis[\"mode_comparison\"] = {}\n        \n        # Compare letter count error distributions\n        thinking_count = error_analysis[\"thinking\"][\"letter_count_errors\"]\n        non_thinking_count = error_analysis[\"non_thinking\"][\"letter_count_errors\"]\n        \n        error_analysis[\"mode_comparison\"][\"letter_count\"] = {\n            error_type: thinking_count.get(error_type, 0) - non_thinking_count.get(error_type, 0)\n            for error_type in set(thinking_count.keys()) | set(non_thinking_count.keys())\n        }\n        \n        # Compare letter position error distributions\n        thinking_pos = error_analysis[\"thinking\"][\"letter_position_errors\"]\n        non_thinking_pos = error_analysis[\"non_thinking\"][\"letter_position_errors\"]\n        \n        error_analysis[\"mode_comparison\"][\"letter_position\"] = {\n            error_type: thinking_pos.get(error_type, 0) - non_thinking_pos.get(error_type, 0)\n            for error_type in set(thinking_pos.keys()) | set(non_thinking_pos.keys())\n        }\n        \n        # Compare spelling error distributions\n        thinking_spell = error_analysis[\"thinking\"][\"spelling_errors\"]\n        non_thinking_spell = error_analysis[\"non_thinking\"][\"spelling_errors\"]\n        \n        error_analysis[\"mode_comparison\"][\"spelling\"] = {\n            error_type: thinking_spell.get(error_type, 0) - non_thinking_spell.get(error_type, 0)\n            for error_type in set(thinking_spell.keys()) | set(non_thinking_spell.keys())\n        }\n    \n    return error_analysis\n\n# Create performance dashboard with mode comparison\ndef create_performance_dashboard(results, error_analysis, transfer_analysis):\n    # Set up figure\n    fig = plt.figure(figsize=(20, 16))\n    \n    # 1. Accuracy comparison across metrics and modes\n    ax1 = fig.add_subplot(3, 3, 1)\n    metrics = ['spelling_accuracy', 'letter_count_accuracy', 'letter_position_accuracy', 'character_level_accuracy']\n    \n    # Extract values for each mode\n    thinking_values = [results[\"thinking\"].get(m, 0) for m in metrics]\n    non_thinking_values = [results[\"non_thinking\"].get(m, 0) for m in metrics]\n    \n    x = np.arange(len(metrics))\n    width = 0.35\n    \n    ax1.bar(x - width/2, thinking_values, width, label='Thinking Mode')\n    ax1.bar(x + width/2, non_thinking_values, width, label='Non-Thinking Mode')\n    \n    ax1.set_ylabel('Accuracy')\n    ax1.set_title('Accuracy Comparison by Mode')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n    ax1.legend()\n    \n    # 2. Error analysis comparison for letter count\n    ax2 = fig.add_subplot(3, 3, 2)\n    thinking_errors = error_analysis[\"thinking\"][\"letter_count_errors\"]\n    non_thinking_errors = error_analysis[\"non_thinking\"][\"letter_count_errors\"]\n    \n    error_types = list(set(thinking_errors.keys()) | set(non_thinking_errors.keys()))\n    thinking_counts = [thinking_errors.get(t, 0) for t in error_types]\n    non_thinking_counts = [non_thinking_errors.get(t, 0) for t in error_types]\n    \n    x = np.arange(len(error_types))\n    width = 0.35\n    \n    ax2.bar(x - width/2, thinking_counts, width, label='Thinking Mode')\n    ax2.bar(x + width/2, non_thinking_counts, width, label='Non-Thinking Mode')\n    \n    ax2.set_title('Letter Count Error Types by Mode')\n    ax2.set_ylabel('Count')\n    ax2.set_xticks(x)\n    ax2.set_xticklabels(error_types)\n    ax2.legend()\n    plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\")\n    \n    # 3. Error analysis comparison for letter position\n    ax3 = fig.add_subplot(3, 3, 3)\n    thinking_pos_errors = error_analysis[\"thinking\"][\"letter_position_errors\"]\n    non_thinking_pos_errors = error_analysis[\"non_thinking\"][\"letter_position_errors\"]\n    \n    pos_error_types = list(set(thinking_pos_errors.keys()) | set(non_thinking_pos_errors.keys()))\n    thinking_pos_counts = [thinking_pos_errors.get(t, 0) for t in pos_error_types]\n    non_thinking_pos_counts = [non_thinking_pos_errors.get(t, 0) for t in pos_error_types]\n    \n    x = np.arange(len(pos_error_types))\n    width = 0.35\n    \n    ax3.bar(x - width/2, thinking_pos_counts, width, label='Thinking Mode')\n    ax3.bar(x + width/2, non_thinking_pos_counts, width, label='Non-Thinking Mode')\n    \n    ax3.set_title('Letter Position Error Types by Mode')\n    ax3.set_ylabel('Count')\n    ax3.set_xticks(x)\n    ax3.set_xticklabels(pos_error_types)\n    ax3.legend()\n    plt.setp(ax3.get_xticklabels(), rotation=45, ha=\"right\")\n    \n    # 4. Error analysis comparison for spelling\n    ax4 = fig.add_subplot(3, 3, 4)\n    thinking_spell_errors = error_analysis[\"thinking\"][\"spelling_errors\"]\n    non_thinking_spell_errors = error_analysis[\"non_thinking\"][\"spelling_errors\"]\n    \n    spell_error_types = list(set(thinking_spell_errors.keys()) | set(non_thinking_spell_errors.keys()))\n    thinking_spell_counts = [thinking_spell_errors.get(t, 0) for t in spell_error_types]\n    non_thinking_spell_counts = [non_thinking_spell_errors.get(t, 0) for t in spell_error_types]\n    \n    x = np.arange(len(spell_error_types))\n    width = 0.35\n    \n    ax4.bar(x - width/2, thinking_spell_counts, width, label='Thinking Mode')\n    ax4.bar(x + width/2, non_thinking_spell_counts, width, label='Non-Thinking Mode')\n    \n    ax4.set_title('Spelling Error Types by Mode')\n    ax4.set_ylabel('Count')\n    ax4.set_xticks(x)\n    ax4.set_xticklabels(spell_error_types)\n    ax4.legend()\n    plt.setp(ax4.get_xticklabels(), rotation=45, ha=\"right\")\n    \n    # 5. Performance by word length comparison\n    ax5 = fig.add_subplot(3, 3, 5)\n    thinking_length_perf = error_analysis[\"thinking\"][\"word_length_performance\"]\n    non_thinking_length_perf = error_analysis[\"non_thinking\"][\"word_length_performance\"]\n    \n    # Get all word lengths from both modes\n    all_lengths = sorted(set(thinking_length_perf.keys()) | set(non_thinking_length_perf.keys()))\n    \n    # Get spelling accuracy for each mode by word length\n    thinking_spell_acc = [thinking_length_perf.get(l, {}).get(\"spelling\", {}).get(\"accuracy\", 0) for l in all_lengths]\n    non_thinking_spell_acc = [non_thinking_length_perf.get(l, {}).get(\"spelling\", {}).get(\"accuracy\", 0) for l in all_lengths]\n    \n    ax5.plot(all_lengths, thinking_spell_acc, marker='o', label='Thinking Mode - Spelling')\n    ax5.plot(all_lengths, non_thinking_spell_acc, marker='s', label='Non-Thinking Mode - Spelling')\n    \n    # Get position/count accuracy for each mode by word length\n    thinking_pos_acc = [thinking_length_perf.get(l, {}).get(\"position_count\", {}).get(\"accuracy\", 0) for l in all_lengths]\n    non_thinking_pos_acc = [non_thinking_length_perf.get(l, {}).get(\"position_count\", {}).get(\"accuracy\", 0) for l in all_lengths]\n    \n    ax5.plot(all_lengths, thinking_pos_acc, marker='^', linestyle='--', label='Thinking Mode - Position/Count')\n    ax5.plot(all_lengths, non_thinking_pos_acc, marker='v', linestyle='--', label='Non-Thinking Mode - Position/Count')\n    \n    ax5.set_title('Performance by Word Length and Mode')\n    ax5.set_xlabel('Word Length')\n    ax5.set_ylabel('Accuracy')\n    ax5.legend()\n    \n    # 6. Transfer learning correlation comparison\n    ax6 = fig.add_subplot(3, 3, 6)\n    \n    # Extract correlation values for each mode\n    thinking_corr = transfer_analysis.get(\"thinking\", {}).get(\"overall_correlation\", {}).get(\"pearson\", 0)\n    non_thinking_corr = transfer_analysis.get(\"non_thinking\", {}).get(\"overall_correlation\", {}).get(\"pearson\", 0)\n    \n    ax6.bar([\"Thinking Mode\", \"Non-Thinking Mode\"], [thinking_corr, non_thinking_corr])\n    ax6.set_title('Transfer Learning Correlation by Mode')\n    ax6.set_ylabel('Pearson Correlation')\n    \n    # 7. Pattern performance comparison between modes\n    ax7 = fig.add_subplot(3, 3, 7)\n    \n    # Get patterns from both modes\n    thinking_patterns = transfer_analysis.get(\"thinking\", {}).get(\"pattern_performance\", {})\n    non_thinking_patterns = transfer_analysis.get(\"non_thinking\", {}).get(\"pattern_performance\", {})\n    \n    all_patterns = list(set(thinking_patterns.keys()) | set(non_thinking_patterns.keys()))\n    \n    if all_patterns:\n        # Get spelling performance for each pattern and mode\n        thinking_pattern_spell = [thinking_patterns.get(p, {}).get(\"avg_spelling\", 0) for p in all_patterns]\n        non_thinking_pattern_spell = [non_thinking_patterns.get(p, {}).get(\"avg_spelling\", 0) for p in all_patterns]\n        \n        # Get position performance for each pattern and mode\n        thinking_pattern_pos = [thinking_patterns.get(p, {}).get(\"avg_position\", 0) for p in all_patterns]\n        non_thinking_pattern_pos = [non_thinking_patterns.get(p, {}).get(\"avg_position\", 0) for p in all_patterns]\n        \n        # Set up grouped bar chart\n        x = np.arange(len(all_patterns))\n        width = 0.2\n        \n        ax7.bar(x - 1.5*width, thinking_pattern_spell, width, label='Thinking - Spelling')\n        ax7.bar(x - 0.5*width, non_thinking_pattern_spell, width, label='Non-Thinking - Spelling')\n        ax7.bar(x + 0.5*width, thinking_pattern_pos, width, label='Thinking - Position')\n        ax7.bar(x + 1.5*width, non_thinking_pattern_pos, width, label='Non-Thinking - Position')\n        \n        ax7.set_title('Pattern Performance by Mode')\n        ax7.set_xticks(x)\n        ax7.set_xticklabels(all_patterns)\n        ax7.set_ylabel('Accuracy')\n        ax7.legend()\n        plt.setp(ax7.get_xticklabels(), rotation=45, ha=\"right\")\n    else:\n        ax7.text(0.5, 0.5, 'No pattern data available', \n                horizontalalignment='center', verticalalignment='center')\n    \n    # 8. Levenshtein distance comparison by mode\n    ax8 = fig.add_subplot(3, 3, 8)\n    thinking_lev = results[\"thinking\"].get('levenshtein_distance', 0)\n    non_thinking_lev = results[\"non_thinking\"].get('levenshtein_distance', 0)\n    \n    ax8.bar(['Thinking Mode', 'Non-Thinking Mode'], [thinking_lev, non_thinking_lev])\n    ax8.set_title('Normalized Levenshtein Distance by Mode')\n    ax8.set_ylabel('Distance (lower is better)')\n    \n    # 9. Transfer success/failure counts by mode\n    ax9 = fig.add_subplot(3, 3, 9)\n    \n    thinking_success = transfer_analysis.get(\"thinking\", {}).get(\"successful_transfer\", {}).get(\"count\", 0)\n    thinking_failure = transfer_analysis.get(\"thinking\", {}).get(\"unsuccessful_transfer\", {}).get(\"count\", 0)\n    non_thinking_success = transfer_analysis.get(\"non_thinking\", {}).get(\"successful_transfer\", {}).get(\"count\", 0)\n    non_thinking_failure = transfer_analysis.get(\"non_thinking\", {}).get(\"unsuccessful_transfer\", {}).get(\"count\", 0)\n    \n    x = np.arange(2)\n    width = 0.35\n    \n    ax9.bar(x - width/2, [thinking_success, thinking_failure], width, label='Thinking Mode')\n    ax9.bar(x + width/2, [non_thinking_success, non_thinking_failure], width, label='Non-Thinking Mode')\n    \n    ax9.set_title('Transfer Learning Success/Failure by Mode')\n    ax9.set_ylabel('Count')\n    ax9.set_xticks(x)\n    ax9.set_xticklabels(['Successful Transfer', 'Unsuccessful Transfer'])\n    ax9.legend()\n    \n    plt.tight_layout()\n    plt.savefig('results/evaluation/plots/qwen3_mode_comparison_dashboard.png', dpi=300)\n    \n    return fig\n\n# Analyze English token subset effectiveness\ndef analyze_english_token_subset(english_results, all_results):\n    token_analysis = {}\n    \n    # Compare metrics between English-only and all tokens\n    metrics = ['spelling_accuracy', 'letter_count_accuracy', 'letter_position_accuracy', 'character_level_accuracy', 'levenshtein_distance']\n    \n    for mode in ['thinking', 'non_thinking']:\n        token_analysis[mode] = {}\n        \n        for metric in metrics:\n            english_value = english_results[mode].get(metric, 0)\n            all_value = all_results[mode].get(metric, 0)\n            \n            # For Levenshtein, lower is better\n            if metric == 'levenshtein_distance':\n                improvement = all_value - english_value\n            else:\n                improvement = english_value - all_value\n                \n            token_analysis[mode][metric] = {\n                'english_only': english_value,\n                'all_tokens': all_value,\n                'improvement': improvement,\n                'percent_change': (improvement / all_value * 100) if all_value != 0 else 0\n            }\n    \n    # Analyze which task types benefit most from English-only tokens\n    task_benefit = {}\n    \n    for mode in ['thinking', 'non_thinking']:\n        task_benefit[mode] = {\n            'spelling': english_results[mode].get('spelling_accuracy', 0) - all_results[mode].get('spelling_accuracy', 0),\n            'letter_count': english_results[mode].get('letter_count_accuracy', 0) - all_results[mode].get('letter_count_accuracy', 0),\n            'letter_position': english_results[mode].get('letter_position_accuracy', 0) - all_results[mode].get('letter_position_accuracy', 0)\n        }\n        \n        # Determine which task benefits most\n        max_benefit_task = max(task_benefit[mode].items(), key=lambda x: x[1])[0]\n        task_benefit[mode]['most_improved_task'] = max_benefit_task\n    \n    token_analysis['task_benefit'] = task_benefit\n    \n    return token_analysis\n\n# Main evaluation function\ndef evaluate_models(model_path):\n    # Initialize W&B\n    wandb.init(project=\"llm-spelling-finetuning\", name=\"qwen3_mode_evaluation\")\n    \n    # Load test dataset\n    test_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\", split=\"test\")\n    # Alternative: load from local file\n    # with open('data/splits/test.json', 'r') as f:\n    #     test_dataset = json.load(f)\n    \n    # Load model and tokenizer\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    # Evaluate with different modes and token settings\n    results = {}\n    detailed_results = {}\n    \n    # Mode combinations to evaluate\n    configs = [\n        {\"thinking_mode\": True, \"english_only\": False},\n        {\"thinking_mode\": False, \"english_only\": False},\n        {\"thinking_mode\": True, \"english_only\": True},\n        {\"thinking_mode\": False, \"english_only\": True}\n    ]\n    \n    for config in configs:\n        thinking = config[\"thinking_mode\"]\n        english_only = config[\"english_only\"]\n        \n        mode_key = \"thinking\" if thinking else \"non_thinking\"\n        token_key = \"english_only\" if english_only else \"all_tokens\"\n        \n        # Create nested dictionaries if they don't exist\n        if mode_key not in results:\n            results[mode_key] = {}\n        \n        if token_key not in results[mode_key]:\n            results[mode_key][token_key] = {}\n        \n        if mode_key not in detailed_results:\n            detailed_results[mode_key] = {}\n            \n        if token_key not in detailed_results[mode_key]:\n            detailed_results[mode_key][token_key] = {}\n        \n        print(f\"\\nEvaluating with thinking_mode={thinking}, english_only={english_only}...\")\n        \n        # Run evaluations with current configuration\n        spelling_acc, spelling_results = calc_spelling_accuracy(model, tokenizer, test_dataset, thinking, english_only)\n        letter_count_acc, count_results = calc_letter_count_accuracy(model, tokenizer, test_dataset, thinking, english_only)\n        letter_position_acc, position_results = calc_letter_position_accuracy(model, tokenizer, test_dataset, thinking, english_only)\n        char_acc, char_results = calc_character_level_accuracy(model, tokenizer, test_dataset, thinking, english_only)\n        lev_dist, lev_results = calc_levenshtein_metrics(model, tokenizer, test_dataset, thinking, english_only)\n        \n        # Store results\n        results[mode_key][token_key] = {\n            \"spelling_accuracy\": spelling_acc,\n            \"letter_count_accuracy\": letter_count_acc,\n            \"letter_position_accuracy\": letter_position_acc,\n            \"character_level_accuracy\": char_acc,\n            \"levenshtein_distance\": lev_dist\n        }\n        \n        detailed_results[mode_key][token_key] = {\n            \"spelling\": spelling_results,\n            \"letter_count\": count_results,\n            \"letter_position\": position_results,\n            \"character_level\": char_results,\n            \"levenshtein\": lev_results\n        }\n    \n    # Flatten results for easier access in analysis functions\n    flattened_results = {}\n    flattened_detailed_results = {}\n    \n    for mode_key in results:\n        flattened_results[mode_key] = results[mode_key][\"all_tokens\"]\n        flattened_detailed_results[mode_key] = detailed_results[mode_key][\"all_tokens\"]\n    \n    # Perform error analysis\n    error_analysis = perform_error_analysis(flattened_detailed_results)\n    \n    # Analyze transfer learning effectiveness\n    transfer_analysis = {}\n    for mode_key in flattened_detailed_results:\n        transfer_result, transfer_data = analyze_transfer_learning(\n            flattened_detailed_results[mode_key][\"spelling\"],\n            flattened_detailed_results[mode_key][\"letter_count\"] + flattened_detailed_results[mode_key][\"letter_position\"]\n        )\n        transfer_analysis[mode_key] = transfer_result\n    \n    # Analyze English token subset effectiveness\n    english_only_results = {}\n    for mode_key in results:\n        english_only_results[mode_key] = results[mode_key][\"english_only\"]\n    \n    token_analysis = analyze_english_token_subset(english_only_results, flattened_results)\n    \n    # Create performance dashboard\n    dashboard = create_performance_dashboard(flattened_results, error_analysis, transfer_analysis)\n    \n    # Log results to W&B\n    wandb.log({\n        \"results\": results,\n        \"error_analysis\": error_analysis,\n        \"transfer_learning\": transfer_analysis,\n        \"token_analysis\": token_analysis,\n        \"evaluation_dashboard\": wandb.Image(dashboard)\n    })\n    \n    # Save results locally\n    evaluation_results = {\n        \"results\": results,\n        \"detailed_results\": detailed_results,\n        \"error_analysis\": error_analysis,\n        \"transfer_learning\": transfer_analysis,\n        \"token_analysis\": token_analysis\n    }\n    \n    # Ensure directories exist\n    import os\n    os.makedirs('results/evaluation/metrics', exist_ok=True)\n    os.makedirs('results/evaluation/error_analysis', exist_ok=True)\n    os.makedirs('results/evaluation/transfer_analysis', exist_ok=True)\n    os.makedirs('results/evaluation/mode_comparison', exist_ok=True)\n    os.makedirs('results/evaluation/token_analysis', exist_ok=True)\n    os.makedirs('results/evaluation/plots', exist_ok=True)\n    \n    # Save results to appropriate locations\n    with open(\"results/evaluation/metrics/qwen3_evaluation_results.json\", \"w\") as f:\n        json.dump(evaluation_results, f, indent=2)\n    \n    with open(\"results/evaluation/error_analysis/qwen3_error_patterns.json\", \"w\") as f:\n        json.dump(error_analysis, f, indent=2)\n        \n    with open(\"results/evaluation/transfer_analysis/qwen3_transfer_learning_analysis.json\", \"w\") as f:\n        json.dump(transfer_analysis, f, indent=2)\n        \n    with open(\"results/evaluation/mode_comparison/qwen3_mode_comparison.json\", \"w\") as f:\n        json.dump({\"results\": flattened_results, \"error_analysis\": error_analysis, \"transfer_analysis\": transfer_analysis}, f, indent=2)\n        \n    with open(\"results/evaluation/token_analysis/qwen3_token_analysis.json\", \"w\") as f:\n        json.dump(token_analysis, f, indent=2)\n    \n    # Close W&B\n    wandb.finish()\n    \n    return evaluation_results\n```",
      "testStrategy": "1. Verify all evaluation metrics are calculated correctly for both spelling and position/count tasks\n2. Test both thinking and non-thinking modes of Qwen3-4B to ensure proper mode switching\n3. Validate English-only token subset filtering works correctly\n4. Test with Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n5. Confirm error analysis provides meaningful insights for both task types and both modes\n6. Validate transfer learning analysis metrics and correlations between modes\n7. Check that visualizations clearly show the relationship between spelling and position/count performance for each mode\n8. Verify results are properly logged to W&B with mode-specific metrics\n9. Confirm performance comparison between thinking and non-thinking modes across all task types\n10. Test with different model checkpoints to ensure consistent evaluation\n11. Verify final evaluation results are saved locally in the correct directories:\n   - `results/evaluation/metrics/`\n   - `results/evaluation/error_analysis/`\n   - `results/evaluation/transfer_analysis/`\n   - `results/evaluation/mode_comparison/`\n   - `results/evaluation/token_analysis/`\n   - `results/evaluation/plots/`\n12. Test GPU switching functionality in Lightning.AI Studio (CPU → T4 → A100) for optimal resource usage\n13. Verify that all test datasets are properly loaded from `data/splits/test.json` and challenge sets\n14. Check that the evaluation notebooks can successfully load and analyze the mode comparison results\n15. Validate the pattern analysis to ensure it correctly identifies which spelling patterns lead to better position/count performance in each mode\n16. Test Lightning.AI Studio's job system for automated evaluation pipeline execution\n17. Verify proper environment isolation and dependency management in the Lightning.AI Studio\n18. Test the token analysis to ensure it correctly measures the effectiveness of the English token filtering approach",
      "subtasks": [
        {
          "id": 1,
          "title": "Multi-metric evaluation framework implementation",
          "description": "Develop and implement a comprehensive evaluation framework with multiple metrics to assess model performance",
          "dependencies": [],
          "details": "Define evaluation goals and success metrics for the model assessment. Select appropriate metrics covering accuracy, precision, recall, F1-score, latency, and domain-specific measures. Create standardized test datasets with diverse examples. Implement automated evaluation pipelines that can process model outputs against ground truth. Establish baseline performance thresholds for each metric.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Base vs. fine-tuned model comparison",
          "description": "Conduct systematic comparison between base models and their fine-tuned versions across all defined metrics",
          "dependencies": [
            1
          ],
          "details": "Design controlled experiments to compare base and fine-tuned models. Ensure identical test conditions and datasets for fair comparison. Measure performance improvements across all metrics defined in subtask 1. Analyze trade-offs between different aspects of performance (e.g., accuracy vs. latency). Document specific improvements attributable to fine-tuning techniques. Identify areas where fine-tuning provided the most significant gains.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Detailed error analysis system",
          "description": "Create a system to categorize, analyze and track different types of model errors",
          "dependencies": [
            1,
            2
          ],
          "details": "Develop error taxonomy specific to the model's domain and tasks. Implement automated error classification system. Perform qualitative analysis of error patterns and edge cases. Create error frequency distribution reports. Identify correlations between specific input characteristics and error types. Develop recommendations for targeted model improvements based on error patterns.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Performance visualization dashboard",
          "description": "Design and implement an interactive dashboard to visualize model performance metrics and comparisons",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Select appropriate visualization types for different metrics and comparisons. Implement interactive features allowing drill-down into specific performance aspects. Create side-by-side visualizations of base vs. fine-tuned model performance. Design time-series views to track performance changes across model iterations. Ensure visualizations are accessible and interpretable for both technical and non-technical stakeholders.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Evaluation report generation",
          "description": "Create comprehensive evaluation reports documenting findings, methodologies, and recommendations",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Develop standardized report templates covering all evaluation aspects. Document evaluation methodology, metrics, and test datasets. Summarize key performance findings and improvements. Include detailed error analysis with examples. Provide actionable recommendations for further model improvements. Create executive summary for non-technical stakeholders. Ensure reports include all relevant visualizations from the dashboard.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Lightning.AI Studio setup for model evaluation",
          "description": "Configure and set up a dedicated Lightning.AI Studio for model evaluation with GPU switching capabilities",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated evaluation Studio following the \"one Studio, one task\" principle. Configure the Studio with necessary dependencies for model evaluation. Set up GPU switching capabilities (CPU → T4 → A100) for cost-effective resource usage. Install and configure Lightning.AI plugins for experiment tracking and visualization. Set up shared filesystem access for models and datasets. Implement proper environment isolation and dependency management. Create documentation for the Lightning.AI Studio setup and usage. Test the setup with sample models to verify functionality.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "File structure implementation",
          "description": "Set up the file structure for evaluation components and results according to the project organization",
          "dependencies": [],
          "details": "Create the following directory structure:\n- `src/evaluation/` for evaluation code modules\n- `data/splits/` for test datasets and challenge sets\n- `results/evaluation/` for storing evaluation outputs\n- `notebooks/` for analysis notebooks\n- `docs/` for evaluation documentation\n\nEnsure all evaluation code properly uses these paths for loading data and saving results. Update existing code to use the standardized file paths. Create placeholder files and documentation templates as needed.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Analysis notebooks development",
          "description": "Create Jupyter notebooks for analyzing evaluation results and visualizing model performance",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Develop the following notebooks:\n- `notebooks/evaluation_analysis.ipynb`: General analysis of evaluation results\n- `notebooks/error_patterns.ipynb`: Detailed analysis of error patterns and categories\n- `notebooks/model_comparison.ipynb`: Comparative analysis between base and fine-tuned models\n\nEnsure notebooks can load results from the standardized file locations. Implement interactive visualizations and filtering capabilities. Add markdown documentation explaining the analysis methodology and interpretation of results.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Transfer learning analysis implementation",
          "description": "Develop and implement analysis tools to measure transfer learning effectiveness between spelling and position/count tasks",
          "dependencies": [
            1,
            3
          ],
          "details": "Create a transfer learning analysis module in `src/evaluation/transfer_analysis.py`. Implement correlation metrics between spelling performance and position/count task performance. Develop methods to identify which spelling patterns lead to better position/count performance. Create visualizations showing the relationship between spelling ability and position/count task success. Implement statistical tests to validate transfer learning effectiveness. Design analysis tools to identify successful and unsuccessful transfer cases.\n\nEnsure results are saved to `results/evaluation/transfer_analysis/` directory and properly visualized in the dashboard.",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "Separate evaluation pipelines for task types",
          "description": "Implement separate but coordinated evaluation pipelines for spelling tasks and position/count tasks",
          "dependencies": [
            1
          ],
          "details": "Refactor the evaluation framework to handle spelling tasks and position/count tasks separately. Ensure metrics are calculated appropriately for each task type. Implement task-specific error analysis for each pipeline. Create mechanisms to track and correlate performance between the two task types. Design the system to identify which aspects of spelling knowledge transfer to position/count tasks. Ensure all results can be combined for comprehensive reporting and visualization.\n\nUpdate the main evaluator in `src/evaluation/evaluator.py` to coordinate between the separate pipelines.",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "Lightning.AI automated evaluation pipeline",
          "description": "Set up automated evaluation pipelines using Lightning.AI's job system",
          "dependencies": [
            6,
            10
          ],
          "details": "Configure Lightning.AI's job system for automated model evaluation. Create job templates for different evaluation scenarios. Set up job dependencies to ensure proper execution order. Implement resource optimization to use appropriate GPU tiers for different evaluation stages. Configure job notifications and alerts. Create a dashboard to monitor job status and results. Document the job system setup and usage. Test the automated pipeline with sample models.",
          "status": "pending"
        },
        {
          "id": 12,
          "title": "Cost optimization for Lightning.AI resources",
          "description": "Implement cost optimization strategies for efficient resource usage in Lightning.AI",
          "dependencies": [
            6
          ],
          "details": "Analyze resource requirements for different evaluation stages. Implement automatic GPU switching based on computational needs. Set up resource monitoring and usage alerts. Configure automatic shutdown of idle resources. Implement batch processing for efficient resource utilization. Create cost estimation tools for evaluation runs. Document cost optimization strategies and best practices. Test and validate cost savings through optimized resource usage.",
          "status": "pending"
        },
        {
          "id": 13,
          "title": "Lightning.AI evaluation documentation",
          "description": "Create comprehensive documentation for the Lightning.AI evaluation setup and usage",
          "dependencies": [
            6,
            11,
            12
          ],
          "details": "Document the Lightning.AI Studio setup and configuration. Create user guides for running evaluations in the Studio. Document GPU switching functionality and best practices. Create troubleshooting guides for common issues. Document cost optimization strategies and resource management. Create onboarding materials for new team members. Include examples and tutorials for different evaluation scenarios. Maintain documentation with updates for new Lightning.AI features.",
          "status": "pending"
        },
        {
          "id": 14,
          "title": "Qwen3 thinking mode evaluation",
          "description": "Implement evaluation pipeline for Qwen3-4B's thinking and non-thinking modes",
          "dependencies": [
            1,
            10
          ],
          "details": "Extend the evaluation framework to support Qwen3-4B's thinking and non-thinking modes. Create mode-specific evaluation configurations. Implement mode switching in the generation pipeline. Develop metrics to compare performance between modes. Create visualizations showing mode-specific performance differences. Analyze how thinking mode affects transfer learning capabilities. Identify tasks that benefit most from each mode. Document findings and recommendations for mode selection based on task type.",
          "status": "pending"
        },
        {
          "id": 15,
          "title": "English token subset analysis",
          "description": "Implement analysis tools to evaluate performance with English-only token subset",
          "dependencies": [
            1,
            14
          ],
          "details": "Develop token filtering mechanism for Qwen3-4B to restrict generation to English-only tokens. Create analysis tools to compare performance between full vocabulary and English-only subset. Implement metrics to measure the effectiveness of token filtering. Analyze which tasks benefit most from English token filtering. Create visualizations showing the impact of token filtering on different metrics. Document findings and recommendations for token filtering usage. Ensure proper handling of Qwen3's tokenizer patterns in the filtering process.",
          "status": "pending"
        },
        {
          "id": 16,
          "title": "Qwen3 sampling parameters analysis",
          "description": "Analyze the impact of Qwen3's specific sampling parameters on performance",
          "dependencies": [
            1,
            14
          ],
          "details": "Implement evaluation with Qwen3's recommended sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0). Create comparison framework to test different parameter combinations. Analyze how sampling parameters affect performance on different task types. Identify optimal parameter settings for spelling and position/count tasks. Create visualizations showing parameter sensitivity. Document findings and recommendations for parameter tuning. Ensure proper implementation of MinP parameter if supported by the model.",
          "status": "pending"
        },
        {
          "id": 17,
          "title": "Mode comparison dashboard",
          "description": "Create interactive dashboard for comparing thinking and non-thinking mode performance",
          "dependencies": [
            4,
            14
          ],
          "details": "Design and implement a dedicated dashboard for mode comparison. Create side-by-side visualizations of thinking vs. non-thinking mode performance. Implement filtering capabilities to focus on specific metrics or task types. Develop interactive features for exploring mode-specific error patterns. Create visualizations showing transfer learning differences between modes. Ensure the dashboard is accessible and interpretable for both technical and non-technical stakeholders. Save mode comparison visualizations to `results/evaluation/plots/` directory.",
          "status": "pending"
        },
        {
          "id": 18,
          "title": "Qwen3 tokenizer pattern analysis",
          "description": "Analyze and handle Qwen3's specific tokenizer patterns in evaluation",
          "dependencies": [
            1,
            14,
            15
          ],
          "details": "Investigate Qwen3's tokenizer patterns and their impact on evaluation. Develop tools to analyze token distributions in model outputs. Implement proper handling of Qwen3's tokenizer patterns in evaluation metrics. Create visualizations showing token usage patterns. Analyze how tokenizer patterns affect performance on different task types. Document findings and recommendations for handling tokenizer-specific issues. Ensure evaluation metrics properly account for tokenizer characteristics.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 10,
      "title": "Model Publishing and Documentation",
      "description": "Prepare the final model, documentation, and publish to Hugging Face with comprehensive model card, focusing on both spelling training and position/count evaluation capabilities. Include Qwen3-4B specific documentation requirements.",
      "status": "pending",
      "dependencies": [
        9
      ],
      "priority": "medium",
      "details": "1. Prepare model card documentation highlighting transfer learning approach and Qwen3-4B specifics\n2. Create detailed README for the project with both spelling and position/count evaluation, including Qwen3-4B features\n3. Upload the best model to Hugging Face\n4. Ensure dataset is properly published\n5. Create final report with results and findings, emphasizing transfer learning metrics and Qwen3-4B characteristics\n6. Organize documentation in the specified file structure\n7. Document separate endpoints for spelling training and position/count evaluation\n8. Document Lightning.AI Studios setup and usage for model publishing and evaluation\n9. Document Qwen3's thinking/non-thinking modes and their impact on performance\n10. Explain the English-only token subset approach and its benefits\n11. Detail Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n12. Document mode-specific performance characteristics\n13. Explain token filtering methodology and implementation\n14. Provide guidelines for mode selection based on task type\n15. Document transfer learning differences between modes\n16. Include visualizations of mode comparison results\n17. Explain tokenizer pattern handling and considerations\n18. Create comprehensive guides for reproducing experiments with Qwen3-4B\n19. Document Lightning.AI Studio configuration for Qwen3 evaluation\n\nNOTE: If publishing Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local publishing is only for CPU-compatible models.\n\nFile Structure:\n- API docs: `docs/api.md`\n- Deployment guide: `docs/deployment.md`\n- Monitoring guide: `docs/monitoring.md`\n- Lightning.AI Studios guide: `docs/lightning_studios.md`\n- Qwen3-4B guide: `docs/qwen3_4b_guide.md`\n- Experiment reproduction guide: `docs/experiment_reproduction.md`",
      "testStrategy": "1. Verify model card is comprehensive and follows Hugging Face guidelines, with clear transfer learning focus and Qwen3-4B specifics\n2. Confirm README provides clear instructions for using the model for both spelling and position/count tasks, including Qwen3-4B features\n3. Test model upload to Hugging Face\n4. Verify dataset is properly published and accessible\n5. Check that final report includes all required information, especially transfer learning metrics and Qwen3-4B characteristics\n6. Test model loading from Hugging Face\n7. Verify all success criteria from the PRD are met and documented\n8. For Unsloth or GPU-fine-tuned models, verify the publishing process works in a cloud GPU environment\n9. Validate that all documentation files are created in the correct locations\n10. Test API documentation against actual API implementation, ensuring both spelling and position/count endpoints work\n11. Verify deployment instructions work in both Docker and Kubernetes environments, with proper GPU support\n12. Test monitoring setup with Prometheus and Grafana, focusing on transfer learning metrics\n13. Ensure all file paths in documentation match the actual project structure\n14. Test both spelling and position/count endpoints for performance and accuracy\n15. Verify transfer learning metrics are properly tracked and visualized\n16. Test Lightning.AI Studios setup and configuration according to documentation\n17. Verify GPU switching functionality in Lightning.AI Studios\n18. Test shared filesystem operations for data management\n19. Validate environment isolation and dependency management in Studios\n20. Test cost optimization strategies and automatic shutdown functionality\n21. Verify integration with the evaluation framework in Lightning.AI Studios\n22. Test model publishing to Hugging Face from Lightning.AI Studios\n23. Validate troubleshooting procedures for common issues\n24. Test onboarding process for new team members using the documentation\n25. Verify documentation of Qwen3's thinking/non-thinking modes and their performance impact\n26. Test English-only token subset approach and validate its benefits\n27. Verify Qwen3's sampling parameters are correctly documented and implemented\n28. Test mode-specific performance characteristics\n29. Validate token filtering methodology and implementation\n30. Test mode selection guidelines for different task types\n31. Verify documentation of transfer learning differences between modes\n32. Check visualizations of mode comparison results for clarity and accuracy\n33. Test tokenizer pattern handling and verify documentation accuracy\n34. Verify experiment reproduction guides can be followed successfully\n35. Test Lightning.AI Studio configuration specifically for Qwen3 evaluation\n36. Validate that all Qwen3-4B specific documentation is accurate and comprehensive",
      "subtasks": [
        {
          "id": 1,
          "title": "Model Card Creation",
          "description": "Create a comprehensive model card following Hugging Face guidelines with metadata and detailed sections",
          "dependencies": [],
          "details": "Develop a model card as a Markdown file with YAML metadata section. Include: model description, intended uses & limitations, training parameters, datasets used, evaluation results, biases and ethical considerations. Follow the structure from Mitchell, 2018 paper and use the Hugging Face template. Ensure all metadata supports discovery (license, datasets, language identifiers). Include Qwen3-4B specific information such as thinking/non-thinking modes, English-only token subset, and sampling parameters.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Project README and Documentation",
          "description": "Prepare comprehensive project documentation including installation, usage examples, and technical details",
          "dependencies": [],
          "details": "Create a detailed README.md for the project repository (separate from the model card). Include: project overview, installation instructions, dependency requirements, usage examples with code snippets, architecture diagrams, limitations, and acknowledgments. Document the preprocessing and postprocessing steps to ensure reproducibility. Add inline code comments and generate API documentation if applicable. Include Qwen3-4B specific sections on mode selection, token filtering, and performance characteristics.\n\nOrganize documentation in the specified file structure:\n- API docs: `docs/api.md`\n- Deployment guide: `docs/deployment.md`\n- Monitoring guide: `docs/monitoring.md`\n- Lightning.AI Studios guide: `docs/lightning_studios.md`\n- Qwen3-4B guide: `docs/qwen3_4b_guide.md`\n- Experiment reproduction guide: `docs/experiment_reproduction.md`",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Hugging Face Model Publishing and Verification",
          "description": "Publish the model to Hugging Face Hub and verify its functionality",
          "dependencies": [
            1
          ],
          "details": "Use the huggingface_hub library to upload the model, tokenizer, and model card. Configure model tags, set appropriate visibility settings, and verify the model card renders correctly. Test the uploaded model with sample inference code to ensure it works as expected. Validate that all metadata is correctly displayed on the model page and that links to datasets are functional. Include Qwen3-4B specific tags and metadata.\n\nNOTE: If publishing Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local publishing is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Final Report Generation",
          "description": "Create a comprehensive report summarizing the model development, performance, and publication process",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Generate a final report documenting the entire model development lifecycle. Include: executive summary, methodology, training process details, evaluation metrics with visualizations, comparison to baseline models, limitations discovered during testing, deployment considerations, and future improvement recommendations. Format as a professional document with proper citations and appendices for detailed results. Include specific sections on Qwen3-4B features, mode comparisons, and transfer learning differences between modes.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Cloud Environment Setup for Model Publishing",
          "description": "Configure cloud GPU environment for publishing Unsloth or GPU-fine-tuned models",
          "dependencies": [],
          "details": "Set up a cloud GPU environment (Google Colab or https://lightning.ai/lars/home) for publishing models that require GPU resources. Create a notebook or script that handles authentication with Hugging Face, loads the model from local storage or cloud storage, and publishes it to the Hugging Face Hub. Include clear instructions for users on how to use this environment for model publishing. Test the workflow to ensure it works seamlessly with Unsloth-optimized models.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "API Documentation Creation",
          "description": "Create detailed API documentation for model inference endpoints",
          "dependencies": [
            2
          ],
          "details": "Develop comprehensive API documentation in `docs/api.md` that includes:\n- Endpoint descriptions and usage examples\n- Request/response formats with JSON examples\n- Authentication requirements\n- Error handling and status codes\n- Rate limiting information\n- Performance considerations\n- Qwen3-4B specific endpoints and parameters\n\nEnsure documentation matches the actual implementation in `src/api/app.py` and `src/api/routes/`.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Deployment Documentation",
          "description": "Create deployment guide for Docker and Kubernetes environments",
          "dependencies": [
            2
          ],
          "details": "Develop a detailed deployment guide in `docs/deployment.md` covering:\n- Docker deployment instructions\n- Kubernetes deployment configuration\n- Environment variable configuration\n- Resource requirements and scaling recommendations\n- Security considerations\n- Qwen3-4B specific deployment considerations\n\nReference the actual configuration files in `deployment/docker-compose.yml` and `deployment/k8s/`.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Monitoring Documentation",
          "description": "Create monitoring guide for Prometheus and Grafana setup",
          "dependencies": [
            2
          ],
          "details": "Develop a monitoring guide in `docs/monitoring.md` that includes:\n- Prometheus configuration for metrics collection\n- Grafana dashboard setup and import instructions\n- Alert configuration with Alertmanager\n- Log collection and analysis recommendations\n- Performance monitoring best practices\n- Qwen3-4B specific metrics and monitoring considerations\n\nReference the actual configuration files in `deployment/monitoring/`.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Transfer Learning Documentation and Metrics",
          "description": "Document the transfer learning approach and implement metrics tracking",
          "dependencies": [
            1,
            2,
            6,
            8
          ],
          "details": "Create comprehensive documentation on the transfer learning approach used in the project:\n- Update model card to highlight transfer learning aspects\n- Document correlation between spelling performance and position/count performance\n- Create visualization tools for transfer learning metrics\n- Implement monitoring for transfer learning effectiveness in production\n- Add transfer learning analysis to the final report\n- Document transfer learning differences between Qwen3-4B modes\n\nEnsure all documentation clearly explains how training on spelling tasks transfers to position/count tasks, including mode-specific considerations.",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "Dual-Task API Implementation Documentation",
          "description": "Document the implementation of separate endpoints for spelling and position/count tasks",
          "dependencies": [
            6
          ],
          "details": "Create detailed documentation for the dual-task API implementation:\n- Document the separate endpoints for spelling training and position/count evaluation\n- Provide examples for both task types\n- Explain how to configure the API for different task types\n- Document error handling specific to each task type\n- Include performance considerations for both tasks\n- Document Qwen3-4B mode selection guidelines for each task type\n\nEnsure the documentation covers how to efficiently use both capabilities of the model, including mode-specific optimizations.",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "Lightning.AI Studios Documentation",
          "description": "Create comprehensive documentation for Lightning.AI Studios setup and usage",
          "dependencies": [
            2,
            5
          ],
          "details": "Develop detailed documentation in `docs/lightning_studios.md` covering:\n- Account setup and authentication\n- Studio creation and management\n- \"One Studio, one task\" principle implementation\n- GPU configuration and switching options\n- Shared filesystem usage and data management\n- Environment isolation and dependency management\n- Cost optimization guidelines\n- Integration with the evaluation framework\n- Model publishing workflow\n- Troubleshooting and best practices\n- Onboarding process for new team members\n- Qwen3-4B specific considerations for Lightning.AI Studios\n- Specific configuration requirements for Qwen3 evaluation\n\nInclude practical examples and commands for each section, with clear explanations of the benefits and trade-offs of different approaches.",
          "status": "pending"
        },
        {
          "id": 12,
          "title": "Qwen3-4B Specific Documentation",
          "description": "Create comprehensive documentation for Qwen3-4B specific features and considerations",
          "dependencies": [
            1,
            2,
            6,
            9,
            10
          ],
          "details": "Develop detailed documentation in `docs/qwen3_4b_guide.md` covering:\n- Thinking/non-thinking modes and their impact on performance\n- English-only token subset approach and its benefits\n- Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n- Mode-specific performance characteristics\n- Token filtering methodology and implementation\n- Guidelines for mode selection based on task type\n- Transfer learning differences between modes\n- Visualizations of mode comparison results\n- Tokenizer pattern handling and considerations\n\nEnsure the documentation provides clear guidance on when and how to use different Qwen3-4B features for optimal performance in various scenarios.",
          "status": "pending"
        },
        {
          "id": 13,
          "title": "Experiment Reproduction Guide",
          "description": "Create comprehensive guides for reproducing experiments with Qwen3-4B",
          "dependencies": [
            1,
            2,
            11,
            12
          ],
          "details": "Develop detailed documentation in `docs/experiment_reproduction.md` covering:\n- Step-by-step instructions for reproducing all experiments\n- Environment setup requirements specific to Qwen3-4B\n- Data preparation and preprocessing steps\n- Training command examples with all parameters\n- Evaluation procedures and metrics calculation\n- Expected results and performance benchmarks\n- Troubleshooting common issues\n- Hardware requirements and optimization tips\n- Time and resource estimates for each experiment\n\nEnsure the guide is comprehensive enough that a new researcher could reproduce all experiments exactly as performed in the original work.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 11,
      "title": "Lightning.AI Studio Migration Planning and Setup",
      "description": "Plan and implement the migration of the evaluation framework to Lightning.AI Studios, leveraging features like isolated environments, GPU switching, and the plugin system. This includes initial Studio setup, environment configuration, and structuring components for data preparation, model training, and evaluation.",
      "status": "pending",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "Begin by reviewing Lightning.AI Studio documentation to understand its persistent cloud environment, isolated workspace capabilities, GPU management, and plugin architecture. Design a migration plan that maps each component of the current evaluation framework (data preparation, model training, evaluation, etc.) to a corresponding Studio workspace or plugin. Set up a new Studio instance, configure the environment with all necessary dependencies (ensuring compatibility with the existing Python setup from Task 1), and enable GPU switching as required. Establish isolated environments for each component to ensure modularity and reproducibility. Integrate Lightning.AI plugins where beneficial, and document the Studio structure, environment variables, and resource allocation strategies. Ensure the setup supports iterative development and easy scaling for future needs.",
      "testStrategy": "Verify that the Studio instance is accessible and persistent, with all required dependencies installed and functioning. Confirm that each component (data prep, model training, evaluation) runs in its own isolated environment and can access GPU resources as configured. Test the plugin system by integrating at least one relevant plugin and ensuring it operates as expected. Validate that the Studio structure supports modular workflows and that documentation is clear and complete. Run end-to-end tests for each workflow to ensure successful execution and reproducibility within the Studio environment.",
      "subtasks": [
        {
          "id": 1,
          "title": "Lightning.AI Studio Documentation Review and Migration Planning",
          "description": "Thoroughly review Lightning.AI documentation to understand Studio capabilities and create a comprehensive migration plan for the evaluation framework.",
          "dependencies": [],
          "details": "Review Lightning.AI documentation focusing on persistent cloud environments, isolated workspaces, GPU management, and plugin architecture. Create a detailed migration plan mapping each component of the current evaluation framework to appropriate Studio workspaces following the 'one Studio, one task' principle. Document how data preparation, model training, evaluation, and deployment components will be structured across separate Studios. Include considerations for resource allocation, environment variables, and secrets management.",
          "status": "pending",
          "testStrategy": "Create a comprehensive document outlining the migration plan with clear mappings between current framework components and planned Lightning.AI Studio structure."
        },
        {
          "id": 2,
          "title": "Data Preparation Studio Setup and Configuration",
          "description": "Set up and configure a dedicated Lightning.AI Studio for data preparation tasks with appropriate environment and dependencies.",
          "dependencies": [
            1
          ],
          "details": "Create a new Lightning.AI Studio dedicated to data preparation tasks. Configure the environment with all necessary dependencies ensuring compatibility with the existing Python setup. Set up appropriate storage using the teamspace drive for persistent data storage. Configure environment variables and secrets for data access. Document the Studio configuration, sleep mode settings, and resource allocation to optimize costs when not in use.",
          "status": "pending",
          "testStrategy": "Verify successful Studio creation with all dependencies installed. Test data access and processing capabilities with sample datasets."
        },
        {
          "id": 3,
          "title": "Model Training Studio Setup with GPU Configuration",
          "description": "Establish a dedicated Lightning.AI Studio for model training with GPU support and appropriate resource allocation.",
          "dependencies": [
            1
          ],
          "details": "Create a separate Lightning.AI Studio specifically for model training tasks. Configure GPU access and switching capabilities based on training requirements. Install all necessary training dependencies and frameworks. Set up environment variables for training parameters. Configure appropriate storage access for training data and model artifacts. Document GPU usage patterns, sleep mode settings, and cost optimization strategies.",
          "status": "pending",
          "testStrategy": "Verify GPU accessibility and performance with a small training job. Confirm proper environment configuration by running existing training scripts."
        },
        {
          "id": 4,
          "title": "Evaluation Framework Studio Implementation",
          "description": "Develop a dedicated Lightning.AI Studio for model evaluation with appropriate metrics tracking and visualization capabilities.",
          "dependencies": [
            1,
            3
          ],
          "details": "Set up a Lightning.AI Studio dedicated to model evaluation tasks. Configure the environment with necessary evaluation dependencies. Implement access to trained models from the model training Studio. Set up metrics tracking and visualization tools. Configure appropriate CPU/GPU resources based on evaluation needs. Document the evaluation workflow, environment configuration, and integration points with other Studios.",
          "status": "pending",
          "testStrategy": "Test the evaluation Studio with sample models to verify metrics calculation and reporting functionality."
        },
        {
          "id": 5,
          "title": "Lightning.AI Plugin Integration and Deployment Studio Setup",
          "description": "Integrate relevant Lightning.AI plugins and establish a deployment Studio for productionizing models with specific focus on Qwen3-4B requirements.",
          "dependencies": [
            1,
            3,
            4
          ],
          "details": "Research and integrate beneficial Lightning.AI plugins across all Studios. Set up a dedicated deployment Studio for serving trained models, with specific configurations for Qwen3-4B. Configure dual deployment environments for Qwen3-4B's thinking and non-thinking modes. Implement proper sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) for Qwen3-4B. Set up English-only token subset filtering in production environments. Optimize deployment for Qwen3's specific tokenizer patterns. Establish mode-specific endpoints and configurations for thinking/non-thinking modes. Implement robust error handling for token filtering operations. Configure monitoring systems to track mode-specific metrics. Implement proper resource allocation strategies based on Qwen3-4B's requirements. Set up efficient model loading, initialization, and caching strategies for both modes. Configure scaling mechanisms based on mode-specific load patterns. Document the plugin architecture, deployment workflow, and maintenance procedures. Ensure the setup supports iterative development and easy scaling for future needs.",
          "status": "pending",
          "testStrategy": "Verify plugin functionality with test cases. Deploy Qwen3-4B in both thinking and non-thinking modes and validate proper configuration of sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0). Test English-only token filtering functionality and error handling. Validate mode-specific endpoints and configurations. Perform load testing to verify proper resource allocation and scaling. Test caching strategies for both modes. Verify monitoring systems correctly track mode-specific metrics. Run comprehensive validation tests for both deployment modes to ensure proper functionality and performance."
        }
      ]
    },
    {
      "id": 12,
      "title": "Migrate Evaluation Framework Components to Lightning.AI Studios",
      "description": "Migrate all core evaluation framework components—including data preparation scripts, training workflows, and evaluation systems—into their respective Lightning.AI Studios, ensuring full functionality and leveraging Lightning.AI features.",
      "status": "pending",
      "dependencies": [
        11
      ],
      "priority": "high",
      "details": "Begin by auditing existing data preparation, training, and evaluation components to identify dependencies and required resources. For each component, refactor code and workflows as needed to fit Lightning.AI Studio paradigms, such as modular Studio apps. Migrate data preparation scripts into a dedicated Studio, ensuring compatibility with Lightning.AI's persistent storage for datasets and intermediate results. Move training workflows into Studios that utilize GPU switching, configuring resource allocation and environment variables for optimal performance. Transition evaluation systems into Studios, integrating with the plugin system for extensibility and automation. Throughout migration, maintain existing functionality and adapt interfaces to Lightning.AI's UI/UX standards. Document all changes, update configuration files, and ensure seamless integration between Studios. Address any deprecated or incompatible features by leveraging Lightning.AI alternatives or plugins.",
      "testStrategy": "Verify that each migrated component (data preparation, training, evaluation) runs successfully within its respective Lightning.AI Studio, producing expected outputs. Test GPU switching by running training workflows on different GPU types and confirming correct resource usage. Validate persistent storage by saving and retrieving datasets and intermediate results across Studio sessions. Assess plugin integration by installing and running at least one relevant plugin in the evaluation Studio. Perform end-to-end tests to ensure the entire workflow—from data preparation through evaluation—operates as intended. Compare results with pre-migration outputs to confirm parity. Review documentation for completeness and accuracy.",
      "subtasks": [
        {
          "id": 1,
          "title": "Audit and Analyze Existing Evaluation Framework Components",
          "description": "Conduct a comprehensive audit of all current data preparation scripts, training workflows, and evaluation systems to identify dependencies, required resources, and potential compatibility issues with Lightning.AI Studios.",
          "dependencies": [],
          "details": "Review codebases, document external dependencies, assess data storage needs, and map out current workflows to inform migration planning.",
          "status": "pending",
          "testStrategy": "Validate audit completeness by cross-referencing with existing documentation and stakeholder interviews."
        },
        {
          "id": 2,
          "title": "Migrate Data Preparation Scripts to Lightning.AI Studio",
          "description": "Refactor and migrate all data preparation scripts into a dedicated Lightning.AI Studio, ensuring compatibility with Lightning.AI's persistent storage and modular app architecture.",
          "dependencies": [
            1
          ],
          "details": "Adapt scripts to utilize Lightning.AI's storage APIs, modularize code as Studio apps, and ensure intermediate results are correctly handled.",
          "status": "pending",
          "testStrategy": "Run end-to-end data preparation workflows in the Studio and verify output consistency with pre-migration results."
        },
        {
          "id": 3,
          "title": "Migrate Training Workflows with GPU Switching and Resource Optimization",
          "description": "Transition training workflows into Lightning.AI Studios, implementing GPU switching, configuring resource allocation, and setting environment variables for optimal performance.",
          "dependencies": [
            2
          ],
          "details": "Refactor training code to leverage Lightning.AI's resource management features, ensure compatibility with Studio paradigms, and document environment configurations.",
          "status": "pending",
          "testStrategy": "Execute training runs in the Studio, monitor resource utilization, and compare training metrics to baseline results."
        },
        {
          "id": 4,
          "title": "Migrate and Integrate Evaluation Systems with Plugin Support",
          "description": "Move evaluation systems into Lightning.AI Studios, integrating with the plugin system for extensibility and automation, and adapting interfaces to Lightning.AI's UI/UX standards.",
          "dependencies": [
            3
          ],
          "details": "Refactor evaluation logic as modular Studio apps, implement plugin hooks, and update UI components for seamless user experience.",
          "status": "pending",
          "testStrategy": "Perform evaluation runs using migrated systems, test plugin integrations, and validate UI/UX compliance."
        },
        {
          "id": 5,
          "title": "System Integration, Testing, and Documentation",
          "description": "Ensure seamless integration between all migrated Studios, address deprecated or incompatible features, and document all changes and updated configurations.",
          "dependencies": [
            4
          ],
          "details": "Conduct system-wide integration tests, resolve any migration issues, and update technical documentation and configuration files.",
          "status": "pending",
          "testStrategy": "Run comprehensive integration and regression tests across Studios, verify documentation accuracy, and confirm end-to-end functionality."
        },
        {
          "id": 6,
          "title": "Implement Qwen3-4B Thinking and Non-Thinking Mode Optimization",
          "description": "Optimize both thinking and non-thinking modes of Qwen3-4B within the Lightning.AI Studios framework.",
          "dependencies": [
            3
          ],
          "details": "Develop and implement efficient mode switching mechanisms between thinking and non-thinking modes. Configure resource allocation optimization based on the active mode. Implement performance monitoring systems for both modes to track efficiency metrics.",
          "status": "pending",
          "testStrategy": "Benchmark performance of both modes, measure mode switching latency, and verify resource utilization patterns match expectations for each mode."
        },
        {
          "id": 7,
          "title": "Implement English-Only Token Filtering for Qwen3-4B",
          "description": "Develop and integrate efficient English-only token filtering for Qwen3-4B within the Lightning.AI Studios environment.",
          "dependencies": [
            2
          ],
          "details": "Design and implement memory-optimized token filtering algorithms for English-only subset. Develop efficient error handling mechanisms for token filtering edge cases. Optimize tokenizer performance specifically for the English-only subset.",
          "status": "pending",
          "testStrategy": "Test token filtering accuracy on diverse English text samples, measure memory usage during filtering operations, and validate error handling with malformed inputs."
        },
        {
          "id": 8,
          "title": "Optimize Sampling Parameters for Qwen3-4B",
          "description": "Configure and optimize sampling parameter settings (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) for Qwen3-4B in Lightning.AI Studios.",
          "dependencies": [
            3
          ],
          "details": "Implement the specified sampling parameter configurations. Develop efficient batch processing mechanisms that maintain optimal sampling parameters across both thinking and non-thinking modes.",
          "status": "pending",
          "testStrategy": "Validate output quality with specified sampling parameters, measure inference performance with optimized settings, and compare results against baseline configurations."
        },
        {
          "id": 9,
          "title": "Implement Caching Strategies for Qwen3-4B",
          "description": "Design and implement efficient caching strategies for both thinking and non-thinking modes of Qwen3-4B.",
          "dependencies": [
            6
          ],
          "details": "Develop mode-specific caching mechanisms that optimize for the different usage patterns of thinking and non-thinking modes. Optimize model loading and initialization processes to leverage caching for faster startup times.",
          "status": "pending",
          "testStrategy": "Measure cache hit rates in both modes, benchmark startup times with caching enabled, and validate cache coherence during mode switching."
        }
      ]
    },
    {
      "id": 13,
      "title": "Optimize and Fine-Tune Lightning.AI Studios for Performance, Cost, and Scalability",
      "description": "Optimize the Lightning.AI Studios setup for maximum performance, efficient GPU usage, cost management, and maintainability, while implementing best practices for organization, monitoring, and future scalability, with specific focus on Qwen3-4B model requirements.",
      "status": "pending",
      "dependencies": [
        12
      ],
      "priority": "medium",
      "details": "Review the current Lightning.AI Studios configuration post-migration (from Task 12) and identify bottlenecks in data loading, model training, and evaluation workflows. Implement dataset optimizations to minimize GPU idle time and reduce cloud costs by ensuring fast data transfer and leveraging Lightning's optimize operator where appropriate[1][2]. Adjust GPU allocation and sleep settings to balance performance with cost efficiency, using Lightning's built-in controls and best practices. Organize Studios and components following Lightning.AI's recommended structure for clarity and maintainability[3]. Set up comprehensive monitoring tools tailored for Qwen3-4B's specific requirements, including monitoring for both thinking and non-thinking modes, English-only token filtering performance, and sampling parameter effectiveness (Temperature=0.6, TopP=0.95, TopK=20, MinP=0). Establish automated alerts for resource usage anomalies and mode-specific issues. Document and automate regular maintenance procedures, including dependency updates and Studio health checks. Plan for future scaling by enabling multi-GPU support, low-precision training, and efficient configuration management using YAML recipes and parameter-efficient finetuning methods[5].",
      "testStrategy": "Verify that all Studios demonstrate improved performance by benchmarking training and evaluation times before and after optimization. Confirm reduced GPU idle time and lower cloud costs through resource usage reports. Ensure GPU sleep settings are correctly applied and tested for both cost savings and rapid wake-up. Review Studio organization for adherence to best practices and clarity. Validate that monitoring dashboards are operational for both thinking and non-thinking modes, with specific metrics for token filtering performance, sampling parameter effectiveness, and mode-specific performance. Test that alerts trigger appropriately for mode-specific issues. Verify comprehensive logging is implemented for both modes. Test latency monitoring for mode switching and cache effectiveness tracking. Validate maintenance scripts and procedures for reliability. Simulate scaling scenarios (e.g., multi-GPU, larger datasets) to confirm readiness for future growth.",
      "subtasks": [
        {
          "id": 1,
          "title": "Optimize Dataset for Performance",
          "description": "Review and optimize datasets to minimize GPU idle time and reduce cloud costs by ensuring fast data transfer and leveraging Lightning's optimize operator.",
          "dependencies": [],
          "details": "Use Lightning's optimize operator to convert datasets into compressed binary files for faster loading and reduced GPU idle time.",
          "status": "pending",
          "testStrategy": "Monitor GPU utilization and data loading speed"
        },
        {
          "id": 2,
          "title": "Tune GPU Allocation and Cost Efficiency",
          "description": "Adjust GPU allocation and sleep settings to balance performance with cost efficiency using Lightning's built-in controls and best practices.",
          "dependencies": [
            1
          ],
          "details": "Configure GPU settings to optimize performance while minimizing unnecessary costs.",
          "status": "pending",
          "testStrategy": "Evaluate cost savings and performance impact"
        },
        {
          "id": 3,
          "title": "Organize Studios for Maintainability",
          "description": "Organize Lightning.AI Studios and components following the recommended structure for clarity and maintainability.",
          "dependencies": [
            2
          ],
          "details": "Implement a structured organization of Studios to enhance clarity and facilitate future updates.",
          "status": "pending",
          "testStrategy": "Assess organizational clarity and ease of maintenance"
        },
        {
          "id": 4,
          "title": "Setup Qwen3-4B Specific Monitoring and Alerts",
          "description": "Set up comprehensive monitoring tools tailored for Qwen3-4B's specific requirements, including monitoring for both thinking and non-thinking modes.",
          "dependencies": [
            3
          ],
          "details": "Configure monitoring tools to track performance metrics specific to Qwen3-4B, including English-only token filtering performance, sampling parameter effectiveness (Temperature=0.6, TopP=0.95, TopK=20, MinP=0), mode-specific performance metrics, token filtering accuracy, cache effectiveness, and latency for mode switching. Set up alerts for mode-specific issues and resource usage anomalies.",
          "status": "pending",
          "testStrategy": "Verify alert functionality for mode-specific issues, validate monitoring accuracy for both thinking and non-thinking modes, and test latency monitoring for mode switching"
        },
        {
          "id": 5,
          "title": "Implement Comprehensive Logging for Qwen3-4B",
          "description": "Implement comprehensive logging system for Qwen3-4B's thinking and non-thinking modes.",
          "dependencies": [
            4
          ],
          "details": "Set up detailed logging for both thinking and non-thinking modes, capturing token filtering operations, sampling parameter applications, mode transitions, and error events.",
          "status": "pending",
          "testStrategy": "Verify log completeness, accuracy, and usefulness for both modes"
        },
        {
          "id": 6,
          "title": "Create Performance Dashboards for Qwen3-4B",
          "description": "Develop dedicated performance dashboards for monitoring Qwen3-4B in both thinking and non-thinking modes.",
          "dependencies": [
            4,
            5
          ],
          "details": "Create visual dashboards that display real-time metrics for token filtering performance, sampling parameter effectiveness, mode-specific performance, resource usage, cache effectiveness, and error rates.",
          "status": "pending",
          "testStrategy": "Validate dashboard functionality, data accuracy, and usefulness for performance optimization"
        },
        {
          "id": 7,
          "title": "Document and Automate Maintenance",
          "description": "Document and automate regular maintenance procedures, including dependency updates and Studio health checks.",
          "dependencies": [
            4,
            5,
            6
          ],
          "details": "Create documentation and automate scripts for regular maintenance tasks to ensure Studio health and efficiency, with specific procedures for Qwen3-4B model maintenance.",
          "status": "pending",
          "testStrategy": "Test automation scripts and review documentation completeness"
        },
        {
          "id": 8,
          "title": "Monitor Token Filtering Accuracy and Performance",
          "description": "Implement specific monitoring for English-only token filtering performance and accuracy.",
          "dependencies": [
            4
          ],
          "details": "Set up metrics to track token filtering accuracy, performance impact, and error rates specifically for English-only filtering in Qwen3-4B.",
          "status": "pending",
          "testStrategy": "Validate accuracy of token filtering metrics and error rate tracking"
        }
      ]
    },
    {
      "id": 14,
      "title": "Migrate to Qwen3-4B Model with English Token Filtering and Mode Support",
      "description": "Analyze and integrate the Qwen3-4B model into the codebase, extract English-only tokens from its tokenizer, and update model loading, configuration, and inference logic to support Qwen3-4B’s unique features, including thinking/non-thinking modes and specific sampling parameters.",
      "details": "1. Review the Qwen3-4B model documentation and Hugging Face integration requirements, ensuring compatibility with the latest transformers library (>=4.51.0) to avoid loading errors.\n2. Analyze the Qwen3-4B tokenizer to identify and extract the set of English-only tokens. Implement a script or utility to filter these tokens, documenting the extraction process and results.\n3. Refactor the model loading and configuration code to support Qwen3-4B, including device mapping, torch_dtype, and context length settings as per model specs.\n4. Update inference logic to handle Qwen3-4B’s thinking and non-thinking modes by leveraging the tokenizer’s `apply_chat_template` with `enable_thinking` parameter, and implement parsing logic to separate thinking content from final output (e.g., using the </think> token ID).\n5. Ensure that sampling parameters for thinking mode are set as follows: Temperature=0.6, TopP=0.95, TopK=20, MinP=0. Make these configurable and document their usage.\n6. Update documentation and code comments to reflect all changes, including migration steps, new configuration options, and any model-specific considerations.",
      "testStrategy": "- Verify that the codebase loads Qwen3-4B without errors and that model inference works in both thinking and non-thinking modes, producing expected outputs.\n- Confirm that the English-only token extraction utility correctly identifies and outputs the relevant tokens, and validate the results against known English token sets.\n- Test that the sampling parameters are correctly applied during inference in thinking mode, and that outputs change as expected when parameters are varied.\n- Ensure that the separation of thinking and content outputs works by running sample prompts and checking that both sections are parsed and displayed correctly.\n- Review updated documentation for clarity and completeness, and perform code review to ensure maintainability and adherence to project standards.",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Set up Qwen3-4B model with transformers library",
          "description": "Configure the development environment with the latest transformers library (>=4.51.0) and implement basic model loading for Qwen3-4B to ensure compatibility.",
          "dependencies": [],
          "details": "Install or update transformers to version 4.51.0 or higher to avoid the 'KeyError: qwen3' error. Create a basic implementation to load the Qwen3-4B model using AutoModelForCausalLM and AutoTokenizer from the Hugging Face transformers library. Configure proper device mapping and torch_dtype settings for optimal performance. Test basic model loading and simple inference to verify setup.\n<info added on 2025-05-10T09:22:09.195Z>\n## Environment Setup\n- Install or update transformers to version 4.51.0 or higher: `pip install --upgrade transformers>=4.51.0`\n- Verify installation with: `import transformers; print(transformers.__version__)`\n- Install any additional dependencies: `torch`, `accelerate` if needed\n\n## Implementation Plan\n1. Create implementation file at `src/models/qwen3_loader.py`\n2. Use the following code structure:\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ndef load_qwen3_model():\n    model_id = \"Qwen/Qwen3-4B\"\n    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)\n    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    \n    # Load model with appropriate dtype\n    dtype = torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, \n        torch_dtype=dtype,\n        device_map=device\n    )\n    \n    return model, tokenizer, device\n\ndef test_inference(model, tokenizer, device):\n    prompt = \"Hello, world!\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    # Generate text\n    outputs = model.generate(**inputs, max_new_tokens=20)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {prompt}\")\n    print(f\"Output: {generated_text}\")\n    return generated_text\n```\n\n3. Create a test script at `scripts/test_qwen3_loading.py`:\n```python\nfrom src.models.qwen3_loader import load_qwen3_model, test_inference\n\nif __name__ == \"__main__\":\n    print(\"Loading Qwen3-4B model...\")\n    model, tokenizer, device = load_qwen3_model()\n    print(\"Model loaded successfully!\")\n    \n    print(\"\\nTesting basic inference...\")\n    test_inference(model, tokenizer, device)\n```\n\n## Potential Challenges\n- Memory requirements: Qwen3-4B may require 8GB+ RAM\n- If MPS/CUDA fails, fallback to CPU with `device_map=\"auto\"` and `low_cpu_mem_usage=True`\n- May need to use quantization for memory-constrained environments\n\n## Success Criteria\n- Model loads without errors\n- Basic text generation works with a simple prompt\n- Code is modular and reusable for the next subtask (token filtering)\n</info added on 2025-05-10T09:22:09.195Z>\n<info added on 2025-05-10T09:41:51.958Z>\nInstall or update transformers to version 4.51.0 or higher to avoid the 'KeyError: qwen3' error. Create a basic implementation to load the Qwen3-4B model using AutoModelForCausalLM and AutoTokenizer from the Hugging Face transformers library. Configure proper device mapping and torch_dtype settings for optimal performance. Test basic model loading and simple inference to verify setup.\n\n<info added on 2025-05-10T09:22:09.195Z>\n## Environment Setup\n- Install or update transformers to version 4.51.0 or higher: `pip install --upgrade transformers>=4.51.0`\n- Verify installation with: `import transformers; print(transformers.__version__)`\n- Install any additional dependencies: `torch`, `accelerate` if needed\n\n## Implementation Plan\n1. Create implementation file at `src/models/qwen3_loader.py`\n2. Use the following code structure:\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ndef load_qwen3_model():\n    model_id = \"Qwen/Qwen3-4B\"\n    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)\n    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    \n    # Load model with appropriate dtype\n    dtype = torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, \n        torch_dtype=dtype,\n        device_map=device\n    )\n    \n    return model, tokenizer, device\n\ndef test_inference(model, tokenizer, device):\n    prompt = \"Hello, world!\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    # Generate text\n    outputs = model.generate(**inputs, max_new_tokens=20)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {prompt}\")\n    print(f\"Output: {generated_text}\")\n    return generated_text\n```\n\n3. Create a test script at `scripts/test_qwen3_loading.py`:\n```python\nfrom src.models.qwen3_loader import load_qwen3_model, test_inference\n\nif __name__ == \"__main__\":\n    print(\"Loading Qwen3-4B model...\")\n    model, tokenizer, device = load_qwen3_model()\n    print(\"Model loaded successfully!\")\n    \n    print(\"\\nTesting basic inference...\")\n    test_inference(model, tokenizer, device)\n```\n\n## Potential Challenges\n- Memory requirements: Qwen3-4B may require 8GB+ RAM\n- If MPS/CUDA fails, fallback to CPU with `device_map=\"auto\"` and `low_cpu_mem_usage=True`\n- May need to use quantization for memory-constrained environments\n\n## Success Criteria\n- Model loads without errors\n- Basic text generation works with a simple prompt\n- Code is modular and reusable for the next subtask (token filtering)\n</info added on 2025-05-10T09:22:09.195Z>\n\n## Multi-Environment Implementation Strategy\n\nFor this project, we'll use different environments for different stages of the workflow:\n\n1. **Local Development & Data Preparation**: Use Hugging Face transformers\n   - Model ID: 'Qwen/Qwen3-4B'\n   - Use for all data preparation, tokenization analysis, and validation\n   - Ideal for token extraction and filtering work in the next subtask\n   - Provides full access to tokenizer internals needed for English token filtering\n\n2. **Quantized Inference**: Use Ollama\n   - Model: 'qwen:4b'\n   - Implement using the Ollama Python API\n   - Example code:\n   ```python\n   import ollama\n   \n   response = ollama.chat(model='qwen:4b', \n                         messages=[{'role': 'user', 'content': 'Hello, world!'}])\n   print(response['message']['content'])\n   ```\n   - Advantages: Lower memory footprint, faster inference on consumer hardware\n\n3. **Fine-tuning**: Use Unsloth\n   - Environment: Google Colab or Lightning.ai (NOT on Mac)\n   - Provides optimized fine-tuning for Qwen models\n   - Warning: Do not attempt to install or use Unsloth or xformers on Mac systems\n\n4. **Implementation Notes**:\n   - Create separate modules for each environment\n   - Use factory pattern to abstract model interactions\n   - Document environment-specific requirements in README.md\n   - Create environment-specific setup scripts for each workflow\n</info added on 2025-05-10T09:41:51.958Z>\n<info added on 2025-05-10T10:03:45.695Z>\n## Tokenizer-Only Approach for Data Tasks\n\nFor data preparation and token extraction tasks, we should only load the Qwen3-4B tokenizer without loading the full model. This approach significantly improves efficiency since:\n\n1. The tokenizer is much smaller (~100MB) compared to the full model (~8GB)\n2. Tokenization operations run faster without model overhead\n3. Memory requirements are drastically reduced\n4. Token analysis can be performed entirely with the tokenizer\n\n### Updated Implementation Structure\n\n1. Create a separate tokenizer-only loader function in `src/models/qwen3_loader.py`:\n\n```python\ndef load_qwen3_tokenizer_only():\n    model_id = \"Qwen/Qwen3-4B\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    return tokenizer\n\ndef load_qwen3_model_and_tokenizer():\n    \"\"\"Only use this function for evaluation or prompt testing\"\"\"\n    model_id = \"Qwen/Qwen3-4B\"\n    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)\n    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    \n    # Load model with appropriate dtype\n    dtype = torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, \n        torch_dtype=dtype,\n        device_map=device\n    )\n    \n    return model, tokenizer, device\n```\n\n2. Update the test script to demonstrate both approaches:\n\n```python\nfrom src.models.qwen3_loader import load_qwen3_tokenizer_only, load_qwen3_model_and_tokenizer, test_inference\n\nif __name__ == \"__main__\":\n    # For data preparation tasks (token extraction, analysis)\n    print(\"Loading Qwen3-4B tokenizer only...\")\n    tokenizer = load_qwen3_tokenizer_only()\n    print(\"Tokenizer loaded successfully!\")\n    \n    # Example tokenization\n    text = \"Hello, world!\"\n    tokens = tokenizer.encode(text)\n    decoded = tokenizer.decode(tokens)\n    print(f\"Tokenized '{text}' to {tokens}\")\n    print(f\"Decoded back to: '{decoded}'\")\n    \n    # Only for evaluation/testing (comment out when not needed)\n    print(\"\\nLoading full Qwen3-4B model (only needed for inference)...\")\n    model, full_tokenizer, device = load_qwen3_model_and_tokenizer()\n    print(\"Model loaded successfully!\")\n    print(\"\\nTesting basic inference...\")\n    test_inference(model, full_tokenizer, device)\n```\n\n### Documentation Updates\n\nAdd the following to project documentation:\n\n- For all data preparation, token extraction, and analysis tasks, use ONLY the tokenizer\n- Full model loading should be restricted to:\n  - Final evaluation of filtered token sets\n  - Testing prompt generation with filtered vocabulary\n  - Benchmarking model performance with modified tokenizer\n\n### Memory and Performance Benefits\n\n- Tokenizer-only: ~100-200MB RAM usage\n- Full model: 8GB+ RAM usage\n- Tokenization speed: 5-10x faster without model loaded\n- Startup time: Near-instant for tokenizer vs. 30+ seconds for model\n\nThis approach will be particularly important for the next subtask \"Analyze and extract English-only tokens from Qwen3-4B tokenizer\" which requires extensive tokenizer operations but no model inference.\n</info added on 2025-05-10T10:03:45.695Z>",
          "status": "done",
          "testStrategy": "Verify successful model loading without errors and confirm basic text generation works with a simple prompt."
        },
        {
          "id": 2,
          "title": "Analyze and extract English-only tokens from Qwen3-4B tokenizer",
          "description": "Develop a methodology to identify and extract English tokens from the Qwen3-4B tokenizer, creating a filtered subset for English-only operations.",
          "dependencies": [
            1
          ],
          "details": "Load the Qwen3-4B tokenizer and analyze its vocabulary. Develop criteria for identifying English tokens (e.g., using regex patterns, Unicode ranges, or language detection algorithms). Create a script that extracts and saves the English token subset. Document the extraction methodology, criteria used, and statistical analysis of the results (total tokens, percentage of English tokens, etc.).\n<info added on 2025-05-10T10:14:27.786Z>\nLoad the Qwen3-4B tokenizer and analyze its vocabulary. Develop criteria for identifying English tokens (e.g., using regex patterns, Unicode ranges, or language detection algorithms). Create a script that extracts and saves the English token subset. Document the extraction methodology, criteria used, and statistical analysis of the results (total tokens, percentage of English tokens, etc.).\n\nImplementation Plan:\n1. Load the Qwen3-4B tokenizer using the existing function `load_qwen3_tokenizer_only()` in `src/models/qwen3_loader.py`.\n2. Access the tokenizer's vocabulary via `tokenizer.get_vocab()`.\n3. Develop criteria for identifying English tokens:\n   - Use regex to match tokens containing only English alphabet characters (A-Z, a-z), common English punctuation, and optionally numbers.\n   - Optionally, use Unicode ranges to further filter out non-English scripts.\n   - Consider using the `regex` library for advanced pattern matching.\n4. Write a script (suggested location: `scripts/extract_english_tokens.py`) that:\n   - Loads the tokenizer\n   - Iterates over the vocabulary\n   - Applies the English-token criteria\n   - Saves the filtered English tokens to a file (e.g., `data/processed/english_tokens_qwen3.txt`)\n   - Outputs statistics: total tokens, number and percentage of English tokens\n5. Document the methodology and criteria in the script docstring and output a summary report.\n6. For validation, encode/decode a sample English sentence using both the full and filtered token sets, and compare results.\n\nSpecial considerations:\n- Handle subwords and tokens with special characters appropriately\n- Account for byte-level or BPE encoding in the tokenizer\n- Apply Unicode normalization for edge cases\n- Ensure the filtered token set maintains coherence for English text processing\n</info added on 2025-05-10T10:14:27.786Z>",
          "status": "done",
          "testStrategy": "Validate the extracted English token set by encoding/decoding sample English texts and comparing results with the full tokenizer."
        },
        {
          "id": 3,
          "title": "Implement thinking/non-thinking mode support",
          "description": "Update the inference logic to handle Qwen3-4B's thinking and non-thinking modes, including parsing logic for separating thinking content from final output.",
          "dependencies": [
            1
          ],
          "details": "Modify the chat template application to use the 'enable_thinking' parameter in the tokenizer's apply_chat_template method. Implement parsing logic to identify and separate thinking content from final output using the </think> token ID (151668). Create utility functions to extract thinking content and final response from generation results. Add configuration options to enable/disable thinking mode based on use case requirements.",
          "status": "pending",
          "testStrategy": "Test with thinking mode enabled and disabled, verifying correct separation of thinking content and final responses in both scenarios."
        },
        {
          "id": 4,
          "title": "Configure model-specific sampling parameters",
          "description": "Implement and configure the specific sampling parameters required for Qwen3-4B, particularly for thinking mode operation.",
          "dependencies": [
            3
          ],
          "details": "Create a configuration system for Qwen3-4B sampling parameters with defaults set to: Temperature=0.6, TopP=0.95, TopK=20, MinP=0 for thinking mode. Make these parameters configurable through API or configuration files. Implement parameter validation to ensure values are within acceptable ranges. Update the model generation code to apply these parameters during inference.",
          "status": "pending",
          "testStrategy": "Verify that changing sampling parameters produces expected variations in model outputs, especially in thinking mode responses."
        },
        {
          "id": 5,
          "title": "Integrate Qwen3-4B into existing codebase and document changes",
          "description": "Refactor the existing codebase to fully support Qwen3-4B, including model loading, configuration, and inference logic, with comprehensive documentation.",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Update model factory or loading mechanisms to recognize and properly initialize Qwen3-4B. Modify configuration schemas to include Qwen3-4B specific parameters (context length of 32,768, GQA attention heads configuration, etc.). Create migration guides for developers explaining how to transition to Qwen3-4B. Document all new features, parameters, and configuration options with examples. Update API documentation to reflect changes in behavior and capabilities.",
          "status": "pending",
          "testStrategy": "Perform end-to-end testing of the integrated solution, verifying all components work together correctly with both simple and complex prompts."
        }
      ]
    },
    {
      "id": 15,
      "title": "Convert Existing Work and Datasets for Qwen3-4B Compatibility",
      "description": "Update training data format, evaluation framework, and scripts to ensure compatibility with Qwen3-4B, including support for its tokenizer and thinking/non-thinking modes, while validating the integrity of the original experiment.",
      "details": "This task involves converting all existing training datasets and evaluation frameworks to be compatible with the Qwen3-4B model. Key steps include: (1) Updating the training data format to align with Qwen3's tokenizer, ensuring that only English tokens are used as per the English-only subset; (2) Adapting the evaluation framework to support Qwen3-4B's unique features, such as thinking and non-thinking modes, and ensuring that all evaluation metrics and scripts are updated to reflect these capabilities; (3) Modifying all existing scripts and utilities to work seamlessly with the new tokenizer and model configuration; (4) Validating that the conversion process maintains the integrity of the original experiment's goals by comparing results before and after the migration. Special attention should be paid to the tokenizer's handling of English-only data and the model's reasoning modes, as these are critical for accurate and fair evaluation. Documentation should be updated to reflect all changes and provide clear guidance for future maintenance.",
      "testStrategy": "To verify task completion, perform the following: (1) Run the updated training pipeline with the converted datasets and confirm that the model trains without errors; (2) Execute the evaluation framework with both thinking and non-thinking modes enabled, ensuring that all metrics are reported correctly and that the results are consistent with the original experiment; (3) Validate that all scripts and utilities function as expected with the new tokenizer and model configuration; (4) Compare the results of the original and converted pipelines to ensure that the integrity of the experiment is maintained; (5) Review updated documentation for accuracy and completeness. Automated tests should be added where possible to catch regressions in data processing, tokenization, and evaluation logic.",
      "status": "pending",
      "dependencies": [
        14
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Convert Training Datasets to Qwen3-4B English-Only Tokenizer Format",
          "description": "Transform all existing training datasets to be compatible with the Qwen3-4B tokenizer, ensuring only English tokens are included as required by the English-only subset.",
          "dependencies": [],
          "details": "Analyze the Qwen3-4B tokenizer specifications and preprocess datasets to filter out non-English tokens, reformatting data as needed for model ingestion.",
          "status": "pending",
          "testStrategy": "Validate that all tokens in the converted datasets are recognized by the Qwen3-4B tokenizer and that sample batches can be tokenized without errors."
        },
        {
          "id": 2,
          "title": "Update Evaluation Framework for Qwen3-4B Reasoning Modes",
          "description": "Adapt the evaluation framework to support Qwen3-4B's unique features, including thinking and non-thinking modes, and update evaluation metrics and scripts accordingly.",
          "dependencies": [
            1
          ],
          "details": "Review Qwen3-4B's API and reasoning mode requirements, modifying evaluation scripts to trigger and assess both thinking and non-thinking outputs, and ensure metrics capture relevant model behaviors.",
          "status": "pending",
          "testStrategy": "Run evaluation on sample data in both modes and verify that outputs and metrics reflect the intended reasoning mode distinctions."
        },
        {
          "id": 3,
          "title": "Refactor Scripts and Utilities for Qwen3-4B Compatibility",
          "description": "Modify all existing scripts and utilities to work seamlessly with the Qwen3-4B model configuration and tokenizer.",
          "dependencies": [
            1
          ],
          "details": "Update data loading, preprocessing, and inference scripts to use the latest Qwen3-4B APIs and ensure compatibility with the model's configuration and tokenizer requirements.",
          "status": "pending",
          "testStrategy": "Execute end-to-end training and inference pipelines using the updated scripts and confirm successful runs without compatibility errors."
        },
        {
          "id": 4,
          "title": "Validate Integrity of Experimental Results Post-Conversion",
          "description": "Ensure that the conversion process maintains the integrity of the original experiment by comparing results before and after migration to Qwen3-4B.",
          "dependencies": [
            2,
            3
          ],
          "details": "Design and run comparative experiments using both the original and converted setups, analyzing key metrics to confirm that experimental goals and data integrity are preserved.",
          "status": "pending",
          "testStrategy": "Statistically compare pre- and post-conversion results, documenting any discrepancies and verifying that performance and evaluation outcomes remain consistent."
        },
        {
          "id": 5,
          "title": "Update Documentation and Provide Maintenance Guidance",
          "description": "Revise all relevant documentation to reflect changes made for Qwen3-4B compatibility and offer clear guidance for future maintenance.",
          "dependencies": [
            4
          ],
          "details": "Document new data formats, evaluation procedures, script changes, and best practices for working with Qwen3-4B, including handling of English-only data and reasoning modes.",
          "status": "pending",
          "testStrategy": "Review documentation for completeness and clarity, and solicit feedback from team members to ensure usability for future updates."
        }
      ]
    },
    {
      "id": 16,
      "title": "Update Environment Setup Scripts and Onboarding Docs for Qwen3-4B and Token Extraction",
      "description": "Revise all environment setup scripts and onboarding documentation to ensure compatibility with Qwen3-4B and the new English-only token extraction process, including updated installation instructions for transformers (>=4.51.0) and usage guidance for new token extraction scripts. Clearly distinguish between local (Mac/Apple Silicon) and cloud-based workflows.",
      "status": "pending",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "Review all existing environment setup scripts (e.g., shell scripts, Python setup files) and onboarding documentation to ensure they reference and support Qwen3-4B as the default model. Update instructions to include installation or upgrade steps for the transformers library (version 4.51.0 or higher), specifying any required flags or compatibility notes. \n\nClearly document the split between local and cloud workflows:\n- Local Mac (Apple Silicon) setup: Specify that only Mac-compatible packages should be installed. Explicitly note that Unsloth and xformers should NOT be installed locally. Include instructions to use `uv pip install ollama` for the Ollama Python API. Document using Hugging Face transformers for data preparation and tokenization.\n- Cloud workflow: Clarify that all fine-tuning and Unsloth steps should be performed in cloud environments only (Google Colab or Lightning).\n\nAdd explicit instructions for using Ollama for local quantized inference and transformers for data preparation. Integrate clear, step-by-step guidance for using the new English-only token extraction scripts, including prerequisites, example commands, and troubleshooting tips. Reference the new workflow and model requirements throughout, ensuring that all documentation is consistent and unambiguous. Coordinate with the team to verify that all changes align with the latest project standards and dependencies established in Task 1.",
      "testStrategy": "Verify that a new developer can follow the updated scripts and documentation to set up a working environment from scratch on both local Mac (Apple Silicon) and cloud environments. Test that developers can successfully install transformers (>=4.51.0) and Ollama locally, and run the new token extraction scripts with Qwen3-4B. Verify that the documentation clearly distinguishes which operations should be performed locally versus in the cloud. Confirm that all references to model requirements and workflows are accurate and that no outdated instructions remain. Test the local quantized inference workflow with Ollama. Solicit feedback from at least one team member not involved in the update to ensure clarity and completeness.",
      "subtasks": [
        {
          "id": "16.1",
          "description": "Update README and onboarding docs to clearly document the split between local and cloud workflows",
          "status": "pending"
        },
        {
          "id": "16.2",
          "description": "Create specific installation instructions for Mac/Apple Silicon that exclude Unsloth and xformers",
          "status": "pending"
        },
        {
          "id": "16.3",
          "description": "Add instructions for installing and using Ollama for local quantized inference",
          "status": "pending"
        },
        {
          "id": "16.4",
          "description": "Update documentation for using transformers for local data preparation and tokenization",
          "status": "pending"
        },
        {
          "id": "16.5",
          "description": "Create clear guidance on which operations should be performed in cloud environments only",
          "status": "pending"
        }
      ]
    },
    {
      "id": 17,
      "title": "Refactor Token Extraction Pipeline for Qwen3-4B English Subset",
      "description": "Replace all GPT-2 token extraction scripts and outputs with a new process that extracts the English-only token subset from the Qwen3-4B tokenizer. Validate, document, and update all downstream references to use the new extraction process and outputs.",
      "details": "1. Analyze the Qwen3-4B tokenizer using the latest Hugging Face 'transformers' library to extract its full vocabulary. 2. Define clear criteria for identifying 'English-only' tokens (e.g., tokens composed exclusively of ASCII letters and common English punctuation, excluding tokens with non-English characters or symbols). 3. Implement a new extraction script (preferably in Python) that filters the Qwen3-4B vocabulary to produce the English-only token subset, saving the result in a well-documented JSON format. 4. Validate the extracted subset by sampling and manually inspecting tokens, and by running automated checks for non-English characters. 5. Update all downstream scripts, references, and documentation to use the new Qwen3-4B English token subset and extraction process, removing all GPT-2-specific logic and outputs. 6. Provide comprehensive documentation describing the extraction criteria, process, and usage of the new subset.",
      "testStrategy": "- Run the new extraction script and verify that the output JSON contains only tokens matching the defined English-only criteria. - Manually inspect a random sample of extracted tokens to confirm correctness. - Execute automated tests to ensure no non-English characters are present in the subset. - Confirm that all downstream scripts and documentation reference the new Qwen3-4B English token subset and that no GPT-2-specific code or outputs remain. - Review documentation for completeness and clarity regarding the new extraction process and criteria.",
      "status": "pending",
      "dependencies": [
        2
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement the New Token Extraction Script",
          "description": "Develop and refactor the token extraction pipeline to use the new extraction script, ensuring it is optimized for efficiency and maintains functional consistency with the previous implementation.",
          "dependencies": [],
          "details": "This involves rewriting or updating the extraction logic, integrating any new requirements, and ensuring the script is ready for validation. Focus on minimizing token usage and maintaining coherent context as per the new model's needs.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Validate and Document the Extraction Output",
          "description": "Test the new extraction script to ensure its output matches expected results and document the extraction process, output format, and any changes from the previous version.",
          "dependencies": [
            1
          ],
          "details": "Perform thorough validation using representative data, compare outputs with the previous pipeline, and document the new script's behavior, usage instructions, and output specifications for future reference.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Update Downstream References and Scripts",
          "description": "Identify and update all downstream scripts and references that depend on the token extraction pipeline to ensure compatibility with the new extraction script and output format.",
          "dependencies": [
            2
          ],
          "details": "Audit all dependent codebases, update integration points, and perform end-to-end testing to confirm that downstream processes function correctly with the refactored pipeline.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 18,
      "title": "Recreate All Dataset Splits Using Qwen3-4B English-Only Token Subset",
      "description": "Regenerate the training, validation, and test datasets using the Qwen3-4B English-only token subset, ensuring compatibility with the new tokenizer and updating all metadata and data structures to match the revised format.",
      "details": "Leverage the updated token extraction pipeline from Task 17 to process all raw data and regenerate the training, validation, and test splits exclusively using the Qwen3-4B English-only token subset. For each split, re-tokenize all examples with the Qwen3-4B tokenizer (as available via Hugging Face Transformers) and verify that every example is fully compatible—i.e., all tokens are present in the English-only subset, and no legacy or out-of-vocabulary tokens remain. Update all dataset metadata (e.g., token counts, vocabulary statistics, data format descriptors) to reflect the new tokenization. Ensure that the data structures, file formats, and split boundaries strictly follow the updated requirements, and that all downstream consumers of the data are notified of the changes. Document any edge cases or data exclusions resulting from incompatibility with the new tokenizer.",
      "testStrategy": "1. For each dataset split, confirm that all examples are tokenized exclusively with the Qwen3-4B English-only token subset and that no out-of-vocabulary or legacy tokens are present. 2. Validate that the number of examples and split boundaries match the intended design and that no data leakage occurs between splits. 3. Check that all metadata fields (e.g., token counts, vocabulary lists, format descriptors) are accurate and up-to-date. 4. Run automated compatibility tests to ensure all downstream scripts and pipelines can load and process the new datasets without errors. 5. Manually inspect a sample of examples from each split to confirm correct tokenization and data structure. 6. Document and review any data exclusions or format changes with the team before final sign-off.",
      "status": "pending",
      "dependencies": [
        3,
        17
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Regenerate Dataset Splits with New Token Subset",
          "description": "Process the dataset with the new tokenizer subset and create appropriate train/test/validation splits",
          "dependencies": [],
          "details": "Use the train_test_split() function to create appropriate dataset splits with the new tokenization. Consider using a 80/10/10 split for train/validation/test or adjust based on project requirements. Ensure proper shuffling of data before splitting. For large datasets, consider using sharding techniques to manage memory efficiently.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Validate Dataset Compatibility and Metadata",
          "description": "Verify the regenerated dataset splits maintain compatibility with models and contain correct metadata",
          "dependencies": [
            1
          ],
          "details": "Compare token distributions between old and new datasets. Verify schema consistency across all splits. Check for potential data leakage between splits. Validate that tokenization is consistent and properly cached to avoid redundant processing. Run basic model inference tests to ensure compatibility with existing models.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Update Downstream Consumers and Documentation",
          "description": "Communicate changes and update all systems that consume the dataset",
          "dependencies": [
            2
          ],
          "details": "Update pipeline configurations to use the new dataset splits. Modify any hardcoded references to the old dataset structure. Update documentation with new dataset statistics and tokenization details. Communicate changes to all stakeholders, including information about potential impacts on model performance. Create examples showing how to properly load and use the new dataset splits.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 19,
      "title": "Revalidate and Update Template-Generated Training Data for Qwen3-4B English Token Subset",
      "description": "Audit all template-generated training data to ensure strict compatibility with the Qwen3-4B English-only token subset, updating templates, separators, and formatting scripts as necessary. Regenerate non-compliant data and revise documentation to reflect all changes, clearly distinguishing between local data processing and cloud-based training workflows.",
      "status": "pending",
      "dependencies": [
        4,
        18
      ],
      "priority": "high",
      "details": "Begin by auditing all existing template-generated training data, focusing on tokenization compatibility with the Qwen3-4B English-only token subset as established in Task 18. Review and update all data generation templates, separator styles, and formatting scripts to ensure they produce outputs that exclusively use valid English tokens recognized by the Qwen3-4B tokenizer. For any data that fails validation, regenerate it using the updated templates and scripts. Ensure that all formatting (including separators, prompt structures, and metadata) adheres to the new requirements. All template generation and validation should be performed locally using Hugging Face transformers and the Qwen3-4B tokenizer. Update internal documentation to describe the revised templates, formatting conventions, and validation procedures, with clear distinction between local data processing workflows and cloud-based training workflows. Explicitly document that fine-tuning and LoRA optimizations are cloud-only operations (Colab/Lightning), and that Unsloth or xformers should not be used in local environments. Coordinate with the teams responsible for dataset splits and downstream consumers to ensure seamless integration of the updated data.",
      "testStrategy": "1. Run automated tokenization checks on all template-generated data to confirm exclusive use of Qwen3-4B English-only tokens using local Hugging Face transformers. 2. Manually inspect a representative sample of data for correct template application, separator usage, and formatting. 3. Verify that all scripts and templates have been updated and are producing compliant outputs. 4. Confirm that all previously non-compliant data has been regenerated and replaced. 5. Review updated documentation for completeness and clarity, especially regarding the separation between local data processing and cloud-based training workflows. 6. Ensure downstream data consumers can ingest and process the revised datasets without errors. 7. Validate that documentation clearly indicates which operations should be performed locally versus in cloud environments.",
      "subtasks": [
        {
          "id": 1,
          "title": "Audit and Update Templates/Scripts for Tokenizer Compatibility",
          "description": "Review all existing templates and scripts to identify non-compliant data generation patterns. Update them to ensure compatibility with the tokenizer and maintain audit trails for changes.",
          "dependencies": [],
          "details": "This includes examining current data handling processes, identifying sensitive or non-compliant data streams, and updating templates/scripts to align with tokenizer requirements. Ensure all changes are logged for compliance and future audits.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Regenerate Non-Compliant Data",
          "description": "Use the updated templates/scripts to regenerate any previously generated data that does not meet tokenizer compatibility standards.",
          "dependencies": [
            1
          ],
          "details": "After updating the templates/scripts, identify all instances of non-compliant data and systematically regenerate them using the new, compliant processes. Ensure that regenerated data is properly mapped and stored according to tokenization best practices.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Revise Documentation and Downstream Integration",
          "description": "Update all relevant documentation and notify or adjust downstream systems and integrations to reflect the changes in data generation and tokenization processes.",
          "dependencies": [
            2
          ],
          "details": "Revise user guides, technical documentation, and integration instructions to reflect the new processes. Communicate changes to stakeholders and ensure downstream systems are tested and updated as needed to handle the revised data formats.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Document Local vs. Cloud Workflow Separation",
          "description": "Create clear documentation distinguishing between local data processing and cloud-based training workflows.",
          "dependencies": [
            1
          ],
          "details": "Document that all template generation and validation can be performed locally using Hugging Face transformers and the Qwen3-4B tokenizer. Explicitly note that fine-tuning and LoRA optimizations are cloud-only operations (Colab/Lightning), and that Unsloth or xformers should not be used in local environments. Create workflow diagrams showing the separation between local and cloud processes for data generation, validation, and training.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Implement Local Validation Tools Using HF Transformers",
          "description": "Develop and document local validation tools using Hugging Face transformers to verify token compatibility.",
          "dependencies": [
            1
          ],
          "details": "Create scripts that leverage the Hugging Face transformers library to locally validate that all generated data uses only tokens from the Qwen3-4B English-only subset. Document the installation and usage of these tools, ensuring they can be run in standard local environments without cloud-specific optimizations like Unsloth or xformers.",
          "status": "pending"
        }
      ]
    }
  ]
}