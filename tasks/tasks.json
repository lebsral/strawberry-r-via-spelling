{
  "tasks": [
    {
      "id": 1,
      "title": "Development Environment Setup",
      "description": "Set up the Python development environment with all required dependencies using uv for package management.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "1. Install uv package manager: `curl -fsSL https://astral.sh/uv/install.sh | bash`\n2. Create project directory structure following the repository structure in the PRD\n3. Install dependencies directly using uv commands instead of editing requirements.txt:\n   - `uv pip install torch transformers datasets wandb dspy lightning matplotlib seaborn pandas jupyter notebook ipywidgets`\n4. After installation, generate requirements.txt for documentation: `uv pip freeze > requirements.txt`\n5. Set up Git repository with proper .gitignore\n6. Configure Weights & Biases account: `wandb login`\n7. Set up Hugging Face account and API access: `huggingface-cli login`\n8. Create initial Jupyter notebook for experimentation\n9. Verify all imports work correctly\n\nNote: For Unsloth or GPU-based fine-tuning, use Google Colab or https://lightning.ai/lars/home.",
      "testStrategy": "1. Verify all libraries install without errors using uv\n2. Confirm successful authentication with W&B\n3. Confirm successful authentication with Hugging Face\n4. Test import of all required libraries in a Jupyter notebook\n5. Make initial commit with environment setup completed\n6. Verify environment reproducibility by creating a new environment using the generated requirements.txt",
      "subtasks": [
        {
          "id": 1,
          "title": "Installation Phase",
          "description": "Set up the development environment by installing all necessary software, tools, and dependencies using uv package manager",
          "dependencies": [],
          "details": "Install uv package manager first, then use it to install all required libraries directly with 'uv pip install' commands. Document all installation paths and versions for future reference. Use uv for all Python package management to ensure reproducibility and faster installation times.\n<info added on 2025-05-07T14:46:36.027Z>\nInstall uv package manager first, then use it to install all required libraries directly with 'uv pip install' commands. Document all installation paths and versions for future reference. Use uv for all Python package management to ensure reproducibility and faster installation times.\n\nThis task can be worked on independently and in parallel with others. The Installation Phase has no dependencies and is parallelizable (parallelizable: true).\n</info added on 2025-05-07T14:46:36.027Z>\n<info added on 2025-05-07T15:14:41.583Z>\nThe Installation Phase has been completed successfully. The following steps were taken to set up the development environment:\n\n1. Created a new Python virtual environment using uv:\n   ```sh\n   uv venv .venv\n   ```\n2. Activated the environment:\n   ```sh\n   source .venv/bin/activate\n   ```\n3. Installed core development dependencies:\n   ```sh\n   uv pip install black ruff mypy ipython requests\n   ```\n   (Additional project-specific packages can be added as needed)\n4. Generated requirements.txt for reproducibility:\n   ```sh\n   uv pip freeze > requirements.txt\n   ```\n5. Documented uv version:\n   ```sh\n   uv --version\n   ```\n\nAll installation paths and versions have been documented as required. The development environment is now fully set up and ready for use. Setup instructions have been added to the README for reference. The next phase (Configuration Phase) can now begin.\n</info added on 2025-05-07T15:14:41.583Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Configuration Phase",
          "description": "Configure all installed components to work together properly and set up authentication for external services",
          "dependencies": [
            1
          ],
          "details": "Set environment variables, configure IDE settings, establish database connections, set up version control repositories, configure build tools, and establish authentication for any external services or APIs required for development. Generate requirements.txt using 'uv pip freeze > requirements.txt' for documentation purposes.\n<info added on 2025-05-07T14:26:08.046Z>\nSet environment variables, configure IDE settings, establish database connections, set up version control repositories, configure build tools, and establish authentication for any external services or APIs required for development. Generate requirements.txt using 'uv pip freeze > requirements.txt' for documentation purposes.\n\nCreate a proper environment configuration by duplicating the .env.example file to .env:\n1. Copy the .env.example file to .env using the command: `cp .env.example .env` (Unix/Mac) or `copy .env.example .env` (Windows)\n2. Open the newly created .env file and fill in all required values\n3. Ensure all environment variables are properly set according to your local development environment\n4. Document any custom environment variables added to the project in the .env.example file with appropriate comments\n5. Verify that sensitive information (API keys, passwords, etc.) is not committed to version control\n6. Update the project documentation to include information about required environment variables\n</info added on 2025-05-07T14:26:08.046Z>\n\nNote: For Unsloth or GPU-based fine-tuning, use Google Colab or https://lightning.ai/lars/home. The local environment should only include packages compatible with Mac (Apple Silicon) and not require GPU or xformers.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Verification Phase",
          "description": "Test the complete development environment to ensure all components work together properly",
          "dependencies": [
            2
          ],
          "details": "Run test scripts to verify installations, validate configurations, test connections to external services, perform a sample build process, and document any issues encountered along with their resolutions. Test environment reproducibility by creating a new environment using the generated requirements.txt and uv.\n\nNote: Verify that only Mac (Apple Silicon) compatible packages are installed. GPU-dependent packages like Unsloth and xformers should not be included in the local environment. For GPU-accelerated workflows, document the process of using Google Colab or https://lightning.ai/lars/home instead.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Create and Maintain README.md for Developer Onboarding",
          "description": "Develop and maintain a comprehensive README.md file to help new developers set up the environment, understand the project structure, and follow best practices.",
          "details": "1. Write a clear project overview and purpose.\n2. Document the setup process, including using uv for environment management, installing dependencies, and configuring the .env file.\n3. Explain the directory structure and the role of key files (e.g., requirements.txt, .env, .env.example, notebooks, scripts).\n4. Provide instructions for running Jupyter notebooks and scripts.\n5. List common troubleshooting tips and FAQs.\n6. Include contribution guidelines and code style references.\n7. Update the README.md as the project evolves to ensure accuracy and completeness.\n8. Add a dedicated section explaining that the local environment is designed for Mac (Apple Silicon) compatibility and does not include GPU-dependent packages like Unsloth and xformers.\n9. Document how to use Google Colab or https://lightning.ai/lars/home for GPU-accelerated workflows and Unsloth-based fine-tuning.\n<info added on 2025-05-07T14:46:49.605Z>\n1. Write a clear project overview and purpose.\n2. Document the setup process, including using uv for environment management, installing dependencies, and configuring the .env file.\n3. Explain the directory structure and the role of key files (e.g., requirements.txt, .env, .env.example, notebooks, scripts).\n4. Provide instructions for running Jupyter notebooks and scripts.\n5. List common troubleshooting tips and FAQs.\n6. Include contribution guidelines and code style references.\n7. Update the README.md as the project evolves to ensure accuracy and completeness.\n\nNote: This task can be worked on independently and in parallel with others. It has no dependencies and is parallelizable: true.\n</info added on 2025-05-07T14:46:49.605Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 1
        },
        {
          "id": 5,
          "title": "Document Cloud-based GPU Environment Setup",
          "description": "Create documentation for setting up GPU-accelerated environments on Google Colab and Lightning.ai for Unsloth-based fine-tuning",
          "dependencies": [
            1
          ],
          "details": "1. Create a dedicated Jupyter notebook with setup instructions for Google Colab that includes:\n   - Installing Unsloth and other GPU-dependent packages\n   - Setting up authentication for W&B and Hugging Face\n   - Example code for fine-tuning with Unsloth\n2. Document the process for using Lightning.ai/lars for GPU-accelerated workflows\n3. Include instructions for transferring local work to cloud environments\n4. Document how to sync results and models back to the local environment\n5. Add troubleshooting tips specific to cloud GPU environments",
          "status": "done",
          "parentTaskId": 1
        }
      ]
    },
    {
      "id": 2,
      "title": "Token Extraction from GPT-2 Vocabulary",
      "description": "Extract all multi-character, letter-based tokens from the GPT-2 tokenizer vocabulary and save them to a JSON file.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "1. Load the GPT-2 tokenizer from Hugging Face\n2. Extract all tokens from the vocabulary\n3. Filter tokens to include only multi-character, letter-based tokens\n4. Save the filtered token list to a JSON file with the structure specified in the PRD\n5. Create a Jupyter notebook to verify token selection\n6. Analyze token frequency and length distribution\n\nThis task has been broken down into three parallelizable subtasks that can be worked on independently:\n- Script Writing: Implementing the token extraction logic\n- Validation & Testing: Ensuring the extracted tokens meet requirements\n- Documentation: Creating clear documentation for the process and results\n\nFile Organization:\n- Main token extraction script: `src/data/token_extractor.py`\n- Extracted tokens file: `data/processed/gpt2_letter_tokens.json`\n- Validation notebook: `notebooks/token_validation.ipynb`\n- Token analysis visualizations: `results/token_analysis/`\n- Documentation: `docs/token_extraction.md`\n\nImplementation:\n```python\nfrom transformers import GPT2Tokenizer\nimport json\nimport re\nimport os\n\ndef extract_tokens():\n    # Load tokenizer\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n    # Extract and filter tokens\n    filtered_tokens = []\n    for token_id, token_text in tokenizer.get_vocab().items():\n        # Remove special tokens and decode byte tokens\n        decoded_token = tokenizer.convert_tokens_to_string([token_text])\n\n        # Filter for multi-character letter-based tokens\n        if len(decoded_token) > 1 and re.match(r'^[a-zA-Z]+$', decoded_token):\n            filtered_tokens.append({\n                \"token\": decoded_token,\n                \"token_id\": token_id,\n                \"char_length\": len(decoded_token)\n            })\n\n    # Ensure directory exists\n    os.makedirs(\"data/processed\", exist_ok=True)\n    \n    # Save to file\n    with open(\"data/processed/gpt2_letter_tokens.json\", \"w\") as f:\n        json.dump({\"tokens\": filtered_tokens}, f, indent=2)\n\n    return filtered_tokens\n\ntokens = extract_tokens()\nprint(f\"Extracted {len(tokens)} multi-character letter-based tokens\")\n```\n\nResults:\n- Successfully extracted 46,789 tokens from the GPT-2 vocabulary\n- All tokens are multi-character and letter-based as required\n- Tokens saved to JSON file with proper structure",
      "testStrategy": "1. Verify JSON file is successfully created at `data/processed/gpt2_letter_tokens.json`\n2. Confirm file contains at least 5,000 tokens\n3. Randomly sample tokens to confirm they are multi-character and letter-based\n4. Verify no special tokens (like <|endoftext|>) are included\n5. Create visualizations of token length distribution and save to `results/token_analysis/`\n6. Commit token extraction script and results\n\nAll tests have been successfully completed. The extracted token set contains 46,789 tokens, which exceeds the minimum requirement of 5,000 tokens. Validation confirmed all tokens are multi-character and letter-based with no special tokens included.",
      "subtasks": [
        {
          "id": 2.1,
          "title": "Script Writing",
          "description": "Implement the token extraction logic from the GPT-2 vocabulary",
          "details": "1. Create the script at `src/data/token_extractor.py`\n2. Load the GPT-2 tokenizer from Hugging Face\n3. Extract all tokens from the vocabulary\n4. Filter tokens to include only multi-character, letter-based tokens\n5. Save the filtered token list to `data/processed/gpt2_letter_tokens.json` with the structure specified in the PRD\n6. Ensure all necessary directories are created if they don't exist\n\nThis task can be worked on independently and in parallel with others.\n\nparallelizable: true",
          "status": "completed"
        },
        {
          "id": 2.2,
          "title": "Validation & Testing",
          "description": "Ensure the extracted tokens meet requirements and create validation tools",
          "details": "1. Create a Jupyter notebook at `notebooks/token_validation.ipynb` to verify token selection\n2. Verify JSON file is successfully created at `data/processed/gpt2_letter_tokens.json`\n3. Confirm file contains at least 5,000 tokens\n4. Randomly sample tokens to confirm they are multi-character and letter-based\n5. Verify no special tokens (like <|endoftext|>) are included\n6. Create visualizations of token length distribution and save to `results/token_analysis/`\n\nThis task can be worked on independently and in parallel with others.\n\nparallelizable: true",
          "status": "completed"
        },
        {
          "id": 2.3,
          "title": "Documentation",
          "description": "Create clear documentation for the token extraction process and results",
          "details": "1. Create documentation file at `docs/token_extraction.md`\n2. Document the token extraction methodology\n3. Analyze token frequency and length distribution\n4. Create a README explaining how to use the extraction script\n5. Document any interesting patterns or observations in the token set\n6. Include references to file locations (`src/data/token_extractor.py`, `data/processed/gpt2_letter_tokens.json`, etc.)\n7. Prepare documentation for integration with other components\n\nThis task can be worked on independently and in parallel with others.\n\nparallelizable: true",
          "status": "completed"
        }
      ]
    },
    {
      "id": 3,
      "title": "Dataset Creation and Splitting",
      "description": "Create training, validation, and test datasets for spelling tasks, ensuring proper separation between training tokens and validation/test words to evaluate if training on spelling improves model performance on position and count question metrics.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "1. Download English word list from dwyl/english-words repository\n2. Create training set from tokenizer vocabulary (multi-character, letter-only tokens)\n3. Create validation/test sets from English dictionary words that:\n   - Split into multiple tokens by the tokenizer\n   - Have at least one token in the split that is multi-character and letter-only\n   - Do not appear in the training set\n4. Generate letter count questions (\"How many X's are in Y?\")\n5. Generate letter position questions (\"What is the Nth letter in Y?\")\n6. Split data based on source rather than percentage: training from tokenizer vocabulary, validation/test from filtered external word lists\n7. Format as a Hugging Face dataset with appropriate splits\n8. Create notebook to visualize dataset statistics\n9. Establish a Hugging Face benchmark with evaluation scripts and leaderboard integration\n\nNOTE: The dataset split is NOT based on percentage. The training set (universal set) comes from tokenizer vocabulary, while validation and test sets (hold-out sets) come from external word lists. This source-based split is essential for the experiment's purpose of determining if training on spelling improves model performance on position and count question metrics.\n\nFile Structure:\n- Raw word lists: `data/raw/word_lists/`\n- Processed word lists: `data/processed/word_lists/`\n- Training set: `data/splits/train.json`\n- Validation set: `data/splits/val.json`\n- Test set: `data/splits/test.json`\n- Question generation scripts: `src/data/question_generator.py` and `src/data/utils.py`\n- Dataset formatting scripts: `src/data/dataset_formatter.py` and `src/data/dataset_builder.py`\n- Documentation: `docs/dataset.md` and `docs/split_methodology.md`\n- Analysis notebooks: `notebooks/dataset_analysis.ipynb` and `notebooks/split_verification.ipynb`\n\nImplementation:\n```python\nimport json\nimport random\nimport string\nimport os\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nimport requests\n\n# Create directories if they don't exist\nos.makedirs(\"data/raw/word_lists\", exist_ok=True)\nos.makedirs(\"data/processed/word_lists\", exist_ok=True)\nos.makedirs(\"data/splits\", exist_ok=True)\n\n# Download English word list\nword_list_url = \"https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt\"\nresponse = requests.get(word_list_url)\nall_words = response.text.splitlines()\n\n# Save raw word list\nwith open(\"data/raw/word_lists/english_words.txt\", \"w\") as f:\n    f.write(\"\\n\".join(all_words))\n\n# Load tokenizer and filtered tokens\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nwith open(\"gpt2_letter_tokens.json\", \"r\") as f:\n    tokens_data = json.load(f)\n\ntokens = [t[\"token\"] for t in tokens_data[\"tokens\"]]\n\n# Save processed tokens\nwith open(\"data/processed/word_lists/tokenizer_vocabulary.json\", \"w\") as f:\n    json.dump({\"tokens\": tokens}, f, indent=2)\n\n# Find valid validation/test words\nvalid_words = []\nfor word in all_words:\n    if not word.isalpha():\n        continue\n    \n    # Tokenize the word\n    token_ids = tokenizer.encode(word, add_special_tokens=False)\n    tokens_in_word = tokenizer.convert_ids_to_tokens(token_ids)\n    \n    # Check if word splits into multiple tokens with at least one multi-character token\n    if len(tokens_in_word) > 1 and any(len(tokenizer.convert_tokens_to_string([t])) > 1 for t in tokens_in_word):\n        # Ensure word is not in training set\n        if word.lower() not in [t.lower() for t in tokens]:\n            valid_words.append(word)\n\n# Save processed valid words\nwith open(\"data/processed/word_lists/valid_external_words.json\", \"w\") as f:\n    json.dump({\"words\": valid_words}, f, indent=2)\n\n# Split valid words into validation and test sets\nval_words, test_words = train_test_split(valid_words, test_size=0.5, random_state=42)\n\n# Generate questions using question_generator.py\nfrom src.data.question_generator import generate_questions\n\n# Generate training, validation, and test questions\ntrain_questions = generate_questions(tokens, \"letter_count\") + generate_questions(tokens, \"letter_position\")\nval_questions = generate_questions(val_words, \"letter_count\") + generate_questions(val_words, \"letter_position\")\ntest_questions = generate_questions(test_words, \"letter_count\") + generate_questions(test_words, \"letter_position\")\n\n# Save splits to JSON files\nwith open(\"data/splits/train.json\", \"w\") as f:\n    json.dump({\"questions\": train_questions}, f, indent=2)\n\nwith open(\"data/splits/val.json\", \"w\") as f:\n    json.dump({\"questions\": val_questions}, f, indent=2)\n\nwith open(\"data/splits/test.json\", \"w\") as f:\n    json.dump({\"questions\": test_questions}, f, indent=2)\n\n# Create datasets using dataset_formatter.py and dataset_builder.py\nfrom src.data.dataset_formatter import format_dataset\nfrom src.data.dataset_builder import build_and_push_dataset\n\n# Format and build the dataset\ndatasets = format_dataset(train_questions, val_questions, test_questions)\ncombined_dataset = build_and_push_dataset(datasets, \"YOUR-USERNAME/llm-spelling-dataset\")\n```",
      "testStrategy": "1. Verify dataset successfully generates 2,000+ questions\n2. Confirm questions are grammatically correct\n3. Verify train/validation/test splits come from appropriate sources (training from tokenizer vocabulary, validation/test from filtered external words)\n4. Manually check 20 random samples to ensure answers correctly match questions\n5. Confirm dataset is successfully pushed to Hugging Face\n6. Verify local JSON files are created for each split in the correct locations:\n   - `data/splits/train.json`\n   - `data/splits/val.json`\n   - `data/splits/test.json`\n7. Create and review notebook `notebooks/dataset_analysis.ipynb` exploring dataset statistics\n8. Test evaluation scripts to ensure they correctly measure performance on position and count question metrics\n9. Verify benchmark integration with Hugging Face leaderboard\n10. Use `notebooks/split_verification.ipynb` to verify there is no overlap between the universal set (training) and hold-out sets (validation/test) to ensure valid measurement of model performance improvements\n11. Check that all documentation files (`docs/dataset.md` and `docs/split_methodology.md`) are complete and accurate",
      "subtasks": [
        {
          "id": 1,
          "title": "Word List Acquisition",
          "description": "Gather a comprehensive and diverse word list from reliable sources, ensuring coverage of the desired vocabulary scope.",
          "dependencies": [],
          "details": "Implementation involves sourcing words from open datasets, dictionaries, or APIs. Validation criteria include checking for duplicates, ensuring language consistency, and verifying word authenticity. Challenges include handling noisy data, inconsistent formats, and ensuring the list is representative of the target domain.\n<info added on 2025-05-07T14:45:31.121Z>\nImplementation involves sourcing words from open datasets, dictionaries, or APIs. Validation criteria include checking for duplicates, ensuring language consistency, and verifying word authenticity. Challenges include handling noisy data, inconsistent formats, and ensuring the list is representative of the target domain.\n\nThis task is broken down into three subtasks that can be executed in parallel:\n\n1. Sourcing: Identify and collect words from multiple reliable sources such as open datasets, dictionaries, APIs, and academic word lists. Focus on gathering a comprehensive set that covers the desired vocabulary scope. This task can be worked on independently and in parallel with others. (parallelizable: true)\n\n2. Cleaning: Process the collected words to remove duplicates, standardize formats, handle special characters, and ensure consistent casing. Address any encoding issues and normalize variations of the same word. This task can be worked on independently and in parallel with others. (parallelizable: true)\n\n3. Validation: Verify the authenticity and appropriateness of words in the list. Check for language consistency, filter out inappropriate content, and ensure the words meet the project's requirements. This task can be worked on independently and in parallel with others. (parallelizable: true)\n\nThe overall Word List Acquisition task is parallelizable, with team members able to work on different subtasks simultaneously to improve efficiency.\n</info added on 2025-05-07T14:45:31.121Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Training/Validation/Test Set Creation with Filtering",
          "description": "Create distinct datasets from different sources: training set from tokenizer vocabulary and validation/test sets from filtered external word lists to establish a true holdout set for testing.",
          "dependencies": [
            1
          ],
          "details": "Implementation requires extracting tokenizer vocabulary for training and applying strict filtering criteria to external word lists for validation/test sets. Ensure no overlap between training tokens and validation/test words. Validation involves statistical checks for distribution balance and manual spot checks for leakage. Challenges include maintaining diversity across splits and implementing robust filtering logic.\n\nFile Structure:\n- Raw word lists stored in: `data/raw/word_lists/`\n- Processed word lists stored in: `data/processed/word_lists/`\n- Training set saved to: `data/splits/train.json`\n- Validation set saved to: `data/splits/val.json`\n- Test set saved to: `data/splits/test.json`\n\nVerification of splits should be documented in `notebooks/split_verification.ipynb`.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Question Generation for Each Type",
          "description": "Automatically generate letter count and letter position questions for each word in the dataset to establish metrics for evaluating model performance.",
          "dependencies": [
            2
          ],
          "details": "Implementation uses templates to generate questions per word for both letter count and letter position types. Validation includes checking for grammatical correctness, relevance, and uniqueness of questions. Challenges involve ensuring variety in question phrasing and scaling generation efficiently across the universal set and holdout set.\n\nImplementation should be in:\n- Main script: `src/data/question_generator.py`\n- Utility functions: `src/data/utils.py`\n\nThe generated questions will be stored in the split files:\n- `data/splits/train.json`\n- `data/splits/val.json`\n- `data/splits/test.json`",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Dataset Formatting and Splitting",
          "description": "Format the generated data according to Hugging Face requirements, ensuring proper structure and metadata for each split based on their distinct sources.",
          "dependencies": [
            3
          ],
          "details": "Implementation involves structuring data as JSON, CSV, or other required formats, with clear fields for input, output, and metadata. Validation checks include schema compliance, correct split assignments, and tokenization compatibility. Challenges include handling edge cases in formatting and ensuring compatibility with downstream tools. Note that splits are based on source (not percentage): training uses tokenizer vocabulary while validation/test use external word lists.\n\nImplementation should use:\n- Main formatting script: `src/data/dataset_formatter.py`\n- HuggingFace dataset script: `src/data/dataset_builder.py`\n\nThe formatted datasets should be saved to:\n- `data/splits/train.json`\n- `data/splits/val.json`\n- `data/splits/test.json`",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Dataset Publishing and Benchmark Creation",
          "description": "Publish the finalized dataset to Hugging Face and establish a benchmark with evaluation scripts and leaderboard integration.",
          "dependencies": [
            4
          ],
          "details": "Implementation includes uploading dataset files, creating evaluation scripts that measure performance on position and count question metrics, integrating with Hugging Face leaderboard, and writing detailed documentation. Validation involves verifying downloadability, documentation clarity, and reproducibility. Challenges include ensuring evaluation scripts accurately reflect the experiment's purpose of determining if training on spelling improves model performance.\n\nDocumentation should be created in:\n- `docs/dataset.md` - General dataset documentation\n- `docs/split_methodology.md` - Detailed explanation of the split methodology\n\nDataset analysis should be performed in:\n- `notebooks/dataset_analysis.ipynb`",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Universal Set and Holdout Set Verification",
          "description": "Verify that the training set (universal set) and test set (true holdout set) are properly separated to enable valid measurement of model performance improvements.",
          "dependencies": [
            2
          ],
          "details": "Implementation involves comprehensive checks to ensure no overlap between training tokens and test words. Create verification scripts to confirm the integrity of the splits. Validation includes statistical analysis of word distributions and characteristics across splits. Challenges include defining appropriate metrics to verify the splits serve the experiment's purpose of determining if training on spelling improves model performance on position and count question metrics.\n\nVerification should be performed and documented in:\n- `notebooks/split_verification.ipynb`\n\nThis notebook should analyze the splits stored in:\n- `data/splits/train.json`\n- `data/splits/val.json`\n- `data/splits/test.json`",
          "status": "done"
        },
        {
          "id": 7,
          "title": "Source-Based Split Documentation",
          "description": "Document the source-based split approach and its importance for the experiment's validity.",
          "dependencies": [
            2,
            4
          ],
          "details": "Create clear documentation explaining why the dataset uses a source-based split (training from tokenizer vocabulary, validation/test from external word lists) rather than a percentage-based split. Explain how this approach creates a true universal set and hold-out set, which is essential for validly measuring if training on spelling improves model performance on position and count question metrics.\n\nDocumentation should be created in:\n- `docs/split_methodology.md` - Detailed explanation of the split methodology\n- `docs/dataset.md` - General dataset documentation with references to the split methodology\n\nThis documentation should also be included in the dataset card when publishing to Hugging Face.",
          "status": "done"
        }
      ]
    },
    {
      "id": 4,
      "title": "Training Data Formatting with Template Variations",
      "description": "Format the training data using various template formats for spelling examples to maximize LLM generalization and token-awareness.",
      "status": "done",
      "dependencies": [
        3
      ],
      "priority": "medium",
      "details": "1. Create a script to format the training data for fine-tuning\n2. Implement the template variations specified in the PRD, including:\n   - Simple variations (spelling first)\n   - Narrative/playful versions (spelling first)\n   - Educational/formal tone (spelling first)\n   - Spoken word/emphatic style (spelling first)\n   - Simple variations (word first)\n   - Narrative/playful versions (word first)\n   - Educational/formal tone (word first)\n   - Spoken word/emphatic style (word first)\n   - LLM-friendly structured training format (no \"spell\")\n3. Include additional variations for token separation:\n   - No separator between tokens\n   - Arrows between tokens\n   - Various punctuation and formatting\n4. Create Python scripts for analysis and visualization\n5. Implement efficient DataLoader with proper batching\n\nFile Structure:\n- Template definitions: `configs/templates/`\n- Template categories: `configs/templates/categories.json`\n- Token separation: `src/data/token_separator.py`\n- Template processor: `src/data/template_processor.py`\n- Example generator: `src/data/example_generator.py`\n- Data loader: `src/data/data_loader.py`\n- Formatted training data: `data/processed/training_data/`\n- Template variations: `data/processed/template_variations/`\n- Analysis scripts: `src/analysis/template_analysis.py`\n- Performance analysis: `src/analysis/template_performance.py`\n- Visualization utilities: `src/analysis/visualization_utils.py`\n- Report generator: `src/analysis/report_generator.py`\n- Results output: `results/token_analysis/`\n- Template documentation: `docs/templates.md`\n- Data format specification: `docs/data_format.md`\n\nImplementation:\n```python\ndef format_training_examples(dataset):\n    formatted_examples = []\n    \n    # Template categories\n    templates = {\n        \"spelling_first_simple\": [\n            \"s t r a w — that spells '{word}.'\\n\",\n            \"The letters s, t, r, a, w spell the word '{word}.'\\n\",\n            \"s-t-r-a-w makes the word '{word}.'\\n\",\n            \"Put together, s t r a w spells {word}.\\n\",\n            \"When you combine s, t, r, a, and w, you get {word}.\\n\"\n        ],\n        \"spelling_first_playful\": [\n            \"Say it with me: s...t...r...a...w — {word}!\\n\",\n            \"Five little letters — s, t, r, a, w — team up to make '{word}.'\\n\",\n            \"You line up s, t, r, a, and w, and what do you get? {word}!\\n\",\n            \"It starts with an 's' and ends with a 'w' — that's '{word}.'\\n\",\n            \"One letter at a time: s, t, r, a, w. Together? {word}.\\n\"\n        ],\n        # Add all other template categories from the PRD\n    }\n    \n    # Token separation styles\n    separators = [\n        \"\", # No separator\n        \" \", # Space\n        \", \", # Comma and space\n        \"-\", # Dash\n        \"...\", # Triple dots\n        \" -> \" # Arrow\n    ]\n    \n    for example in dataset:\n        word = example[\"word\"]\n        letters = list(word)\n        \n        # Randomly select template category and template\n        category = random.choice(list(templates.keys()))\n        template = random.choice(templates[category])\n        \n        # Randomly select separator\n        separator = random.choice(separators)\n        \n        # Format the letters with the chosen separator\n        spelled_letters = separator.join(letters)\n        \n        # Format the example using the template\n        formatted_text = template.format(word=word, letters=spelled_letters)\n        \n        formatted_examples.append({\n            \"input\": formatted_text,\n            \"output\": word,\n            \"template_category\": category,\n            \"separator\": separator\n        })\n    \n    return formatted_examples\n\n# Create custom collation function for efficient batching\ndef custom_collate_fn(batch):\n    input_ids = [item[\"input_ids\"] for item in batch]\n    attention_mask = [item[\"attention_mask\"] for item in batch]\n\n    # Pad sequences to the maximum length in the batch\n    max_length = max(len(ids) for ids in input_ids)\n\n    # Pad input_ids and attention_mask\n    input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n    attention_mask = [mask + [0] * (max_length - len(mask)) for mask in attention_mask]\n\n    # Convert to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_mask = torch.tensor(attention_mask)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask\n    }\n```",
      "testStrategy": "1. Verify script runs without errors\n2. Confirm dataset contains all template variations specified in the PRD\n3. Check that examples use a mix of punctuation and formatting\n4. Ensure no template is over-represented\n5. Test analysis scripts with sample data\n6. Test DataLoader with custom collation function\n7. Verify efficient batching with varied text lengths\n8. Validate that all files are created in the correct locations:\n   - Check template files in `configs/templates/`\n   - Verify processed data in `data/processed/training_data/`\n   - Ensure analysis scripts produce expected outputs in `results/token_analysis/`\n9. Test the complete pipeline from template processing to data loading\n10. Verify HTML reports are generated correctly\n11. Test command-line interfaces for analysis scripts",
      "subtasks": [
        {
          "id": 1,
          "title": "Template Design and Categorization",
          "description": "Create and categorize various template formats for training data based on different use cases and model requirements",
          "dependencies": [],
          "details": "Develop a comprehensive template system that supports various data types (text, images, audio, video). Create templates for different ML tasks and ensure they follow best practices for data formatting. Categorize templates based on complexity, use case, and required model architecture. Quality metrics should include template coverage, flexibility, and adherence to formatting standards. Test by validating templates with sample data across different domains.\n<info added on 2025-05-07T20:23:50.045Z>\nDevelop a comprehensive template system that supports various data types (text, images, audio, video). Create templates for different ML tasks and ensure they follow best practices for data formatting. Categorize templates based on complexity, use case, and required model architecture. Quality metrics should include template coverage, flexibility, and adherence to formatting standards. Test by validating templates with sample data across different domains.\n\nThe template design and categorization has been completed with the following structure:\n\n1. Template Categories:\n   - Spelling-first templates with variations: simple, playful, educational, and emphatic styles\n   - Word-first templates with variations: simple, playful, educational, and emphatic styles\n   - Structured templates: token-based and JSON-like formats\n\n2. Documentation:\n   - templates.md: Provides a comprehensive overview of the template system, categories, and usage guidelines\n   - data_format.md: Contains detailed specifications for data formats and processing guidelines\n\n3. Template Implementation Details:\n   - Multiple formatting styles implemented for each category\n   - Various token separation methods defined (to be implemented in next subtask)\n   - Structured formats designed specifically for machine learning applications\n   - Consistent variable substitution patterns established across all templates\n\n4. Project Organization:\n   - Template configurations stored in configs/templates/ directory\n   - Categories defined in configs/templates/categories.json\n   - Documentation placed in docs/ directory\n   - Clear file structure established for implementation phase\n\nThe template system is now fully designed and categorized, providing a solid foundation for the token separation strategy implementation in the next subtask.\n</info added on 2025-05-07T20:23:50.045Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Token Separation Strategy Implementation",
          "description": "Develop and implement various token separation strategies for different data types and model requirements",
          "dependencies": [
            1
          ],
          "details": "Research and implement multiple token separation approaches (whitespace, subword, character-level, etc.). Create a configurable system that allows switching between strategies based on language or data type. Develop custom tokenization rules for domain-specific data. Quality metrics should include tokenization accuracy, processing speed, and vocabulary coverage. Test with diverse multilingual datasets and measure impact on model performance. Implement in `src/data/token_separator.py`.\n<info added on 2025-05-07T20:26:12.086Z>\nResearch and implement multiple token separation approaches (whitespace, subword, character-level, etc.). Create a configurable system that allows switching between strategies based on language or data type. Develop custom tokenization rules for domain-specific data. Quality metrics should include tokenization accuracy, processing speed, and vocabulary coverage. Test with diverse multilingual datasets and measure impact on model performance. Implement in `src/data/token_separator.py`.\n\nThe TokenSeparator class has been successfully implemented in src/data/token_separator.py with the following features:\n\n1. Multiple built-in separator styles:\n   - none: tokens without separators\n   - space: tokens separated by spaces\n   - comma: tokens separated by commas\n   - dash: tokens separated by dashes\n   - dots: tokens separated by dots\n   - arrow: tokens separated by arrows\n\n2. A flexible SeparatorConfig dataclass that provides configuration options:\n   - Style selection from predefined styles\n   - Support for custom separator strings\n   - Control over spacing around separators\n   - Token capitalization options\n\n3. Utility functions to enhance usability:\n   - get_all_separator_examples(): Generates examples using all available styles\n   - create_custom(): Creates separators with custom configuration\n   - get_random_separator(): Selects a random style for variety in outputs\n\n4. A comprehensive test script (scripts/test_token_separator.py) that demonstrates:\n   - All built-in separator styles in action\n   - How to use custom separators\n   - Random style selection functionality\n   - Proper token processing workflow\n\n5. Testing with sample tokens confirms:\n   - All separator styles function as expected\n   - Proper spacing and formatting is maintained\n   - Custom separator functionality works correctly\n   - Random style selection provides appropriate variation\n\nThe implementation is now ready for integration with the template processor in the next subtask (Dynamic Example Generation System).\n</info added on 2025-05-07T20:26:12.086Z>",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Dynamic Example Generation System",
          "description": "Build a system that can dynamically generate training examples with appropriate variations and augmentations",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement data augmentation techniques for different data types (text rotation, image transformation, etc.). Create a pipeline for generating variations of training examples to prevent overfitting. Develop rules for maintaining data balance across classes. Quality metrics should include variation diversity, generation speed, and class distribution balance. Test by measuring model performance improvements with augmented data versus baseline. Implement in `src/data/example_generator.py` and store outputs in `data/processed/template_variations/`.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Efficient Data Loading and Batching",
          "description": "Optimize data loading and batching processes for improved training efficiency",
          "dependencies": [
            2,
            3
          ],
          "details": "Implement efficient data loading mechanisms that minimize memory usage and processing time. Develop smart batching strategies that group similar-length sequences together. Create data splitting functionality for training, validation, and testing sets. Quality metrics should include loading speed, memory efficiency, and training throughput. Test by benchmarking different loading approaches and measuring impact on training time. Implement in `src/data/data_loader.py`.",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Template Variation Analysis and Visualization",
          "description": "Analyze and visualize the effectiveness of different template variations on model performance",
          "dependencies": [
            1,
            3,
            4
          ],
          "details": "Develop Python scripts to analyze and visualize how different template designs affect model training. Create metrics to quantify template effectiveness across different data types and tasks. Implement automated analysis to recommend optimal template configurations. Quality metrics should include visualization clarity, analysis accuracy, and recommendation relevance. Test by comparing model performance across different template variations and validating analysis results.\n\nImplement the following scripts:\n- `src/analysis/template_analysis.py`: Main analysis script with command-line interface\n- `src/analysis/template_performance.py`: Performance analysis across template variations\n- `src/analysis/visualization_utils.py`: Shared plotting utilities for consistent visualization\n- `src/analysis/report_generator.py`: HTML report generation for easy sharing of results\n\nOutput structure:\n- `results/token_analysis/figures/`: All PNG/PDF visualizations\n- `results/token_analysis/reports/`: HTML reports\n- `results/token_analysis/data/`: Processed CSV/JSON data\n\nEnsure all scripts have proper command-line interfaces, documentation, and error handling.",
          "status": "done"
        },
        {
          "id": 6,
          "title": "Documentation and File Structure Setup",
          "description": "Create and organize the file structure and documentation for template variations and data formatting",
          "dependencies": [],
          "details": "Set up the required directory structure for template files, implementation files, output files, and analysis files. Create comprehensive documentation in `docs/templates.md` and `docs/data_format.md` explaining the template system, data formats, and usage guidelines. Ensure all file paths are correctly referenced in the implementation code. Test by verifying that all directories exist and documentation is complete and accurate.",
          "status": "done"
        },
        {
          "id": 7,
          "title": "Analysis Scripts and Results Structure Setup",
          "description": "Set up the Python script-based analysis system and results directory structure",
          "dependencies": [
            6
          ],
          "details": "Create the necessary directory structure for analysis scripts and results output:\n\n1. Create the following directories:\n   - `src/analysis/` for all analysis scripts\n   - `results/token_analysis/figures/` for visualizations\n   - `results/token_analysis/reports/` for HTML reports\n   - `results/token_analysis/data/` for processed analysis data\n\n2. Set up script templates with proper imports, documentation, and command-line interfaces:\n   - `src/analysis/template_analysis.py`\n   - `src/analysis/template_performance.py`\n   - `src/analysis/visualization_utils.py`\n   - `src/analysis/report_generator.py`\n\n3. Implement basic functionality in each script:\n   - Command-line argument parsing\n   - Configuration loading\n   - Logging setup\n   - Error handling\n   - Basic documentation\n\n4. Create unit tests for each script to verify basic functionality\n\n5. Update documentation to reflect the new script-based analysis approach",
          "status": "done"
        }
      ]
    },
    {
      "id": 5,
      "title": "Baseline Model Evaluation",
      "description": "Evaluate the Qwen3-4B model on letter count and position tasks to establish a performance baseline for transfer learning effectiveness, leveraging Lightning.AI Studios for efficient computation and experiment tracking.",
      "status": "pending",
      "dependencies": [
        3
      ],
      "priority": "medium",
      "details": "1. Set up DsPy for multi-shot prompting\n2. Create Python scripts to evaluate the Qwen3-4B model\n3. Test ONLY on position and character count metrics using multi-token words\n4. Ensure output format is correct (single integer for character count or single letter for character position)\n5. Document the baseline performance for comparison\n\nQwen3-4B Configuration:\n- Configure model with enable_thinking=False (non-thinking mode only)\n- Focus evaluation on English-only token subset\n\nLightning.AI Studios Integration:\n- Create a dedicated evaluation Studio following the \"one Studio, one task\" principle\n- Utilize GPU switching feature (CPU → T4 → A100) for cost-effective evaluation\n- Ensure sufficient memory allocation for Qwen3-4B model requirements\n- Leverage Lightning.AI plugins for experiment tracking and visualization\n- Use shared filesystem for accessing models and datasets\n\nData Sources:\n- Training data: Spelling variations with multicharacter tokens (for training only)\n- Evaluation data: Multi-token words for position and character count questions ONLY\n\nEvaluation Approach:\n- Focus on measuring transfer learning effectiveness from spelling training to position/count tasks\n- No traditional train/val/test split since evaluation uses different data\n- Separate evaluation pipeline for position and count metrics ONLY\n- Implement evaluation metrics using TorchMetrics for optimized distributed evaluation\n\nFile Structure:\n- Main framework: `src/evaluation/framework.py`\n- Metrics definitions: `src/evaluation/metrics.py`\n- Evaluation config: `configs/evaluation/base_config.yaml`\n- Lightning.AI Studio config: `configs/evaluation/lightning_studio.yaml`\n- Letter count evaluator: `src/evaluation/letter_count.py`\n- Letter position evaluator: `src/evaluation/letter_position.py`\n- Common utilities: `src/evaluation/utils.py`\n- Visualization utilities: `src/evaluation/visualization.py`\n- Report generation: `src/evaluation/report.py`\n- Results directory: `results/evaluation/`\n- Visualizations: `results/evaluation/figures/`\n- HTML reports: `results/evaluation/reports/`\n- Processed data: `results/evaluation/data/`\n- Raw metrics: `results/evaluation/data/metrics.json`\n- Detailed analysis: `results/evaluation/data/analysis.json`\n\nImplementation:\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport dspy\nimport wandb\nimport argparse\nimport os\nimport json\nimport pandas as pd\nimport torchmetrics\nfrom lightning.pytorch import loggers as pl_loggers\nfrom lightning.app import LightningApp, LightningWork\nfrom lightning.app.components import TracerPythonScript\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Evaluate Qwen3-4B model on letter count and position tasks\")\n    parser.add_argument(\"--model\", type=str, default=\"Qwen/Qwen3-4B\", help=\"Model to evaluate\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"results/evaluation\", help=\"Directory to save results\")\n    parser.add_argument(\"--log_wandb\", action=\"store_true\", help=\"Whether to log results to W&B\")\n    parser.add_argument(\"--use_lightning\", action=\"store_true\", help=\"Whether to use Lightning.AI for distributed evaluation\")\n    parser.add_argument(\"--gpu_tier\", type=str, default=\"T4\", choices=[\"CPU\", \"T4\", \"A100\"], help=\"GPU tier to use for evaluation\")\n    return parser.parse_args()\n\nclass EvaluationWork(LightningWork):\n    def __init__(self, model_name, output_dir, log_wandb=False, gpu_tier=\"T4\"):\n        super().__init__(cloud_compute={\"gpu_type\": gpu_tier.lower() if gpu_tier != \"CPU\" else None})\n        self.model_name = model_name\n        self.output_dir = output_dir\n        self.log_wandb = log_wandb\n        \n    def run(self):\n        # Create output directories\n        os.makedirs(f\"{self.output_dir}/data\", exist_ok=True)\n        os.makedirs(f\"{self.output_dir}/figures\", exist_ok=True)\n        os.makedirs(f\"{self.output_dir}/reports\", exist_ok=True)\n        \n        # Initialize W&B if requested\n        if self.log_wandb:\n            wandb.init(project=\"llm-spelling-finetuning\", name=\"qwen3-baseline-evaluation\")\n        \n        # Load Qwen3-4B model and tokenizer\n        model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map=\"auto\", trust_remote_code=True)\n        model.eval()\n        tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n        \n        # Load evaluation dataset with multi-token words\n        from datasets import load_dataset\n        eval_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\", split=\"evaluation\")\n        \n        # Initialize TorchMetrics for evaluation\n        letter_count_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=20)  # Assuming max 20 characters\n        letter_position_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=26)  # A-Z positions\n        \n        # Define generation function with Qwen3-specific parameters\n        def generate_answer(model, tokenizer, question, max_length=10):\n            inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n            \n            # Configure generation parameters (non-thinking mode only)\n            generation_config = {\n                \"max_new_tokens\": max_length,\n                \"do_sample\": False,\n                \"pad_token_id\": tokenizer.pad_token_id,\n                \"enable_thinking\": False  # Explicitly disable thinking mode\n            }\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    inputs.input_ids,\n                    **generation_config\n                )\n            \n            response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n            \n            # Extract just the first token/character for letter position or first number for letter count\n            if \"How many\" in question:\n                # Extract first number\n                import re\n                numbers = re.findall(r'\\d+', response)\n                return numbers[0] if numbers else response.strip()\n            else:\n                # Extract first character\n                return response.strip()[0] if response.strip() else \"\"\n        \n        # Evaluate on letter count questions\n        def evaluate_letter_count(model, tokenizer, dataset):\n            correct = 0\n            total = 0\n            results = []\n            \n            for item in dataset:\n                if item[\"question_type\"] != \"letter_count\":\n                    continue\n                    \n                prediction = generate_answer(model, tokenizer, item[\"question\"])\n                is_correct = prediction == item[\"answer\"]\n                \n                results.append({\n                    \"question\": item[\"question\"],\n                    \"expected\": item[\"answer\"],\n                    \"prediction\": prediction,\n                    \"correct\": is_correct\n                })\n                \n                correct += int(is_correct)\n                total += 1\n            \n            accuracy = correct / total if total > 0 else 0\n            print(f\"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})\")\n            \n            return accuracy, results\n        \n        # Evaluate on letter position questions\n        def evaluate_letter_position(model, tokenizer, dataset):\n            correct = 0\n            total = 0\n            results = []\n            \n            for item in dataset:\n                if item[\"question_type\"] != \"letter_position\":\n                    continue\n                    \n                prediction = generate_answer(model, tokenizer, item[\"question\"])\n                is_correct = prediction.lower() == item[\"answer\"].lower()\n                \n                results.append({\n                    \"question\": item[\"question\"],\n                    \"expected\": item[\"answer\"],\n                    \"prediction\": prediction,\n                    \"correct\": is_correct\n                })\n                \n                correct += int(is_correct)\n                total += 1\n            \n            accuracy = correct / total if total > 0 else 0\n            print(f\"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})\")\n            \n            return accuracy, results\n        \n        # Run evaluation - only for count and position tasks\n        count_accuracy, count_results = evaluate_letter_count(model, tokenizer, eval_dataset)\n        position_accuracy, position_results = evaluate_letter_position(model, tokenizer, eval_dataset)\n        \n        # Log results to W&B if requested\n        if self.log_wandb:\n            wandb.log({\n                \"letter_count_accuracy\": count_accuracy,\n                \"letter_position_accuracy\": position_accuracy,\n                \"count_examples\": wandb.Table(dataframe=pd.DataFrame(count_results[:20])),\n                \"position_examples\": wandb.Table(dataframe=pd.DataFrame(position_results[:20]))\n            })\n        \n        # Save results locally\n        metrics_data = {\n            \"model\": self.model_name,\n            \"letter_count_accuracy\": count_accuracy,\n            \"letter_position_accuracy\": position_accuracy,\n            \"count_results\": count_results,\n            \"position_results\": position_results\n        }\n        \n        with open(f\"{self.output_dir}/data/metrics.json\", \"w\") as f:\n            json.dump(metrics_data, f, indent=2)\n        \n        # Generate visualizations\n        from src.evaluation.visualization import create_accuracy_chart, create_error_analysis\n        create_accuracy_chart(metrics_data, f\"{self.output_dir}/figures/accuracy.png\")\n        create_error_analysis(metrics_data, f\"{self.output_dir}/figures/error_analysis.png\")\n        \n        # Generate HTML report\n        from src.evaluation.report import generate_html_report\n        generate_html_report(metrics_data, f\"{self.output_dir}/reports/baseline_report.html\")\n        \n        print(f\"Evaluation complete. Results saved to {self.output_dir}\")\n\ndef main():\n    args = parse_args()\n    \n    if args.use_lightning:\n        # Use Lightning.AI for distributed evaluation\n        eval_work = EvaluationWork(\n            model_name=args.model,\n            output_dir=args.output_dir,\n            log_wandb=args.log_wandb,\n            gpu_tier=args.gpu_tier\n        )\n        app = LightningApp(eval_work)\n        app.run()\n    else:\n        # Create output directories\n        os.makedirs(f\"{args.output_dir}/data\", exist_ok=True)\n        os.makedirs(f\"{args.output_dir}/figures\", exist_ok=True)\n        os.makedirs(f\"{args.output_dir}/reports\", exist_ok=True)\n        \n        # Initialize W&B if requested\n        if args.log_wandb:\n            wandb.init(project=\"llm-spelling-finetuning\", name=\"qwen3-baseline-evaluation\")\n        \n        # Load Qwen3-4B model and tokenizer\n        model_name = args.model\n        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n        model.eval()\n        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        \n        # Load evaluation dataset with multi-token words\n        from datasets import load_dataset\n        eval_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\", split=\"evaluation\")\n        \n        # Define generation function with Qwen3-specific parameters\n        def generate_answer(model, tokenizer, question, max_length=10):\n            inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n            \n            # Configure generation parameters (non-thinking mode only)\n            generation_config = {\n                \"max_new_tokens\": max_length,\n                \"do_sample\": False,\n                \"pad_token_id\": tokenizer.pad_token_id,\n                \"enable_thinking\": False  # Explicitly disable thinking mode\n            }\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    inputs.input_ids,\n                    **generation_config\n                )\n            \n            response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n            \n            # Extract just the first token/character for letter position or first number for letter count\n            if \"How many\" in question:\n                # Extract first number\n                import re\n                numbers = re.findall(r'\\d+', response)\n                return numbers[0] if numbers else response.strip()\n            else:\n                # Extract first character\n                return response.strip()[0] if response.strip() else \"\"\n        \n        # Evaluate on letter count questions\n        def evaluate_letter_count(model, tokenizer, dataset):\n            correct = 0\n            total = 0\n            results = []\n            \n            for item in dataset:\n                if item[\"question_type\"] != \"letter_count\":\n                    continue\n                    \n                prediction = generate_answer(model, tokenizer, item[\"question\"])\n                is_correct = prediction == item[\"answer\"]\n                \n                results.append({\n                    \"question\": item[\"question\"],\n                    \"expected\": item[\"answer\"],\n                    \"prediction\": prediction,\n                    \"correct\": is_correct\n                })\n                \n                correct += int(is_correct)\n                total += 1\n            \n            accuracy = correct / total if total > 0 else 0\n            print(f\"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})\")\n            \n            return accuracy, results\n        \n        # Evaluate on letter position questions\n        def evaluate_letter_position(model, tokenizer, dataset):\n            correct = 0\n            total = 0\n            results = []\n            \n            for item in dataset:\n                if item[\"question_type\"] != \"letter_position\":\n                    continue\n                    \n                prediction = generate_answer(model, tokenizer, item[\"question\"])\n                is_correct = prediction.lower() == item[\"answer\"].lower()\n                \n                results.append({\n                    \"question\": item[\"question\"],\n                    \"expected\": item[\"answer\"],\n                    \"prediction\": prediction,\n                    \"correct\": is_correct\n                })\n                \n                correct += int(is_correct)\n                total += 1\n            \n            accuracy = correct / total if total > 0 else 0\n            print(f\"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})\")\n            \n            return accuracy, results\n        \n        # Run evaluation - only for count and position tasks\n        count_accuracy, count_results = evaluate_letter_count(model, tokenizer, eval_dataset)\n        position_accuracy, position_results = evaluate_letter_position(model, tokenizer, eval_dataset)\n        \n        # Log results to W&B if requested\n        if args.log_wandb:\n            wandb.log({\n                \"letter_count_accuracy\": count_accuracy,\n                \"letter_position_accuracy\": position_accuracy,\n                \"count_examples\": wandb.Table(dataframe=pd.DataFrame(count_results[:20])),\n                \"position_examples\": wandb.Table(dataframe=pd.DataFrame(position_results[:20]))\n            })\n        \n        # Save results locally\n        metrics_data = {\n            \"model\": model_name,\n            \"letter_count_accuracy\": count_accuracy,\n            \"letter_position_accuracy\": position_accuracy,\n            \"count_results\": count_results,\n            \"position_results\": position_results\n        }\n        \n        with open(f\"{args.output_dir}/data/metrics.json\", \"w\") as f:\n            json.dump(metrics_data, f, indent=2)\n        \n        # Generate visualizations\n        from src.evaluation.visualization import create_accuracy_chart, create_error_analysis\n        create_accuracy_chart(metrics_data, f\"{args.output_dir}/figures/accuracy.png\")\n        create_error_analysis(metrics_data, f\"{args.output_dir}/figures/error_analysis.png\")\n        \n        # Generate HTML report\n        from src.evaluation.report import generate_html_report\n        generate_html_report(metrics_data, f\"{args.output_dir}/reports/baseline_report.html\")\n        \n        print(f\"Evaluation complete. Results saved to {args.output_dir}\")\n\nif __name__ == \"__main__\":\n    main()\n```",
      "testStrategy": "1. Verify the evaluation scripts run without errors with Qwen3-4B model\n2. Test command-line arguments for flexibility, including Lightning.AI specific arguments\n3. Confirm output format is correct (single integer for count, single letter for position)\n4. Check that results are properly logged to W&B when specified\n5. Verify baseline performance metrics are saved to `results/evaluation/data/metrics.json`\n6. Ensure visualizations are correctly generated in `results/evaluation/figures/`\n7. Validate HTML reports are generated in `results/evaluation/reports/`\n8. Test error handling for edge cases\n9. Analyze error patterns in Qwen3-4B model predictions and document in `results/evaluation/data/analysis.json`\n10. Ensure documentation is complete in `docs/evaluation.md`, `docs/metrics.md`, `docs/baseline_results.md`, and `docs/lightning_studio_setup.md`\n11. Verify that the Python scripts can be imported and used as modules by other components\n12. Confirm the evaluation correctly uses multi-token words for position and count tasks ONLY\n13. Validate that the evaluation framework properly measures transfer learning effectiveness\n14. Test for correlation analysis between spelling training and position/count task performance\n15. Verify Lightning.AI Studio setup and configuration works correctly with Qwen3-4B's memory requirements\n16. Test GPU switching functionality (CPU → T4 → A100) for cost optimization\n17. Validate TorchMetrics integration for distributed evaluation\n18. Test automated job submission for batch evaluations\n19. Verify environment isolation and dependency management in Lightning.AI Studio\n20. Verify that thinking mode is never enabled in any configuration or code path\n21. Test English-only token subset focus in evaluation\n22. Verify no spelling evaluation is included in any part of the evaluation process\n23. Confirm all metrics, reports, and visualizations focus exclusively on position and count tasks",
      "subtasks": [
        {
          "id": 1,
          "title": "Evaluation Framework Setup",
          "description": "Establish the hierarchical evaluation framework structure for NLP model assessment",
          "dependencies": [],
          "details": "Create a modular evaluation framework that supports both automated and human evaluation components. Implement a transfer learning evaluation approach using multi-token words for position and count tasks only. Ensure the framework can handle diverse linguistic structures and edge cases. Set up configuration files for evaluation parameters and thresholds.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Metrics Definition and Implementation",
          "description": "Define and implement comprehensive evaluation metrics for model assessment",
          "dependencies": [
            1
          ],
          "details": "Implement position accuracy and count accuracy metrics for multi-token words. Add specialized metrics for transfer learning effectiveness evaluation. Create a metrics registry system that allows for easy addition of new metrics. Ensure all metrics are properly documented with mathematical formulations. Implement analysis of error patterns and potential correlation with spelling training.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Letter Count Evaluator Implementation",
          "description": "Develop specialized evaluator for letter count assessment",
          "dependencies": [
            2
          ],
          "details": "Implement a dedicated evaluator that analyzes the model's ability to count letters in multi-token words. Create test cases with varying complexity levels. Implement error tolerance thresholds. Design the evaluator to track performance across different text lengths. Include detailed logging of evaluation results for later analysis of transfer learning effectiveness.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Position Evaluator Implementation",
          "description": "Develop specialized evaluator for letter position assessment",
          "dependencies": [
            2
          ],
          "details": "Create an evaluator that tests the model's ability to identify letter positions within multi-token words. Implement position-based metrics including absolute and relative position accuracy. Design test cases with varying complexity. Include support for different character sets. Implement detailed error tracking for position-based mistakes to analyze transfer learning effectiveness.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Results Visualization System",
          "description": "Develop comprehensive visualization tools for evaluation results",
          "dependencies": [
            2,
            3,
            4
          ],
          "details": "Create interactive dashboards showing performance across position and count metrics only. Implement comparison visualizations between model versions. Design visualizations to show transfer learning effectiveness from spelling training to position/count tasks. Include error distribution visualizations. Ensure all visualizations are exportable in multiple formats (PNG, PDF, interactive HTML).",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Error Analysis Framework",
          "description": "Implement systematic error analysis capabilities",
          "dependencies": [
            3,
            4,
            5
          ],
          "details": "Develop tools to categorize and analyze error patterns for position and count tasks only. Create clustering algorithms to group similar errors. Implement analysis of correlation between spelling training and position/count task performance. Design interfaces for domain experts to review and annotate errors. Include recommendation generation for model improvements based on error patterns.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Documentation and Reporting",
          "description": "Create comprehensive documentation and automated reporting",
          "dependencies": [
            1,
            2,
            5,
            6
          ],
          "details": "Document the entire evaluation framework architecture with focus on transfer learning approach from spelling training to position/count tasks. Create user guides for running evaluations. Implement automated report generation with executive summaries and detailed technical appendices. Include benchmark comparisons against industry standards. Design templates for different stakeholder audiences (technical, management, etc.).",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Integration with External Components",
          "description": "Ensure seamless integration with other system components",
          "dependencies": [
            1,
            2,
            5,
            7
          ],
          "details": "Develop APIs for integration with model training pipelines. Implement webhooks for continuous evaluation triggers. Create data exchange formats for evaluation results. Design integration with CI/CD pipelines for automated testing. Implement monitoring capabilities to track transfer learning effectiveness over time.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Transfer Learning Effectiveness Analysis",
          "description": "Implement analysis of transfer learning from spelling training to position/count tasks",
          "dependencies": [
            3,
            4,
            6
          ],
          "details": "Develop metrics to quantify transfer learning effectiveness. Create visualization tools to show correlation between spelling training and position/count task performance. Implement statistical analysis to identify significant patterns. Design experiments to test different transfer learning hypotheses. Document findings in comprehensive reports.",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "Lightning.AI Studio Setup",
          "description": "Configure dedicated Lightning.AI Studio for evaluation tasks",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated evaluation Studio following the \"one Studio, one task\" principle. Configure proper environment isolation and dependency management. Set up shared filesystem access for models and datasets. Implement GPU switching functionality (CPU → T4 → A100) for cost optimization. Create configuration files for Lightning.AI Studio setup.",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "TorchMetrics Integration",
          "description": "Implement evaluation metrics using TorchMetrics for optimized distributed evaluation",
          "dependencies": [
            2,
            10
          ],
          "details": "Refactor existing metrics to use TorchMetrics for better performance in distributed environments. Implement custom TorchMetrics classes for specialized evaluation needs. Ensure metrics are properly synchronized across distributed processes. Add support for metric serialization and deserialization for result persistence.",
          "status": "pending"
        },
        {
          "id": 12,
          "title": "Automated Job Submission",
          "description": "Set up automated job submission for batch evaluations",
          "dependencies": [
            10,
            11
          ],
          "details": "Implement batch job submission system for running multiple evaluations. Create job templates for different evaluation scenarios. Set up job scheduling and prioritization. Implement notification system for job completion. Design job monitoring dashboard for tracking evaluation progress.",
          "status": "pending"
        },
        {
          "id": 13,
          "title": "Lightning.AI Documentation",
          "description": "Create comprehensive documentation for Lightning.AI Studio setup and usage",
          "dependencies": [
            10,
            11,
            12
          ],
          "details": "Document Lightning.AI Studio setup process. Create tutorials for running evaluations in Lightning.AI. Document GPU switching functionality and cost optimization strategies. Include troubleshooting guides for common issues. Create reference documentation for all Lightning.AI specific configuration options.",
          "status": "pending"
        },
        {
          "id": 16,
          "title": "Non-Thinking Mode Configuration",
          "description": "Ensure Qwen3-4B is properly configured with non-thinking mode",
          "dependencies": [
            1,
            2
          ],
          "details": "Configure Qwen3-4B with enable_thinking=False to ensure compliance with project policy. Verify all code paths explicitly disable thinking mode. Update all configuration files to reflect non-thinking mode only. Document the non-thinking mode configuration in all relevant documentation. Create validation tests to ensure thinking mode is never enabled.",
          "status": "pending"
        },
        {
          "id": 15,
          "title": "English-only Token Subset Focus",
          "description": "Configure evaluation to focus on English-only token subset",
          "dependencies": [
            1,
            16
          ],
          "details": "Implement filtering mechanisms to focus evaluation on English-only token subset. Create analysis tools to measure performance differences between full vocabulary and English-only subset. Document impact of token subset focus on evaluation results. Implement visualization tools to highlight performance differences across token subsets.",
          "status": "pending"
        },
        {
          "id": 17,
          "title": "Position and Count Task Filtering",
          "description": "Ensure evaluation focuses exclusively on position and count tasks",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement strict filtering to ensure only position and count tasks are evaluated. Create validation checks to verify no spelling evaluation is included. Update all configuration files to exclude spelling tasks. Document the focus on position and count tasks in all relevant documentation. Create validation tests to ensure no spelling evaluation occurs.",
          "status": "pending"
        },
        {
          "id": 18,
          "title": "Evaluation Report Refactoring",
          "description": "Refactor reporting to focus exclusively on position and count metrics",
          "dependencies": [
            5,
            7
          ],
          "details": "Update all reporting templates to focus exclusively on position and count metrics. Remove any references to spelling evaluation from reports. Create new visualization templates specific to position and count tasks. Ensure all metrics and analyses in reports relate only to position and count tasks. Update documentation to reflect the focus on position and count tasks only.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 6,
      "title": "Hyperparameter Tuning Infrastructure",
      "description": "Create a configuration system for hyperparameter experiments focused on position and character count task performance and transfer learning, and set up experiment tracking with Weights & Biases using Python scripts instead of notebooks, leveraging Lightning.AI Studios for efficient training and experimentation.",
      "status": "pending",
      "dependencies": [
        1,
        5
      ],
      "priority": "medium",
      "details": "1. Create a configuration system for hyperparameter experiments with focus on position and character count tasks and transfer learning\n2. Set up Python scripts for experiment tracking with metrics for position and character count tasks\n3. Create a script that can run training with different hyperparameters optimized for position and count tasks\n4. Set up W&B for experiment tracking with correlation analysis between position and count tasks\n5. Define a clear set of metrics for comparing experiments across position and count task performance\n6. Create a dedicated Lightning.AI Studio for hyperparameter tuning following the \"one Studio, one task\" principle\n7. Implement GPU switching for efficient resource usage (CPU → T4 → A100)\n8. Leverage Lightning.AI's job system for managing multiple training runs\n9. Add support for Qwen3-4B tokenizer analysis, focusing on English-only token subset\n10. Implement analysis of multi-token word behavior in Qwen3-4B\n\nFile Structure:\n- Base configs: `configs/hyperparameters/`\n- Model configs: `configs/hyperparameters/models/`\n- Training configs: `configs/hyperparameters/training/`\n- Evaluation configs: `configs/hyperparameters/evaluation/`\n- Search space definitions: `configs/hyperparameters/search_spaces/`\n- Lightning.AI configs: `configs/hyperparameters/lightning/`\n- Token analysis configs: `configs/hyperparameters/token_analysis/`\n\nPython Module Structure:\n- Config manager: `src/tuning/config.py`\n- W&B integration: `src/tuning/wandb_integration.py`\n- Lightning.AI integration: `src/tuning/lightning_integration.py`\n- Grid search: `src/tuning/grid.py`\n- Experiment executor: `src/tuning/executor.py`\n- Visualization tools: `src/tuning/visualization.py`\n- Report generation: `src/tuning/report.py`\n- Task analysis: `src/tuning/task_analysis.py`\n- Lightning.AI job manager: `src/tuning/lightning_jobs.py`\n- Token analysis: `src/tuning/token_analysis.py`\n- Qwen3 tokenizer utilities: `src/tuning/qwen3_tokenizer.py`\n\nResults Structure:\n- Experiment results: `results/tuning/data/`\n- Best configurations: `results/tuning/configs/`\n- Performance plots: `results/tuning/figures/`\n- Task analysis: `results/tuning/tasks/`\n- HTML reports: `results/tuning/reports/`\n- Token analysis: `results/tuning/token_analysis/`\n- English token subset: `results/tuning/token_analysis/english_tokens/`\n- Documentation: `docs/hyperparameter_tuning.md`, `docs/config_system.md`, `docs/tuning_results.md`, `docs/position_count_tasks.md`, `docs/lightning_studio_setup.md`, `docs/qwen3_token_analysis.md`\n\nImplementation:\n```python\nimport yaml\nimport os\nfrom datetime import datetime\nimport wandb\nimport argparse\nimport pytorch_lightning as pl\nfrom lightning_app import LightningApp, LightningFlow, LightningWork\nfrom lightning_app.storage import Path\nfrom transformers import AutoTokenizer\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef create_experiment_config(\n    exp_name,\n    lora_r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    learning_rate=2e-4,\n    batch_size=8,\n    grad_accum_steps=4,\n    max_steps=1000,\n    warmup_steps=100,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    # Training parameters\n    spelling_data_ratio=0.7,\n    spelling_augmentation=True,\n    spelling_difficulty=\"medium\",\n    # Task evaluation parameters\n    eval_frequency=100,\n    position_task_weight=0.5,\n    count_task_weight=0.5,\n    # Lightning.AI parameters\n    gpu_tier=\"cpu\",  # Options: \"cpu\", \"t4\", \"a100\"\n    auto_scale=True,\n    sleep_when_idle=True,\n    # Qwen3 tokenizer parameters\n    use_english_only_tokens=False,\n):\n    \"\"\"Create and save an experiment configuration with position and count task focus.\"\"\"\n    config = {\n        \"experiment_name\": exp_name,\n        \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n        \"lora_config\": {\n            \"r\": lora_r,\n            \"alpha\": lora_alpha,\n            \"dropout\": lora_dropout,\n            \"target_modules\": target_modules,\n        },\n        \"training_config\": {\n            \"learning_rate\": learning_rate,\n            \"per_device_train_batch_size\": batch_size,\n            \"gradient_accumulation_steps\": grad_accum_steps,\n            \"max_steps\": max_steps,\n            \"warmup_steps\": warmup_steps,\n            \"spelling_data_ratio\": spelling_data_ratio,\n            \"spelling_augmentation\": spelling_augmentation,\n            \"spelling_difficulty\": spelling_difficulty,\n        },\n        \"evaluation_config\": {\n            \"eval_frequency\": eval_frequency,\n            \"position_task_weight\": position_task_weight,\n            \"count_task_weight\": count_task_weight,\n            # No spelling evaluation per project policy\n        },\n        \"lightning_config\": {\n            \"gpu_tier\": gpu_tier,\n            \"auto_scale\": auto_scale,\n            \"sleep_when_idle\": sleep_when_idle,\n        },\n        \"tokenizer_config\": {\n            \"model_name\": \"Qwen/Qwen3-4B\",\n            \"use_english_only_tokens\": use_english_only_tokens,\n            \"enable_thinking\": False,  # Always set to False per project policy\n        }\n    }\n\n    # Create experiments directory if it doesn't exist\n    os.makedirs(\"configs/hyperparameters/\", exist_ok=True)\n\n    # Save config to file\n    config_path = f\"configs/hyperparameters/{exp_name}_{config['timestamp']}.yaml\"\n    with open(config_path, \"w\") as f:\n        yaml.dump(config, f)\n\n    print(f\"Created experiment config: {config_path}\")\n    return config_path\n\n# Define hyperparameter grid with position and count task focus\ndef create_hyperparameter_grid():\n    grid = {\n        # LoRA parameters\n        \"lora_r\": [4, 8, 16, 32],\n        \"lora_alpha\": [8, 16, 32, 64],\n        # Training parameters\n        \"learning_rate\": [1e-4, 2e-4, 5e-4, 1e-3],\n        \"batch_size\": [4, 8, 16, 32],\n        \"grad_accum_steps\": [1, 2, 4, 8],\n        \"max_steps\": [500, 1000, 2000, 5000],\n        # Spelling training parameters (for training only)\n        \"spelling_data_ratio\": [0.5, 0.7, 0.9],\n        \"spelling_difficulty\": [\"easy\", \"medium\", \"hard\"],\n        # Task evaluation parameters\n        \"position_task_weight\": [0.3, 0.5, 0.7],\n        \"count_task_weight\": [0.3, 0.5, 0.7],\n        # Lightning.AI parameters\n        \"gpu_tier\": [\"cpu\", \"t4\", \"a100\"],\n        # Qwen3 tokenizer parameters\n        \"use_english_only_tokens\": [False, True],\n    }\n    return grid\n\n# Create experiment configs for grid search\ndef create_grid_search_configs(base_name=\"position_count_exp\"):\n    grid = create_hyperparameter_grid()\n    configs = []\n    \n    # Start with default configuration\n    configs.append(create_experiment_config(f\"{base_name}_default\"))\n    \n    # Create configs for each hyperparameter variation\n    for param, values in grid.items():\n        for value in values:\n            # Skip the default value\n            if param == \"lora_r\" and value == 16: continue\n            if param == \"lora_alpha\" and value == 32: continue\n            if param == \"learning_rate\" and value == 2e-4: continue\n            if param == \"batch_size\" and value == 8: continue\n            if param == \"grad_accum_steps\" and value == 4: continue\n            if param == \"max_steps\" and value == 1000: continue\n            if param == \"spelling_data_ratio\" and value == 0.7: continue\n            if param == \"spelling_difficulty\" and value == \"medium\": continue\n            if param == \"position_task_weight\" and value == 0.5: continue\n            if param == \"count_task_weight\" and value == 0.5: continue\n            if param == \"gpu_tier\" and value == \"cpu\": continue\n            if param == \"use_english_only_tokens\" and value == False: continue\n                \n            kwargs = {param: value}\n            config_path = create_experiment_config(f\"{base_name}_{param}_{value}\", **kwargs)\n            configs.append(config_path)\n    \n    return configs\n\n# Initialize W&B sweep with position and count task metrics\ndef create_wandb_sweep():\n    sweep_config = {\n        \"method\": \"grid\",\n        \"metric\": {\n            \"name\": \"combined_task_score\",  # Combined metric for position and count tasks\n            \"goal\": \"maximize\"\n        },\n        \"parameters\": {\n            # LoRA parameters\n            \"lora_r\": {\"values\": [4, 8, 16, 32]},\n            \"lora_alpha\": {\"values\": [8, 16, 32, 64]},\n            # Training parameters\n            \"learning_rate\": {\"values\": [1e-4, 2e-4, 5e-4, 1e-3]},\n            \"batch_size\": {\"values\": [4, 8, 16, 32]},\n            \"grad_accum_steps\": {\"values\": [1, 2, 4, 8]},\n            \"max_steps\": {\"values\": [500, 1000, 2000, 5000]},\n            # Spelling training parameters (for training only)\n            \"spelling_data_ratio\": {\"values\": [0.5, 0.7, 0.9]},\n            \"spelling_difficulty\": {\"values\": [\"easy\", \"medium\", \"hard\"]},\n            # Task evaluation parameters\n            \"position_task_weight\": {\"values\": [0.3, 0.5, 0.7]},\n            \"count_task_weight\": {\"values\": [0.3, 0.5, 0.7]},\n            # Lightning.AI parameters\n            \"gpu_tier\": {\"values\": [\"cpu\", \"t4\", \"a100\"]},\n            # Qwen3 tokenizer parameters\n            \"use_english_only_tokens\": {\"values\": [False, True]},\n        }\n    }\n    \n    sweep_id = wandb.sweep(sweep_config, project=\"llm-position-count-tasks\")\n    return sweep_id\n\n# Calculate combined task score from position and count metrics\ndef calculate_combined_task_score(position_accuracy, count_accuracy, position_weight=0.5, count_weight=0.5):\n    \"\"\"Calculate a combined score that measures position and count task performance.\"\"\"\n    combined_score = position_weight * position_accuracy + count_weight * count_accuracy\n    # Correlation bonus: reward configurations where position and count tasks both perform well\n    correlation_bonus = min(position_accuracy, count_accuracy) * 0.2\n    return combined_score + correlation_bonus\n\n# Qwen3 tokenizer analysis functions\ndef analyze_qwen3_tokenizer(use_english_only=False):\n    \"\"\"Analyze the Qwen3-4B tokenizer with focus on English tokens.\"\"\"\n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")\n    vocab = tokenizer.get_vocab()\n    \n    # Create results directory\n    os.makedirs(\"results/tuning/token_analysis\", exist_ok=True)\n    \n    # Basic tokenizer statistics\n    stats = {\n        \"vocab_size\": len(vocab),\n        \"special_tokens\": len([t for t in vocab.keys() if t.startswith(\"<\") and t.endswith(\">\")])  \n    }\n    \n    # Analyze multi-token words\n    english_words = [\"apple\", \"banana\", \"computer\", \"algorithm\", \"intelligence\", \"understanding\", \n                    \"knowledge\", \"learning\", \"development\", \"hyperparameter\"]\n    \n    word_tokens = {}\n    for word in english_words:\n        tokens = tokenizer.tokenize(word)\n        word_tokens[word] = {\n            \"tokens\": tokens,\n            \"token_count\": len(tokens)\n        }\n    \n    # Identify English tokens\n    english_tokens = {}\n    if use_english_only:\n        # Simple heuristic: tokens that only contain ASCII characters are likely English\n        for token, id in vocab.items():\n            if all(ord(c) < 128 for c in token) and not token.startswith(\"<\"):\n                english_tokens[token] = id\n        \n        # Save English token subset\n        os.makedirs(\"results/tuning/token_analysis/english_tokens\", exist_ok=True)\n        with open(\"results/tuning/token_analysis/english_tokens/english_vocab.json\", \"w\") as f:\n            json.dump(english_tokens, f, indent=2)\n            \n        stats[\"english_token_count\"] = len(english_tokens)\n        stats[\"english_token_percentage\"] = len(english_tokens) / len(vocab) * 100\n    \n    # Save analysis results\n    with open(\"results/tuning/token_analysis/tokenizer_stats.json\", \"w\") as f:\n        json.dump(stats, f, indent=2)\n        \n    with open(\"results/tuning/token_analysis/multi_token_words.json\", \"w\") as f:\n        json.dump(word_tokens, f, indent=2)\n    \n    # Create visualizations\n    if use_english_only:\n        # Plot token distribution comparison\n        labels = ['Full Vocabulary', 'English-only Subset']\n        sizes = [len(vocab) - len(english_tokens), len(english_tokens)]\n        \n        plt.figure(figsize=(10, 6))\n        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n        plt.axis('equal')\n        plt.title('Qwen3-4B Token Distribution')\n        plt.savefig(\"results/tuning/token_analysis/token_distribution.png\")\n    \n    return stats\n\n# Lightning.AI Work class for hyperparameter tuning\nclass HyperparameterTuningWork(LightningWork):\n    def __init__(self, config_path, gpu_tier=\"cpu\"):\n        super().__init__(cloud_compute=gpu_tier)\n        self.config_path = config_path\n        self.results = None\n        \n    def run(self):\n        # Load configuration\n        with open(self.config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n            \n        # Set up PyTorch Lightning trainer\n        trainer = pl.Trainer(\n            max_steps=config[\"training_config\"][\"max_steps\"],\n            accelerator=\"auto\",\n            devices=1,\n            precision=16,\n            logger=True,\n        )\n        \n        # Run training and evaluation\n        # ... training code here ...\n        \n        # Calculate combined task score\n        position_accuracy = 0.75  # Example value\n        count_accuracy = 0.70     # Example value\n        combined_score = calculate_combined_task_score(\n            position_accuracy, \n            count_accuracy,\n            position_weight=config[\"evaluation_config\"][\"position_task_weight\"],\n            count_weight=config[\"evaluation_config\"][\"count_task_weight\"]\n        )\n        \n        # Store results\n        self.results = {\n            \"position_accuracy\": position_accuracy,\n            \"count_accuracy\": count_accuracy,\n            \"combined_score\": combined_score,\n        }\n        \n        # Save results to shared storage\n        results_dir = os.path.join(\"results/tuning/data\", config[\"experiment_name\"])\n        os.makedirs(results_dir, exist_ok=True)\n        with open(os.path.join(results_dir, \"results.yaml\"), \"w\") as f:\n            yaml.dump(self.results, f)\n\n# Lightning.AI Flow class to manage hyperparameter tuning jobs\nclass HyperparameterTuningFlow(LightningFlow):\n    def __init__(self):\n        super().__init__()\n        self.tuning_jobs = {}\n        self.completed_jobs = {}\n        \n    def run(self):\n        # Check status of running jobs\n        for job_id, job in list(self.tuning_jobs.items()):\n            if job.status.ready:\n                self.completed_jobs[job_id] = job.results\n                del self.tuning_jobs[job_id]\n                \n        # Report progress\n        if self.tuning_jobs or self.completed_jobs:\n            print(f\"Running jobs: {len(self.tuning_jobs)}, Completed jobs: {len(self.completed_jobs)}\")\n            \n    def add_job(self, config_path, job_id=None):\n        if job_id is None:\n            job_id = f\"job_{len(self.tuning_jobs) + len(self.completed_jobs)}\"\n            \n        # Load configuration to get GPU tier\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n            gpu_tier = config.get(\"lightning_config\", {}).get(\"gpu_tier\", \"cpu\")\n            \n        # Create and add job\n        job = HyperparameterTuningWork(config_path=config_path, gpu_tier=gpu_tier)\n        self.tuning_jobs[job_id] = job\n        return job_id\n\n# Command-line interface for experiment execution\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run hyperparameter tuning experiments for position and count tasks\")\n    parser.add_argument(\"--mode\", choices=[\"grid\", \"sweep\", \"single\", \"lightning\", \"token-analysis\"], default=\"single\",\n                        help=\"Experiment mode: grid search, W&B sweep, single experiment, Lightning.AI jobs, or token analysis\")\n    parser.add_argument(\"--name\", type=str, default=\"position_count_exp\",\n                        help=\"Base name for the experiment\")\n    parser.add_argument(\"--config\", type=str, help=\"Path to a specific config file (for single mode)\")\n    parser.add_argument(\"--focus\", choices=[\"position\", \"count\", \"balanced\"], default=\"balanced\",\n                        help=\"Focus of the experiment: position task, count task, or balanced\")\n    parser.add_argument(\"--gpu-tier\", choices=[\"cpu\", \"t4\", \"a100\"], default=\"cpu\",\n                        help=\"GPU tier to use for Lightning.AI jobs\")\n    parser.add_argument(\"--english-only\", action=\"store_true\", help=\"Use only English tokens from Qwen3 tokenizer\")\n    \n    args = parser.parse_args()\n    \n    # Adjust weights based on experiment focus\n    position_weight = 0.5\n    count_weight = 0.5\n    if args.focus == \"position\":\n        position_weight = 0.7\n        count_weight = 0.3\n    elif args.focus == \"count\":\n        position_weight = 0.3\n        count_weight = 0.7\n    \n    if args.mode == \"token-analysis\":\n        print(\"Running Qwen3-4B tokenizer analysis...\")\n        stats = analyze_qwen3_tokenizer(use_english_only=args.english_only)\n        print(f\"Analysis complete. Results saved to results/tuning/token_analysis/\")\n        print(f\"Vocabulary size: {stats['vocab_size']}\")\n        if 'english_token_count' in stats:\n            print(f\"English tokens: {stats['english_token_count']} ({stats['english_token_percentage']:.2f}%)\")\n    elif args.mode == \"grid\":\n        configs = create_grid_search_configs(args.name)\n        print(f\"Created {len(configs)} configurations for grid search with {args.focus} focus\")\n    elif args.mode == \"sweep\":\n        sweep_id = create_wandb_sweep()\n        print(f\"Created W&B sweep with ID: {sweep_id} and {args.focus} focus\")\n    elif args.mode == \"lightning\":\n        # Set up Lightning.AI app for hyperparameter tuning\n        flow = HyperparameterTuningFlow()\n        app = LightningApp(flow)\n        \n        # Create configurations\n        configs = create_grid_search_configs(args.name)\n        \n        # Add jobs to the flow\n        for config in configs:\n            flow.add_job(config)\n            \n        # Run the app\n        app.run()\n    elif args.mode == \"single\":\n        if args.config:\n            print(f\"Using provided config: {args.config} with {args.focus} focus\")\n        else:\n            config_path = create_experiment_config(\n                args.name, \n                position_task_weight=position_weight,\n                count_task_weight=count_weight,\n                gpu_tier=args.gpu_tier,\n                use_english_only_tokens=args.english_only,\n            )\n            print(f\"Created single experiment config: {config_path} with {args.focus} focus\")\n\nif __name__ == \"__main__\":\n    main()\n```",
      "testStrategy": "1. Verify configuration system creates valid YAML files with position and count task parameters in the correct directories (`configs/hyperparameters/`)\n2. Confirm W&B experiment tracking properly tracks position and count task metrics (and does NOT track spelling evaluation metrics)\n3. Test that the hyperparameter grid generates the expected number of configurations including position and count task parameters\n4. Verify W&B sweep configuration includes only position and count task metrics\n5. Test the command-line interfaces for all Python scripts, especially the new `--focus`, `--gpu-tier`, and `--english-only` parameters\n6. Ensure metrics for comparing experiments only include position and count task performance (no spelling evaluation metrics)\n7. Verify that results are properly saved to `results/tuning/` directories including the task analysis\n8. Test the experiment executor to ensure it correctly tracks position and count task metrics\n9. Verify HTML report generation produces valid reports that show correlations between position and count task performance\n10. Test visualization tools to ensure they generate figures showing relationships between position and count task performance\n11. Verify the task analysis module correctly calculates combined scores and identifies optimal training patterns\n12. Test Lightning.AI Studio creation and configuration with proper environment isolation\n13. Verify GPU switching functionality works correctly (CPU → T4 → A100) based on experiment phase\n14. Test Lightning.AI job system integration for managing multiple training runs\n15. Verify shared filesystem access for datasets and checkpoint saving\n16. Test cost optimization features including GPU sleep settings and resource usage efficiency\n17. Verify PyTorch Lightning integration works correctly with the Lightning.AI platform\n18. Test Qwen3-4B tokenizer analysis functionality with focus on English-only token subset\n19. Verify multi-token word analysis correctly identifies tokenization patterns in Qwen3-4B\n20. Verify the English token filtering process produces a valid subset of the full vocabulary\n21. Test token distribution visualization to ensure it correctly shows the proportion of English tokens\n22. Verify that experiments can be run with both full vocabulary and English-only token subset\n23. Confirm that all configurations explicitly set `enable_thinking=False` for Qwen3 tokenizer\n24. Verify that no code references or enables thinking mode functionality\n25. Test that the tokenizer analysis does not attempt to analyze thinking mode patterns\n26. Verify that no spelling evaluation metrics are included in any reports or visualizations\n27. Confirm that training can still use spelling data, but evaluation is strictly limited to position and count tasks\n28. Test that the combined task score calculation only uses position and count metrics",
      "subtasks": [
        {
          "id": 1,
          "title": "Configuration System Design and Implementation",
          "description": "Design and implement a flexible configuration system for hyperparameter management",
          "dependencies": [],
          "details": "Create a configuration framework that supports defining, validating, and loading hyperparameter configurations. Implement serialization/deserialization of configurations to JSON/YAML formats. Design a hierarchical configuration structure that allows for inheritance and overrides. Include validation mechanisms to ensure hyperparameter values fall within acceptable ranges. Support both discrete values (HPARAM_CANDIDATES) and continuous ranges (HPARAM_RANGE) for different hyperparameter types.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Experiment Tracking Setup with W&B Integration",
          "description": "Implement experiment tracking infrastructure with Weights & Biases integration focused on position and count task metrics",
          "dependencies": [
            1
          ],
          "details": "Set up W&B project structure for hyperparameter experiments with tracking for position and count task metrics. Implement logging mechanisms for position task metrics, count task metrics, and model artifacts in `src/tuning/wandb_integration.py`. Create utilities for experiment initialization, updating, and finalization. Design a consistent naming convention for experiments. Implement automatic synchronization between local experiment state and W&B. Add support for experiment grouping and comparison within the W&B interface. Create visualizations that show correlations between position and count task performance.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Hyperparameter Grid Definition and Validation",
          "description": "Create a system for defining and validating hyperparameter search spaces for position and count task optimization",
          "dependencies": [
            1
          ],
          "details": "Implement a framework for defining hyperparameter search spaces including random, grid, and Bayesian optimization strategies in `src/tuning/grid.py`. Create validation mechanisms to ensure search spaces are properly defined. Support both continuous ranges and discrete value sets for different hyperparameter types. Implement utilities for sampling from defined search spaces. Add functionality to estimate the total number of trials based on the search space definition. Create interfaces for custom search space definitions. Store search space definitions in `configs/hyperparameters/search_spaces/`. Include position and count task parameters (task weights, evaluation frequency) in the search space.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Experiment Execution Framework",
          "description": "Build a framework for executing hyperparameter tuning experiments optimized for position and count task performance",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Implement a job scheduler in `src/tuning/executor.py` for running multiple trials with different hyperparameter configurations. Create mechanisms for early stopping of underperforming trials based on position and count metrics. Design parallel execution capabilities to utilize available computational resources efficiently. Implement checkpointing and resumption of interrupted experiments. Add support for distributed training across multiple machines. Create a monitoring system for active experiments with real-time status updates for position and count task performance. Save experiment results to `results/tuning/data/` with best configurations in `results/tuning/configs/`. Implement command-line interfaces for flexible experiment execution with options to focus on position task, count task, or a balanced approach.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Results Visualization and Reporting System",
          "description": "Develop tools for visualizing and reporting hyperparameter tuning results with focus on position and count task performance",
          "dependencies": [
            2,
            4
          ],
          "details": "Create visualization tools in `src/tuning/visualization.py` for comparing metrics across different hyperparameter configurations, showing relationships between position and count task performance. Implement automated analysis to identify the most influential hyperparameters for both position and count tasks. Design HTML report generation in `src/tuning/report.py` for exploring the hyperparameter search space and task performance patterns. Add functionality to export comparison reports to `results/tuning/reports/`. Implement statistical analysis tools to evaluate the significance of performance differences and correlations between position and count metrics. Create recommendation system for suggesting optimal hyperparameter configurations for future experiments based on task performance goals. Generate performance plots in `results/tuning/figures/` showing relationships between position and count task outcomes.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Documentation and User Guides",
          "description": "Create comprehensive documentation for the hyperparameter tuning system with focus on position and count tasks",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Develop detailed documentation covering the hyperparameter tuning system in `docs/hyperparameter_tuning.md`. Create a configuration guide explaining the structure and usage of the configuration system in `docs/config_system.md`. Write a results analysis guide detailing how to interpret and utilize tuning results in `docs/tuning_results.md`. Create a position and count tasks guide explaining how to optimize training for better performance on these tasks in `docs/position_count_tasks.md`. Include examples, best practices, and troubleshooting information in all documentation. Document command-line interfaces and provide usage examples for all scripts, including the new focus options.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Python Package Structure and Testing",
          "description": "Implement proper Python packaging and testing for the tuning infrastructure",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Organize the tuning code as a proper Python package with appropriate imports and dependencies. Create unit tests for each component of the tuning infrastructure. Implement integration tests to verify the end-to-end workflow. Set up continuous integration for automated testing. Create a requirements.txt or setup.py file to manage dependencies. Ensure compatibility with the rest of the codebase. Add type hints and docstrings for better code documentation.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Task Analysis Module",
          "description": "Develop a module for analyzing position and count task performance across different hyperparameter configurations",
          "dependencies": [
            2,
            4,
            5
          ],
          "details": "Create a dedicated module `src/tuning/task_analysis.py` for analyzing the performance on position and count tasks. Implement metrics that quantify task performance across different hyperparameter configurations. Design visualization tools specifically for task performance analysis. Create correlation analysis between position and count task performance. Implement functions to identify which training patterns lead to better performance on both tasks. Add support for calculating combined scores that balance position and count task performance. Save task analysis results to `results/tuning/tasks/`.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Lightning.AI Studio Integration",
          "description": "Set up a dedicated Lightning.AI Studio for hyperparameter tuning experiments",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Create a dedicated Lightning.AI Studio for hyperparameter tuning following the \"one Studio, one task\" principle. Configure proper environment isolation with all required dependencies. Set up shared filesystem access for datasets and model checkpoints. Implement GPU switching functionality (CPU for development → T4 for testing → A100 for full training). Create configuration files for Lightning.AI Studio in `configs/hyperparameters/lightning/`. Implement cost optimization through GPU sleep settings and efficient resource usage. Document the Lightning.AI Studio setup process in `docs/lightning_studio_setup.md`. Create scripts for launching and monitoring experiments in the Lightning.AI environment.",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "PyTorch Lightning Integration",
          "description": "Implement training using PyTorch Lightning for better integration with Lightning.AI platform",
          "dependencies": [
            1,
            4,
            9
          ],
          "details": "Refactor training code to use PyTorch Lightning for better integration with the Lightning.AI platform. Implement Lightning Modules for position and count task models. Create custom callbacks for tracking task metrics. Set up Lightning DataModules for efficient data loading. Implement checkpointing and model saving compatible with Lightning.AI's shared filesystem. Create Lightning CLI interfaces for experiment configuration. Implement distributed training support using Lightning's built-in capabilities. Add support for mixed precision training to improve performance on GPUs.",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "Lightning.AI Job System Integration",
          "description": "Leverage Lightning.AI's job system for managing multiple training runs",
          "dependencies": [
            9,
            10
          ],
          "details": "Implement integration with Lightning.AI's job system in `src/tuning/lightning_jobs.py`. Create job templates for different experiment types (grid search, single run, etc.). Implement job scheduling based on resource availability. Add support for job dependencies and sequential execution. Create monitoring tools for tracking job status and performance. Implement automatic resource scaling based on experiment requirements. Add support for job prioritization based on expected impact. Create utilities for job result collection and aggregation. Implement cost tracking and optimization for Lightning.AI resources.",
          "status": "pending"
        },
        {
          "id": 12,
          "title": "Qwen3-4B Tokenizer Analysis Implementation",
          "description": "Implement analysis tools for Qwen3-4B tokenizer with focus on English-only token subset",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated module `src/tuning/token_analysis.py` for analyzing the Qwen3-4B tokenizer. Implement functions to identify and extract English-only tokens from the full vocabulary. Create analysis tools for multi-token word behavior in Qwen3-4B. Implement visualization of token distribution between full vocabulary and English-only subset. Create utilities for filtering and using the English-only token subset in experiments. Save analysis results and English token subset to `results/tuning/token_analysis/`. Document the tokenizer analysis process and findings in `docs/qwen3_token_analysis.md`. Ensure all tokenizer configurations explicitly set `enable_thinking=False` in accordance with project policy.",
          "status": "pending"
        },
        {
          "id": 13,
          "title": "English Token Subset Integration",
          "description": "Integrate English-only token subset option into the hyperparameter tuning framework",
          "dependencies": [
            1,
            12
          ],
          "details": "Extend the configuration system to support using English-only token subset from Qwen3-4B. Implement mechanisms to filter and use only English tokens during training and evaluation. Create comparison experiments between full vocabulary and English-only subset. Add metrics to measure the impact of token subset on position and count task performance. Implement visualization tools to compare results between full vocabulary and English-only experiments. Document the English token subset approach and its effects in the hyperparameter tuning documentation.",
          "status": "pending"
        },
        {
          "id": 14,
          "title": "Non-Thinking Mode Enforcement",
          "description": "Ensure all Qwen3-4B usage strictly adheres to non-thinking mode policy",
          "dependencies": [
            1,
            12,
            13
          ],
          "details": "Implement validation checks in the configuration system to ensure `enable_thinking=False` is always set for Qwen3 tokenizer. Create documentation explaining the project policy regarding non-thinking mode usage. Add unit tests to verify that thinking mode is never enabled in any configuration. Update all tokenizer initialization code to explicitly set `enable_thinking=False`. Create a linting rule to prevent accidental enabling of thinking mode in future code. Add warnings in the codebase about the strict non-thinking mode policy. Ensure all experiment configurations and hyperparameter search spaces enforce non-thinking mode.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 7,
      "title": "Unsloth Integration for Optimized Fine-tuning",
      "description": "Set up Unsloth for optimized LoRA fine-tuning of the Qwen3-4B model with memory efficiency optimizations in a cloud GPU environment (Google Colab or Lightning.ai), using Python scripts instead of notebooks for better maintainability and version control. Configure the system to handle separate training (spelling variations) and evaluation (position/count) datasets, with special attention to Qwen3's tokenizer and English-only token subset.",
      "status": "pending",
      "dependencies": [
        6
      ],
      "priority": "high",
      "details": "**NOTE: This task requires a cloud GPU environment. Do not attempt on local Mac.**\n\n1. Install and configure Unsloth for optimized fine-tuning in Google Colab or Lightning.ai\n2. Set up Unsloth-specific environment requirements in the cloud environment\n3. Configure memory-efficient QLoRA training\n4. Set up Flash Attention 2 if available on cloud GPU hardware\n5. Implement proper tokenization for instruction fine-tuning with Qwen3-4B's tokenizer\n6. Configure GPU memory optimizations\n7. Set up separate handling for spelling training data and position/count evaluation data\n8. Implement efficient evaluation of position/count tasks during training\n9. Leverage Lightning.AI Studios for data preparation and processing\n10. Filter and validate data based on Qwen3's English-only token subset\n11. Handle multi-token words appropriately in Qwen3's tokenization patterns\n12. Prepare data in non-thinking mode only (enable_thinking=False)\n\nFile Structure:\n- Environment setup: `src/unsloth/environment.py`\n- Model loading and configuration: `src/unsloth/model.py`\n- Dataset preparation: `src/unsloth/dataset.py`\n- Lightning DataModules: `src/unsloth/datamodules.py`\n- Training setup: `src/unsloth/trainer.py`\n- Training monitoring: `src/unsloth/monitor.py`\n- HTML report generation: `src/unsloth/report.py`\n- Evaluation utilities: `src/unsloth/evaluation.py`\n- Data validation: `src/unsloth/data_validation.py`\n- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`\n- Token filtering utilities: `src/unsloth/token_filter.py`\n\nOutput Structure:\n- `results/unsloth/figures/` (All PNG/PDF visualizations)\n- `results/unsloth/reports/` (HTML reports)\n- `results/unsloth/data/` (Training metrics)\n- `results/unsloth/configs/` (Model configurations)\n- `results/unsloth/evaluation/` (Position/count evaluation results)\n- `results/unsloth/data_versions/` (Data version tracking)\n- `results/unsloth/token_analysis/` (Qwen3 token usage analysis)\n\nImplementation:\n```python\n# Install Unsloth in Google Colab or Lightning.ai environment\n!pip install unsloth\n\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\n\n# Optimize GPU memory\ndef optimize_gpu_memory():\n    if torch.cuda.is_available():\n        # Set GPU memory allocation strategy\n        torch.cuda.set_per_process_memory_fraction(0.9)  # Reserve 10% for system\n        # Enable memory caching for faster allocation\n        torch.backends.cudnn.benchmark = True\n        # Use TF32 precision on Ampere GPUs or later for faster computation\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n# Load model with Unsloth optimizations\ndef load_unsloth_model(config):\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"Qwen/Qwen3-4B\",  # Updated to Qwen3-4B\n        max_seq_length=512,\n        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n        load_in_4bit=True,  # Use 4-bit quantization to reduce memory usage\n        token=None,  # Add your HF token for private models\n    )\n\n    # Add LoRA adapters with Unsloth-specific optimizations\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=config[\"lora_config\"][\"r\"],\n        target_modules=config[\"lora_config\"][\"target_modules\"],\n        lora_alpha=config[\"lora_config\"][\"alpha\"],\n        lora_dropout=config[\"lora_config\"][\"dropout\"],\n        bias=\"none\",  # Unsloth-specific - sets which modules receive adapters\n        use_gradient_checkpointing=True,  # Unsloth-specific - saves memory\n        random_state=42,  # For reproducibility\n        use_rslora=False,  # Set to True for rank-stabilized LoRA (optional)\n        loftq_config=None,  # Optional LoftQ configuration\n    )\n    \n    return model, tokenizer\n\n# Get English-only token subset for Qwen3\ndef get_english_token_subset(tokenizer):\n    # This function identifies the English-only token subset in Qwen3's vocabulary\n    # Implementation depends on Qwen3's specific tokenization patterns\n    english_tokens = []\n    for token_id in range(len(tokenizer)):\n        token = tokenizer.decode([token_id])\n        # Apply filtering logic to identify English-only tokens\n        # This is a simplified example - actual implementation would be more complex\n        if all(c.isascii() for c in token):\n            english_tokens.append(token_id)\n    \n    return set(english_tokens)\n\n# Analyze multi-token word handling in Qwen3\ndef analyze_multi_token_words(tokenizer, common_words):\n    results = {}\n    for word in common_words:\n        tokens = tokenizer.encode(word, add_special_tokens=False)\n        results[word] = {\n            \"token_count\": len(tokens),\n            \"tokens\": [tokenizer.decode([t]) for t in tokens]\n        }\n    return results\n\n# Prepare spelling dataset for Unsloth with Qwen3 tokenizer\ndef prepare_spelling_dataset(dataset, tokenizer, config):\n    # Get English token subset if filtering is enabled\n    english_tokens = get_english_token_subset(tokenizer) if config[\"use_english_only\"] else None\n    \n    def formatting_prompts_func(examples):\n        questions = examples[\"question\"]\n        answers = examples[\"answer\"]\n        \n        # Standard prompt format (non-thinking mode only)\n        prompts = [\n            f\"<human>: {question}\\n<assistant>: \"\n            for question in questions\n        ]\n\n        # Format responses with EOS token\n        formatted_responses = [\n            f\"{answer}{tokenizer.eos_token}\"\n            for answer in answers\n        ]\n        \n        # Validate tokens if English-only filtering is enabled\n        if english_tokens:\n            valid_examples = []\n            valid_responses = []\n            \n            for prompt, response in zip(prompts, formatted_responses):\n                prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n                response_tokens = tokenizer.encode(response, add_special_tokens=False)\n                \n                # Check if all tokens are in the English subset\n                if all(t in english_tokens for t in prompt_tokens + response_tokens):\n                    valid_examples.append(prompt)\n                    valid_responses.append(response)\n                    \n            prompts = valid_examples\n            formatted_responses = valid_responses\n\n        return {\n            \"prompt\": prompts,\n            \"completion\": formatted_responses,\n        }\n    \n    formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n    \n    # Analyze multi-token word handling\n    if config[\"analyze_tokenization\"]:\n        common_words = [example[\"answer\"] for example in dataset[:100]]\n        tokenization_analysis = analyze_multi_token_words(tokenizer, common_words)\n        \n        # Save analysis to file\n        import json\n        with open(\"results/unsloth/token_analysis/multi_token_analysis.json\", \"w\") as f:\n            json.dump(tokenization_analysis, f, indent=2)\n    \n    return formatted_dataset\n\n# Prepare position/count dataset for evaluation with Qwen3 tokenizer\ndef prepare_position_count_dataset(dataset, tokenizer, config):\n    # Get English token subset if filtering is enabled\n    english_tokens = get_english_token_subset(tokenizer) if config[\"use_english_only\"] else None\n    \n    def formatting_prompts_func(examples):\n        questions = examples[\"question\"]\n        answers = examples[\"answer\"]\n\n        # Standard prompt format (non-thinking mode only)\n        prompts = [\n            f\"<human>: {question}\\n<assistant>: \"\n            for question in questions\n        ]\n\n        # Format responses with EOS token\n        formatted_responses = [\n            f\"{answer}{tokenizer.eos_token}\"\n            for answer in answers\n        ]\n        \n        # Validate tokens if English-only filtering is enabled\n        if english_tokens:\n            valid_examples = []\n            valid_responses = []\n            valid_task_types = []\n            \n            for prompt, response, task_type in zip(prompts, formatted_responses, \n                                                 examples.get(\"task_type\", [\"position_count\"] * len(questions))):\n                prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n                response_tokens = tokenizer.encode(response, add_special_tokens=False)\n                \n                # Check if all tokens are in the English subset\n                if all(t in english_tokens for t in prompt_tokens + response_tokens):\n                    valid_examples.append(prompt)\n                    valid_responses.append(response)\n                    valid_task_types.append(task_type)\n                    \n            prompts = valid_examples\n            formatted_responses = valid_responses\n            task_types = valid_task_types\n        else:\n            task_types = examples.get(\"task_type\", [\"position_count\"] * len(questions))\n\n        return {\n            \"prompt\": prompts,\n            \"completion\": formatted_responses,\n            \"task_type\": task_types\n        }\n    \n    formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n    return formatted_dataset\n\n# Set up Unsloth trainer with dual dataset support\ndef create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, eval_dataset, config):\n    trainer = FastLanguageModel.get_trainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,  # Spelling validation dataset\n        args=FastLanguageModel.get_train_args(\n            output_dir=f\"./spelling-lora-{config['experiment_name']}\",\n            per_device_train_batch_size=config[\"training_config\"][\"per_device_train_batch_size\"],\n            gradient_accumulation_steps=config[\"training_config\"][\"gradient_accumulation_steps\"],\n            warmup_steps=config[\"training_config\"][\"warmup_steps\"],\n            max_steps=config[\"training_config\"][\"max_steps\"],\n            learning_rate=config[\"training_config\"][\"learning_rate\"],\n            fp16=not torch.cuda.is_bf16_supported(),\n            bf16=torch.cuda.is_bf16_supported(),\n            logging_steps=10,\n            evaluation_strategy=\"steps\",\n            eval_steps=100,\n            save_strategy=\"steps\",\n            save_steps=200,\n            optim=\"adamw_torch\",  # Unsloth recommends adamw_torch over paged_adamw_8bit\n            max_grad_norm=0.3,    # Gradient clipping - Unsloth recommended value\n            report_to=\"wandb\",\n        ),\n        data_collator=FastLanguageModel.get_data_collator(tokenizer=tokenizer),\n    )\n    \n    # Add position/count evaluation dataset as a custom attribute\n    trainer.position_count_dataset = eval_dataset\n    \n    # Add custom evaluation callback for position/count tasks\n    class PositionCountEvaluationCallback(TrainerCallback):\n        def on_evaluate(self, args, state, control, **kwargs):\n            # Run evaluation on position/count dataset\n            metrics = evaluate_position_count(trainer.model, trainer.tokenizer, \n                                             trainer.position_count_dataset, \n                                             config[\"evaluation_config\"])\n            # Log metrics to wandb\n            wandb.log({f\"position_count_{k}\": v for k, v in metrics.items()}, \n                      step=state.global_step)\n    \n    trainer.add_callback(PositionCountEvaluationCallback())\n    \n    return trainer\n\n# Evaluate model on position/count tasks\ndef evaluate_position_count(model, tokenizer, dataset, config):\n    # Set up metrics\n    metrics = {\n        \"position_accuracy\": 0.0,\n        \"count_accuracy\": 0.0,\n        \"overall_accuracy\": 0.0\n    }\n    \n    # Implement evaluation logic for position/count tasks\n    # This would generate predictions and compare against ground truth\n    \n    return metrics\n\n# Main training function\ndef train_with_unsloth(config_path):\n    # Load config\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    \n    # Initialize W&B\n    wandb.init(project=\"llm-spelling-finetuning\", name=config[\"experiment_name\"], config=config)\n    \n    # Optimize GPU memory\n    optimize_gpu_memory()\n    \n    # Load model and tokenizer\n    model, tokenizer = load_unsloth_model(config)\n    \n    # Load datasets\n    spelling_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\")\n    position_count_dataset = load_dataset(\"YOUR-USERNAME/llm-position-count-dataset\")\n    \n    # Prepare datasets for Unsloth with Qwen3 tokenizer\n    train_dataset = prepare_spelling_dataset(spelling_dataset[\"train\"], tokenizer, config)\n    val_dataset = prepare_spelling_dataset(spelling_dataset[\"validation\"], tokenizer, config)\n    eval_dataset = prepare_position_count_dataset(position_count_dataset[\"validation\"], tokenizer, config)\n    \n    # Create trainer\n    trainer = create_unsloth_trainer(model, tokenizer, train_dataset, val_dataset, eval_dataset, config)\n    \n    # Train model\n    trainer.train()\n    \n    # Save model\n    trainer.save_model()\n    \n    # Final evaluation on both datasets\n    spelling_eval_results = trainer.evaluate()\n    position_count_eval_results = evaluate_position_count(model, tokenizer, eval_dataset, config)\n    \n    # Log final results\n    wandb.log({\n        **spelling_eval_results,\n        **{f\"final_position_count_{k}\": v for k, v in position_count_eval_results.items()}\n    })\n    \n    # Generate comprehensive report\n    generate_evaluation_report(spelling_eval_results, position_count_eval_results, config)\n    \n    # Close W&B\n    wandb.finish()\n    \n    return {\n        \"spelling\": spelling_eval_results,\n        \"position_count\": position_count_eval_results\n    }\n\n# Generate comprehensive evaluation report\ndef generate_evaluation_report(spelling_results, position_count_results, config):\n    # Create HTML report with visualizations for both tasks\n    # Save to results/unsloth/reports/\n    pass\n```",
      "testStrategy": "1. Verify Unsloth installs and imports correctly in Google Colab or Lightning.ai\n2. Confirm memory usage is optimized compared to standard fine-tuning\n3. Test that 4-bit quantization is working correctly on cloud GPU\n4. Measure training speed improvement over baseline implementation\n5. Verify all Unsloth-specific optimizations are configured\n6. Test with a small dataset to ensure the training loop works in cloud environment\n7. Monitor GPU memory usage during training\n8. Verify that the implementation does not contain any local-only dependencies\n9. Test command-line interfaces for all scripts\n10. Verify HTML report generation functionality\n11. Test the integration between all Python modules\n12. Validate output directory structure and file generation\n13. Ensure proper error handling and logging in scripts\n14. Test loading and processing of both spelling and position/count datasets\n15. Verify that evaluation metrics for both tasks are correctly calculated and logged\n16. Test the transfer performance from spelling training to position/count evaluation\n17. Validate that memory usage remains optimized when handling both datasets\n18. Test the custom evaluation callback for position/count tasks\n19. Verify Lightning.AI Studio setup and configuration for data preparation\n20. Test data validation and quality check mechanisms\n21. Validate data versioning and tracking functionality\n22. Test Lightning DataModules integration with Unsloth training pipeline\n23. Verify efficient data streaming and caching mechanisms in Lightning.AI environment\n24. Test Qwen3-4B tokenizer handling for multi-token words\n25. Verify English-only token subset filtering functionality\n26. Test data preparation in non-thinking mode only\n27. Validate token analysis reporting for Qwen3 tokenization patterns\n28. Test compatibility of prepared data with Qwen3's tokenization requirements\n29. Verify that filtered datasets maintain sufficient size for effective training\n30. Test the impact of English-only token filtering on model performance",
      "subtasks": [
        {
          "id": 1,
          "title": "Environment Setup with Optimizations",
          "description": "Configure the cloud GPU environment with Unsloth and necessary dependencies for optimized LLM fine-tuning",
          "dependencies": [],
          "details": "Install Unsloth library and compatible dependencies (bitsandbytes, transformers, PEFT). Configure GPU memory settings for optimal performance. Set up quantization libraries. Verify hardware compatibility (CUDA, ROCm, or Metal backend). Test environment with basic model loading to ensure all components work together.\n<info added on 2025-05-07T14:48:05.432Z>\nInstall Unsloth library and compatible dependencies (bitsandbytes, transformers, PEFT). Configure GPU memory settings for optimal performance. Set up quantization libraries. Verify hardware compatibility (CUDA, ROCm, or Metal backend). Test environment with basic model loading to ensure all components work together.\n\nThis task can be worked on independently and in parallel with others. The environment setup has no dependencies and is parallelizable (parallelizable: true), allowing team members to begin this work immediately while other tasks are being planned or executed.\n</info added on 2025-05-07T14:48:05.432Z>\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Model Loading with Unsloth-specific Configurations",
          "description": "Implement efficient model loading using Unsloth's FastLanguageModel with proper quantization and LoRA setup in cloud GPU environment",
          "dependencies": [
            1
          ],
          "details": "Use FastLanguageModel.from_pretrained() to load base models with quantization in Google Colab or Lightning.ai. Configure LoRA adapters with get_peft_model() using appropriate rank and target modules. Implement proper quantization settings (4-bit, 8-bit) based on available cloud GPU VRAM. Set up gradient checkpointing with 'unsloth' option. Validate model loading with memory profiling.\n\nFile location:\n- Model loading and configuration: `src/unsloth/model.py`\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Dataset Preparation for Unsloth",
          "description": "Prepare and optimize training datasets for efficient processing with Unsloth using Lightning.AI Studios, with special handling for Qwen3-4B's tokenizer and English-only token subset",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated Lightning.AI Studio for data preparation following the \"one Studio, one task\" principle. Format dataset according to Unsloth and Qwen3-4B requirements using Lightning.AI's shared filesystem for efficient data storage and access. Implement Lightning DataModules for better integration with the training pipeline. Set up automated data validation and quality checks to ensure data integrity. Configure proper environment isolation and dependency management in the Studio. Use CPU-optimized instances for data processing tasks to optimize costs. Implement efficient data streaming and caching mechanisms. Set up automated data versioning and tracking for reproducibility.\n\nSpecific Qwen3-4B requirements:\n1. Implement functions to identify and filter for English-only token subset\n2. Create analysis tools for multi-token word handling in Qwen3's tokenization patterns\n3. Set up data preparation in non-thinking mode only\n4. Implement validation to ensure data compatibility with Qwen3's tokenization requirements\n5. Create reporting tools to analyze token usage patterns in the dataset\n\nFile locations:\n- Dataset preparation: `src/unsloth/dataset.py`\n- Lightning DataModules: `src/unsloth/datamodules.py`\n- Data validation: `src/unsloth/data_validation.py`\n- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`\n- Token filtering utilities: `src/unsloth/token_filter.py`\n\nOutput locations:\n- `results/unsloth/data_versions/` (Data version tracking)\n- `results/unsloth/token_analysis/` (Qwen3 token usage analysis)\n\n**NOTE: This task requires a Lightning.AI Studio environment. Documentation should include detailed setup instructions.**",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Trainer Setup with Memory Optimizations",
          "description": "Configure SFTTrainer with Unsloth-optimized parameters for efficient fine-tuning in cloud GPU environment",
          "dependencies": [
            2,
            3
          ],
          "details": "Set up SFTTrainer with optimized batch size and gradient accumulation in Google Colab or Lightning.ai. Configure learning rate and scheduler based on training duration. Implement proper precision settings (bf16/fp16) based on cloud GPU hardware support. Set up memory-efficient optimizers (adamw_8bit). Configure logging and checkpointing. Validate trainer setup with memory usage monitoring during initial training steps.\n\nFile location:\n- Training setup: `src/unsloth/trainer.py`\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Monitoring and Reporting System",
          "description": "Create comprehensive monitoring and reporting system for Unsloth training",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Implement training monitoring system with real-time metrics tracking. Create HTML report generation functionality to summarize training results. Develop visualization utilities for training metrics. Set up proper logging and error handling. Implement command-line interfaces for all scripts.\n\nFile locations:\n- Training monitoring: `src/unsloth/monitor.py`\n- HTML report generation: `src/unsloth/report.py`\n\nOutput locations:\n- `results/unsloth/figures/` (All PNG/PDF visualizations)\n- `results/unsloth/reports/` (HTML reports)\n- `results/unsloth/data/` (Training metrics)\n- `results/unsloth/configs/` (Model configurations)\n\n**NOTE: While development can be done locally, testing should be performed in a cloud GPU environment.**",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Command-line Interface and Integration",
          "description": "Develop command-line interfaces for all Unsloth scripts and ensure proper integration",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Create command-line interfaces for all Unsloth scripts to enable flexible usage. Implement proper argument parsing with sensible defaults. Ensure proper integration between all modules. Set up configuration file handling. Implement proper error handling and user feedback. Create comprehensive documentation for CLI usage.\n\nFile locations:\n- All Python scripts in `src/unsloth/`\n- Main CLI entry point: `src/unsloth/__main__.py`\n\n**NOTE: While development can be done locally, testing should be performed in a cloud GPU environment.**",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Dual Dataset Handling Implementation",
          "description": "Implement efficient handling of both spelling training data and position/count evaluation data with Qwen3-4B tokenizer support",
          "dependencies": [
            3
          ],
          "details": "Create separate data processing pipelines for spelling and position/count datasets. Implement efficient data loading and caching mechanisms for both datasets. Configure memory-efficient data handling during training and evaluation phases. Implement dataset-specific tokenization and formatting for Qwen3-4B. Validate dual dataset handling with performance metrics.\n\nQwen3-4B specific requirements:\n1. Implement English-only token subset filtering for both datasets\n2. Handle multi-token words appropriately in both datasets\n3. Support non-thinking mode only in data preparation\n4. Create analysis tools to validate token usage patterns\n5. Implement efficient caching of tokenized data to improve performance\n\nFile location:\n- Dataset preparation: `src/unsloth/dataset.py`\n- Dual dataset handler: `src/unsloth/dual_dataset.py`\n- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Position/Count Task Evaluation System",
          "description": "Develop evaluation system for position/count tasks during spelling variation training",
          "dependencies": [
            4,
            7
          ],
          "details": "Implement custom evaluation callback for position/count tasks. Create metrics calculation for position and count accuracy. Develop efficient evaluation pipeline that runs during training. Set up proper logging of transfer metrics. Implement visualization utilities for transfer performance. Create comprehensive reporting for both spelling and position/count performance.\n\nFile locations:\n- Evaluation utilities: `src/unsloth/evaluation.py`\n- Training monitoring: `src/unsloth/monitor.py`\n\nOutput locations:\n- `results/unsloth/evaluation/` (Position/count evaluation results)\n- `results/unsloth/figures/` (Transfer performance visualizations)\n\n**NOTE: This task requires a cloud GPU environment (Google Colab or Lightning.ai). Do not attempt on local Mac.**",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Lightning.AI Studio Documentation",
          "description": "Create comprehensive documentation for Lightning.AI Studio setup and configuration",
          "dependencies": [
            3
          ],
          "details": "Document the complete setup process for Lightning.AI Studios for data preparation. Include detailed instructions for environment configuration, dependency management, and resource allocation. Document the shared filesystem usage and data versioning approach. Create step-by-step guides for setting up CPU-optimized instances for data processing. Document the integration between Lightning DataModules and Unsloth training pipeline. Include troubleshooting guides and best practices for efficient data processing in Lightning.AI environment.\n\nFile location:\n- `docs/lightning_studio_setup.md`\n- `docs/data_processing_guide.md`\n\n**NOTE: This documentation should be comprehensive enough for new team members to set up and use Lightning.AI Studios for data preparation.**",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "Qwen3 Tokenizer Analysis and Optimization",
          "description": "Analyze and optimize data processing for Qwen3-4B's tokenizer patterns",
          "dependencies": [
            3
          ],
          "details": "Create comprehensive analysis tools for Qwen3-4B's tokenization patterns. Implement visualization utilities for token usage patterns. Develop optimization strategies for multi-token words. Create efficient filtering mechanisms for English-only token subset. Implement validation tools to ensure data compatibility with Qwen3's tokenization requirements.\n\nSpecific tasks:\n1. Create token frequency analysis tools for Qwen3-4B vocabulary\n2. Implement visualization utilities for token distribution in datasets\n3. Develop optimization strategies for handling multi-token words\n4. Create efficient filtering mechanisms for English-only token subset\n5. Implement validation tools to ensure data compatibility with Qwen3's tokenization\n\nFile locations:\n- Qwen3 tokenizer utilities: `src/unsloth/qwen3_tokenizer.py`\n- Token analysis tools: `src/unsloth/token_analysis.py`\n- Visualization utilities: `src/unsloth/token_visualization.py`\n\nOutput locations:\n- `results/unsloth/token_analysis/` (Qwen3 token usage analysis)\n- `results/unsloth/figures/token_distribution/` (Token distribution visualizations)\n\n**NOTE: While analysis can be done locally, validation should be performed with the actual Qwen3-4B tokenizer in a cloud environment.**",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "Non-Thinking Mode Implementation",
          "description": "Implement support for non-thinking mode in data preparation",
          "dependencies": [
            3,
            7
          ],
          "details": "Create data preparation pipelines that exclusively support non-thinking mode (enable_thinking=False). Ensure all code, configuration, and documentation enforce this project policy.\n\nSpecific tasks:\n1. Implement non-thinking mode formatting for direct response generation\n2. Create configuration validation to ensure thinking mode is never enabled\n3. Develop documentation that clearly states the non-thinking mode requirement\n4. Implement validation checks to prevent accidental use of thinking mode\n5. Create visualization utilities for token usage patterns in non-thinking mode\n\nFile locations:\n- Dataset preparation: `src/unsloth/dataset.py`\n- Configuration handling: `src/unsloth/config.py`\n- Configuration validation: `src/unsloth/config_validation.py`\n\nOutput locations:\n- `results/unsloth/token_analysis/` (Non-thinking mode token analysis)\n- `results/unsloth/figures/token_distribution/` (Token distribution visualizations)\n\n**NOTE: This task requires testing in a cloud GPU environment with the actual Qwen3-4B model.**",
          "status": "pending"
        }
      ]
    },
    {
      "id": 8,
      "title": "Model Fine-tuning and Experimentation",
      "description": "Implement the training loop and run experiments with different hyperparameters to find the optimal configuration for effective transfer learning using Qwen3-4B model in cloud GPU environments.",
      "status": "pending",
      "dependencies": [
        4,
        6,
        7
      ],
      "priority": "high",
      "details": "**IMPORTANT NOTE: This task requires a cloud GPU environment for Qwen3-4B fine-tuning. Do not attempt on local Mac.**\n\n**CLARIFICATION: Training is performed on spelling data, but evaluation is strictly limited to character position and character count tasks. All evaluation metrics, scripts, and documentation must focus exclusively on position and count. Spelling is never used as an evaluation metric.**\n\n1. Create a reusable training script that accepts hyperparameter configs (`src/training/train.py`)\n2. Implement the training loop using Unsloth with Qwen3-4B on Google Colab or https://lightning.ai/lars/home\n3. Set up checkpoint saving and loading system (`src/training/checkpointing.py`)\n4. Implement early stopping based on validation metrics\n5. Configure Qwen3-4B specific features:\n   - Non-thinking mode only (enable_thinking=False) as per project policy\n   - Sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n   - English-only token subset handling during training\n   - Adaptation to Qwen3's tokenizer patterns\n6. Run experiments with different hyperparameters focused on transfer learning effectiveness:\n   - LoRA rank (r): [4, 8, 16, 32]\n   - LoRA alpha: [8, 16, 32, 64]\n   - Learning rate: [1e-4, 2e-4, 5e-4, 1e-3]\n   - Batch size: [4, 8, 16, 32]\n   - Gradient accumulation steps: [1, 2, 4, 8]\n   - Training steps: [500, 1000, 2000, 5000]\n7. Track all experiments in W&B with position and character count metrics only\n8. Implement evaluation metrics for non-thinking mode\n9. Analyze transfer performance on position and character count tasks\n10. Identify training patterns that lead to better transfer learning\n\n**Transfer Learning Focus:**\n- Primary training on spelling variation tasks\n- Evaluate transfer learning effectiveness exclusively on position/count tasks\n- Track performance on position and character count tasks only\n- Identify which training approaches generalize better to position and count tasks\n- Never use spelling as an evaluation metric\n\n**File Structure:**\n- Training Infrastructure:\n  - Training script: `src/training/train.py`\n  - Training utilities: `src/training/utils.py`\n  - Data loaders: `src/training/data_loaders.py`\n  - Model checkpointing: `src/training/checkpointing.py`\n  - Transfer metrics: `src/training/transfer_metrics.py`\n  - Qwen3 utilities: `src/training/qwen3_utils.py`\n\n- Model Components:\n  - Model architecture: `src/models/spelling_model.py`\n  - Loss functions: `src/models/losses.py`\n  - Metrics tracking: `src/models/metrics.py`\n  - Model utilities: `src/models/utils.py`\n  - Transfer evaluation: `src/models/transfer_eval.py`\n\n- Deployment Components:\n  - Model export: `src/deployment/model_export.py`\n  - Lightning.AI Studio setup: `src/deployment/lightning_studio.py`\n  - API implementation: `src/deployment/api.py`\n  - Performance monitoring: `src/deployment/monitoring.py`\n  - Load testing: `src/deployment/benchmark.py`\n  - Performance visualization: `src/deployment/visualization.py`\n  - Report generation: `src/deployment/report.py`\n  - Transfer analysis: `src/deployment/transfer_analysis.py`\n  - Auto-scaling config: `src/deployment/scaling_config.py`\n\n- Configurations:\n  - Training config: `configs/training/config.yaml`\n  - Model config: `configs/models/model_config.yaml`\n  - Optimizer config: `configs/training/optimizer.yaml`\n  - Scheduler config: `configs/training/scheduler.yaml`\n  - Transfer config: `configs/training/transfer_config.yaml`\n  - Qwen3 config: `configs/models/qwen3_config.yaml`\n  - Lightning.AI deployment config: `configs/deployment/lightning_config.yaml`\n\n- Results and Checkpoints:\n  - Model checkpoints: `checkpoints/`\n  - Training logs: `results/training_logs/`\n  - Performance metrics: `results/metrics/`\n  - Transfer analysis: `results/transfer_analysis/`\n  - Deployment results: `results/deployment/`\n    - Figures: `results/deployment/figures/`\n    - Reports: `results/deployment/reports/`\n    - Data: `results/deployment/data/`\n    - Models: `results/deployment/models/`\n\n- Documentation:\n  - Training guide: `docs/training.md`\n  - Model architecture: `docs/model.md`\n  - Results analysis: `docs/results.md`\n  - Transfer learning analysis: `docs/transfer_learning.md`\n  - Lightning.AI deployment guide: `docs/lightning_deployment.md`\n  - Qwen3 specific guide: `docs/qwen3_guide.md`\n\nImplementation:\n```python\nimport os\nimport yaml\nimport torch\nimport wandb\nimport numpy as np\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Main experiment runner\ndef run_experiment(config_path):\n    # Load config\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    \n    # Initialize W&B\n    run = wandb.init(\n        project=\"llm-spelling-finetuning\",\n        name=config[\"experiment_name\"],\n        config=config,\n        reinit=True\n    )\n    \n    # Load model and tokenizer with Unsloth\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"Qwen/Qwen3-4B\",\n        max_seq_length=512,\n        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n        load_in_4bit=True\n    )\n    \n    # Configure Qwen3-specific sampling parameters\n    generation_config = model.generation_config\n    generation_config.temperature = 0.6\n    generation_config.top_p = 0.95\n    generation_config.top_k = 20\n    generation_config.min_p = 0.0\n    \n    # Add LoRA adapters\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=config[\"lora_config\"][\"r\"],\n        target_modules=config[\"lora_config\"][\"target_modules\"],\n        lora_alpha=config[\"lora_config\"][\"alpha\"],\n        lora_dropout=config[\"lora_config\"][\"dropout\"],\n        bias=\"none\",\n        use_gradient_checkpointing=True,\n        random_state=42\n    )\n    \n    # Load datasets - both training and transfer task datasets\n    spelling_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\")\n    position_dataset = load_dataset(\"YOUR-USERNAME/llm-position-dataset\")\n    count_dataset = load_dataset(\"YOUR-USERNAME/llm-count-dataset\")\n    \n    # Format dataset for instruction fine-tuning with Qwen3's format\n    def formatting_func(examples):\n        questions = examples[\"question\"]\n        answers = examples[\"answer\"]\n        \n        # Format with standard mode (non-thinking mode only as per project policy)\n        prompts = [f\"<|im_start|>user\\n{q}\\n<|im_end|>\\n<|im_start|>assistant\\n\" for q in questions]\n        completions = [f\"{a}<|im_end|>\" for a in answers]\n        \n        return {\"prompt\": prompts, \"completion\": completions}\n    \n    # Apply formatting\n    train_dataset = spelling_dataset[\"train\"].map(formatting_func, batched=True)\n    \n    # Format transfer task datasets for evaluation\n    position_eval_dataset = position_dataset[\"validation\"].map(formatting_func, batched=True)\n    count_eval_dataset = count_dataset[\"validation\"].map(formatting_func, batched=True)\n    \n    # Set up output directory\n    output_dir = f\"./results/{config['experiment_name']}_{config['timestamp']}\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create training arguments\n    training_args = FastLanguageModel.get_train_args(\n        output_dir=output_dir,\n        per_device_train_batch_size=config[\"training_config\"][\"per_device_train_batch_size\"],\n        gradient_accumulation_steps=config[\"training_config\"][\"gradient_accumulation_steps\"],\n        warmup_steps=config[\"training_config\"][\"warmup_steps\"],\n        max_steps=config[\"training_config\"][\"max_steps\"],\n        learning_rate=config[\"training_config\"][\"learning_rate\"],\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=10,\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        save_strategy=\"steps\",\n        save_steps=200,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_position_accuracy\",  # Use position task accuracy as primary metric\n        greater_is_better=True,\n        optim=\"adamw_torch\",\n        max_grad_norm=0.3,\n        report_to=\"wandb\"\n    )\n    \n    # Define compute metrics function for transfer learning evaluation\n    def compute_metrics(eval_pred):\n        predictions, labels = eval_pred\n        predictions = np.argmax(predictions, axis=2)\n        \n        # Only consider non-padding tokens\n        mask = labels != -100\n        labels = labels[mask]\n        predictions = predictions[mask]\n        \n        accuracy = accuracy_score(labels, predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n        \n        metrics = {\n            \"accuracy\": accuracy,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        }\n        \n        return metrics\n    \n    # Create trainer with early stopping\n    trainer = FastLanguageModel.get_trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=position_eval_dataset,  # Use position task as primary validation dataset\n        data_collator=FastLanguageModel.get_data_collator(tokenizer),\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n    )\n    \n    # Train model\n    trainer.train()\n    \n    # Save final model\n    trainer.save_model(f\"{output_dir}/final\")\n    \n    # Evaluate on transfer task datasets\n    position_eval_results = trainer.evaluate(eval_dataset=position_eval_dataset)\n    count_eval_results = trainer.evaluate(eval_dataset=count_eval_dataset)\n    \n    # Calculate transfer metrics\n    transfer_metrics = {\n        \"position_accuracy\": position_eval_results[\"eval_accuracy\"],\n        \"count_accuracy\": count_eval_results[\"eval_accuracy\"],\n        \"avg_transfer_score\": (position_eval_results[\"eval_accuracy\"] + count_eval_results[\"eval_accuracy\"]) / 2,\n        \"position_to_count_ratio\": position_eval_results[\"eval_accuracy\"] / count_eval_results[\"eval_accuracy\"] if count_eval_results[\"eval_accuracy\"] > 0 else 0\n    }\n    \n    # Log all results\n    wandb.log({\n        \"position_eval_results\": position_eval_results,\n        \"count_eval_results\": count_eval_results,\n        **transfer_metrics\n    })\n    \n    # Save evaluation results\n    all_results = {\n        \"position_eval_results\": position_eval_results,\n        \"count_eval_results\": count_eval_results,\n        \"transfer_metrics\": transfer_metrics\n    }\n    \n    with open(f\"{output_dir}/eval_results.yaml\", \"w\") as f:\n        yaml.dump(all_results, f)\n    \n    # Close wandb run\n    wandb.finish()\n    \n    return output_dir, all_results\n\n# Run multiple experiments\ndef run_experiments(config_paths):\n    results = {}\n    for config_path in config_paths:\n        print(f\"Running experiment with config: {config_path}\")\n        output_dir, eval_results = run_experiment(config_path)\n        \n        # Extract config name\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n        \n        results[config[\"experiment_name\"]] = {\n            \"output_dir\": output_dir,\n            \"eval_results\": eval_results\n        }\n    \n    # Analyze transfer learning effectiveness across experiments\n    analyze_transfer_effectiveness(results)\n    \n    # Save all results\n    with open(\"experiment_results_summary.yaml\", \"w\") as f:\n        yaml.dump(results, f)\n    \n    return results\n\n# Analyze transfer learning effectiveness\ndef analyze_transfer_effectiveness(results):\n    \"\"\"Analyze which training patterns lead to better transfer learning\"\"\"\n    # Extract key metrics for analysis\n    experiment_metrics = []\n    for exp_name, exp_data in results.items():\n        with open(f\"{exp_data['output_dir']}/config.yaml\", \"r\") as f:\n            config = yaml.safe_load(f)\n        \n        metrics = {\n            \"experiment_name\": exp_name,\n            \"lora_rank\": config[\"lora_config\"][\"r\"],\n            \"lora_alpha\": config[\"lora_config\"][\"alpha\"],\n            \"learning_rate\": config[\"training_config\"][\"learning_rate\"],\n            \"batch_size\": config[\"training_config\"][\"per_device_train_batch_size\"],\n            \"grad_accum_steps\": config[\"training_config\"][\"gradient_accumulation_steps\"],\n            \"training_steps\": config[\"training_config\"][\"max_steps\"],\n            \"position_accuracy\": exp_data[\"eval_results\"][\"position_eval_results\"][\"eval_accuracy\"],\n            \"count_accuracy\": exp_data[\"eval_results\"][\"count_eval_results\"][\"eval_accuracy\"],\n            \"avg_transfer_score\": exp_data[\"eval_results\"][\"transfer_metrics\"][\"avg_transfer_score\"]\n        }\n        experiment_metrics.append(metrics)\n    \n    # Calculate correlations between position and count tasks\n    import pandas as pd\n    df = pd.DataFrame(experiment_metrics)\n    correlation = df[[\"position_accuracy\", \"count_accuracy\", \"avg_transfer_score\"]].corr()\n    \n    # Identify top performing configurations for transfer learning\n    df_sorted = df.sort_values(by=\"avg_transfer_score\", ascending=False)\n    top_configs = df_sorted.head(5)\n    \n    # Save analysis results\n    os.makedirs(\"results/transfer_analysis\", exist_ok=True)\n    correlation.to_csv(\"results/transfer_analysis/metric_correlations.csv\")\n    top_configs.to_csv(\"results/transfer_analysis/top_transfer_configs.csv\")\n    \n    # Generate visualization of transfer learning effectiveness\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df[\"position_accuracy\"], df[\"count_accuracy\"])\n    plt.xlabel(\"Position Task Accuracy\")\n    plt.ylabel(\"Count Task Accuracy\")\n    plt.title(\"Correlation between Position and Count Task Performance\")\n    plt.savefig(\"results/transfer_analysis/position_vs_count.png\")\n    \n    # Log findings to wandb\n    wandb.init(project=\"llm-spelling-finetuning\", name=\"transfer_analysis\", reinit=True)\n    wandb.log({\n        \"correlation_matrix\": wandb.Table(dataframe=correlation),\n        \"top_transfer_configs\": wandb.Table(dataframe=top_configs),\n        \"position_vs_count_plot\": wandb.Image(\"results/transfer_analysis/position_vs_count.png\")\n    })\n    wandb.finish()\n```",
      "testStrategy": "1. Verify training script (`src/training/train.py`) runs without errors on Google Colab or lightning.ai with Qwen3-4B\n2. Confirm non-thinking mode is properly implemented and no thinking mode code exists in the codebase\n3. Confirm experiments are properly tracked in W&B with position and character count metrics only\n4. Verify Qwen3-specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) are correctly applied\n5. Test English-only token subset handling during training\n6. Check that checkpoints are saved correctly to `checkpoints/` directory and can be loaded properly\n7. Verify early stopping works as expected\n8. Test that the best model is loaded at the end of training\n9. Compare performance across different hyperparameter configurations using Python scripts\n10. Ensure all experiment results are properly saved to `results/metrics/` and `results/training_logs/`\n11. Verify the environment is properly set up with GPU access before starting experiments\n12. Validate that all configuration files in `configs/` directory are properly loaded and applied\n13. Test transfer learning evaluation on position and count tasks only\n14. Verify correlation analysis between position and count task performance\n15. Test the transfer analysis visualization generation\n16. Validate the identification of optimal training patterns for transfer learning\n17. Test deployment scripts in `src/deployment/` directory with Qwen3-4B models:\n    - Verify model export functionality in `model_export.py`\n    - Test Lightning.AI Studio setup in `lightning_studio.py`\n    - Test API implementation in `api.py`\n    - Validate monitoring capabilities in `monitoring.py`\n    - Check benchmark functionality in `benchmark.py`\n    - Test visualization generation in `visualization.py`\n    - Verify HTML report generation in `report.py`\n    - Test transfer analysis in `transfer_analysis.py`\n    - Validate auto-scaling configuration in `scaling_config.py`\n18. Ensure all deployment and analysis results are correctly saved to the appropriate directories\n19. Test Lightning.AI deployment with Qwen3-4B models:\n    - Verify dedicated deployment Studio creation\n    - Test environment isolation and dependency management\n    - Validate auto-scaling and load balancing configuration\n    - Check monitoring and logging setup\n    - Test cost optimization mechanisms\n    - Verify automated testing and validation pipeline\n20. Verify that no thinking mode references exist in any documentation, code, or configuration files\n21. Confirm that no spelling evaluation metrics or code exists in the implementation\n22. Verify that all evaluation focuses exclusively on position and character count tasks\n23. Run a comprehensive check to ensure no spelling evaluation metrics are tracked in W&B or any other logging system\n24. Verify all documentation clearly states that training is on spelling data but evaluation is exclusively on position and count tasks",
      "subtasks": [
        {
          "id": 1,
          "title": "Training Script Implementation",
          "description": "Develop a robust training script that handles the fine-tuning process for pre-trained models",
          "dependencies": [],
          "details": "Create a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.\n<info added on 2025-05-07T14:48:16.314Z>\nCreate a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.\n\nThis task can be worked on independently and in parallel with others. The training script implementation has no dependencies and is parallelizable (parallelizable: true).\n</info added on 2025-05-07T14:48:16.314Z>",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Checkpoint Management System",
          "description": "Implement a comprehensive checkpoint system to save and restore model states",
          "dependencies": [
            1
          ],
          "details": "Design a checkpoint manager in `src/training/checkpointing.py` that: saves model weights at configurable intervals, stores optimizer states, implements versioning for checkpoints, provides functionality to resume training from any checkpoint, includes cleanup mechanisms for old checkpoints, and ensures compatibility across different hardware configurations. All checkpoint operations should be compatible with Google Colab or lightning.ai cloud environments. Checkpoints should be saved to the `checkpoints/` directory with appropriate naming conventions.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Early Stopping Mechanism",
          "description": "Develop an early stopping system to prevent overfitting and optimize training time",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement a configurable early stopping mechanism that: monitors validation metrics (loss, accuracy), applies patience parameters to allow for fluctuations, saves best model states when improvements occur, provides restoration of best model after training, includes visualization of stopping point, and allows for custom stopping criteria definition. Ensure the implementation works reliably in cloud GPU environments like Google Colab or lightning.ai. The early stopping configuration should be defined in `configs/training/config.yaml` and the implementation should be integrated with the checkpoint system in `src/training/checkpointing.py`.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Hyperparameter Experimentation Framework",
          "description": "Create a framework for systematic hyperparameter tuning and experimentation",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Develop a hyperparameter experimentation system that: supports grid search and random search methods, enables parallel experiment execution, provides configuration management for experiments, implements parameter scheduling (learning rate decay), integrates with checkpoint system, and includes mechanisms to handle failed experiments gracefully. Design the framework to work efficiently in cloud GPU environments (Google Colab or lightning.ai) and to handle potential session timeouts or disconnections. Configuration files should be stored in the `configs/` directory with appropriate organization. Implement utilities in `src/training/utils.py` to support experiment management.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Results Tracking and Analysis System",
          "description": "Build a comprehensive system to track, visualize and compare experiment results",
          "dependencies": [
            4
          ],
          "details": "Implement a results management system that: stores metrics for all experiments in `results/metrics/`, generates comparative visualizations using Python scripts, calculates statistical significance of improvements, exports results in standard formats, provides filtering and sorting capabilities, and integrates with external visualization tools if needed. Ensure all results are properly saved to persistent storage accessible after cloud GPU sessions end. Implement error analysis functionality in `src/deployment/visualization.py` and `src/deployment/report.py` to help understand model performance and limitations. Use the deployment scripts to generate HTML reports and visualizations that will be saved to `results/deployment/reports/` and `results/deployment/figures/` respectively. Focus exclusively on position and character count task metrics in all visualizations and reports. Ensure no spelling evaluation metrics are tracked or displayed in any reports or visualizations.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Cloud Environment Setup Guide",
          "description": "Create documentation for setting up the required cloud GPU environment",
          "dependencies": [],
          "details": "Develop a comprehensive guide in `docs/training.md` for setting up the training environment on Google Colab or lightning.ai, including: step-by-step instructions for accessing GPU resources, installing Unsloth and other dependencies, configuring W&B integration, handling file storage and persistence, and troubleshooting common issues. Include examples of notebook configurations that work well for this specific fine-tuning task. Create additional documentation in `docs/model.md` for model architecture details and in `docs/results.md` for analyzing training results. Clearly document that while training is performed on spelling data, all evaluation is strictly limited to position and count tasks.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Deployment Scripts Implementation",
          "description": "Develop Python scripts for model deployment and performance analysis",
          "dependencies": [
            5
          ],
          "details": "Create a suite of Python scripts in the `src/deployment/` directory to handle all aspects of model deployment and analysis using Lightning.AI Studios:\n\n1. `model_export.py`: Implement model export and conversion functionality with command-line interface\n2. `lightning_studio.py`: Create dedicated deployment Studio setup following the \"one Studio, one task\" principle\n3. `api.py`: Create an API implementation using Lightning.AI's serving engine\n4. `monitoring.py`: Develop performance monitoring and logging capabilities\n5. `benchmark.py`: Implement load testing functionality\n6. `visualization.py`: Create performance visualization tools\n7. `report.py`: Develop HTML report generation\n8. `scaling_config.py`: Configure auto-scaling and load balancing for the deployment\n\nEnsure all scripts have proper command-line interfaces for flexibility and can be run independently. Implement proper Python packaging with clear separation of concerns. Configure environment isolation and dependency management for the Lightning.AI Studio. Implement cost optimization through efficient resource usage. Set up automated testing and validation in the deployment pipeline.\n\nAll output from these scripts should be saved to the appropriate directories under `results/deployment/`:\n- Figures: `results/deployment/figures/`\n- Reports: `results/deployment/reports/`\n- Performance data: `results/deployment/data/`\n- Exported models: `results/deployment/models/`\n\nCreate comprehensive deployment documentation in `docs/lightning_deployment.md` including Lightning.AI-specific setup instructions. Ensure all monitoring, visualization, and reporting focus exclusively on position and count task metrics, with no spelling evaluation metrics included.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Transfer Learning Evaluation System",
          "description": "Implement a system to evaluate transfer learning effectiveness across different tasks",
          "dependencies": [
            1,
            4
          ],
          "details": "Develop a transfer learning evaluation system in `src/training/transfer_metrics.py` and `src/models/transfer_eval.py` that: evaluates models trained on spelling tasks against position and count tasks only, calculates transfer metrics between position and count tasks, identifies which training patterns lead to better transfer, visualizes the relationship between position and count task performance, and generates comprehensive reports on transfer learning effectiveness. The system should integrate with the existing experimentation framework and results tracking system. All transfer analysis results should be saved to `results/transfer_analysis/` directory. Ensure the system focuses exclusively on position and character count tasks as per project policy. Implement strict validation to ensure no spelling evaluation metrics are tracked or reported.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Transfer Learning Documentation",
          "description": "Create comprehensive documentation on transfer learning analysis and findings",
          "dependencies": [
            8
          ],
          "details": "Develop detailed documentation in `docs/transfer_learning.md` that explains: the transfer learning evaluation methodology, metrics used to assess transfer effectiveness between position and count tasks, analysis of correlation between position and count task performance, identification of optimal training patterns for transfer learning, visualization of key findings, and recommendations for maximizing transfer learning effectiveness. Include examples and case studies from the experiments to illustrate important concepts and findings. Ensure all documentation focuses exclusively on position and character count tasks as per project policy. Clearly document that while training is performed on spelling data, all evaluation is strictly limited to position and count tasks, and spelling is never used as an evaluation metric.",
          "status": "pending"
        },
        {
          "id": 12,
          "title": "Qwen3-4B Non-Thinking Mode Implementation",
          "description": "Implement support for Qwen3-4B's non-thinking mode only",
          "dependencies": [
            1
          ],
          "details": "Create utilities in `src/training/qwen3_utils.py` to support Qwen3-specific features including handling of the English-only token subset during training and adapting to Qwen3's tokenizer patterns. Ensure all implementations strictly follow the project policy of using non-thinking mode only (enable_thinking=False). Create configuration options in `configs/models/qwen3_config.yaml` to control Qwen3-specific parameters. Document the implementation details and usage guidelines in `docs/qwen3_guide.md`, emphasizing that only non-thinking mode is supported.",
          "status": "pending"
        },
        {
          "id": 13,
          "title": "Qwen3-4B Sampling Parameters Configuration",
          "description": "Implement configuration for Qwen3-4B's specific sampling parameters",
          "dependencies": [
            1,
            12
          ],
          "details": "Create a configuration system for Qwen3-4B's sampling parameters in `configs/models/qwen3_config.yaml` that includes Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 as default values. Implement utilities in `src/training/qwen3_utils.py` to apply these parameters during model initialization and inference. Add functionality to experiment with different sampling parameter combinations and analyze their impact on model performance. Document the sampling parameters and their effects in `docs/qwen3_guide.md`.",
          "status": "pending"
        },
        {
          "id": 14,
          "title": "Code Review for Thinking Mode References",
          "description": "Review all code to ensure no thinking mode references remain",
          "dependencies": [
            1,
            12,
            13
          ],
          "details": "Perform a comprehensive review of all code, configuration files, and documentation to ensure that no references to thinking mode remain. This includes checking for any code that might enable thinking mode, any configuration options related to thinking mode, and any documentation that mentions thinking mode. Create a verification script that can scan the codebase for thinking mode references and report any findings. Document the verification process and results to confirm compliance with the project policy of using non-thinking mode only.",
          "status": "pending"
        },
        {
          "id": 15,
          "title": "Remove Spelling Evaluation References",
          "description": "Ensure all evaluation focuses exclusively on position and character count tasks",
          "dependencies": [
            1,
            5,
            8
          ],
          "details": "Perform a comprehensive review of all code, configuration files, and documentation to ensure that no references to spelling evaluation remain. This includes removing any code that evaluates spelling performance, any configuration options related to spelling metrics, and any documentation that mentions spelling evaluation. Create a verification script that can scan the codebase for spelling evaluation references and report any findings. Update all evaluation scripts, metrics tracking, and visualization code to focus exclusively on position and character count tasks as per project policy. Ensure that W&B logging, reports, and all analysis focus only on position and count task metrics.",
          "status": "pending"
        },
        {
          "id": 16,
          "title": "Training vs. Evaluation Documentation",
          "description": "Clearly document the distinction between training and evaluation tasks",
          "dependencies": [
            6,
            9
          ],
          "details": "Create clear documentation that explicitly states the distinction between training data (spelling tasks) and evaluation metrics (position and count tasks only). Update all existing documentation to reflect this distinction. Add prominent notes in all relevant files including README.md, training guides, evaluation scripts, and configuration files. Create a dedicated section in the documentation that explains the transfer learning approach: training on spelling tasks but evaluating exclusively on position and count tasks. Ensure this distinction is clear to all users of the codebase.",
          "status": "pending"
        },
        {
          "id": 17,
          "title": "Metrics Validation System",
          "description": "Implement a validation system to ensure no spelling metrics are tracked",
          "dependencies": [
            5,
            8,
            15
          ],
          "details": "Develop a validation system that checks all metrics being tracked and reported to ensure they are exclusively related to position and count tasks. Implement this as a pre-commit hook and as part of the CI/CD pipeline. Create a whitelist of allowed metrics and validate that only these metrics are being tracked in W&B, saved to result files, or displayed in visualizations. Add assertions in the code to prevent accidental tracking of spelling metrics. Create a comprehensive test suite that verifies no spelling metrics are being tracked or reported.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 9,
      "title": "Comprehensive Model Evaluation",
      "description": "Evaluate the fine-tuned models on the test set using multiple metrics and perform detailed analysis of transfer learning effectiveness from spelling training to position/count tasks, with specific focus on Qwen3-4B's non-thinking mode, English token subset, and sampling parameters, leveraging Lightning.AI Studios for efficient GPU-based evaluation.",
      "status": "pending",
      "dependencies": [
        8
      ],
      "priority": "medium",
      "details": "1. Evaluate the best model on the test set with dedicated pipelines for position/count tasks only\n2. Implement comprehensive evaluation metrics for position/count tasks:\n   - Letter Count Accuracy\n   - Letter Position Accuracy\n   - Character-Level Accuracy for position/count responses\n   - Levenshtein Distance for position/count responses\n   - Token-Level Perplexity for position/count tasks\n   - Transfer Learning Effectiveness Metrics (from spelling training to position/count performance)\n3. Evaluate Qwen3-4B in non-thinking mode only (enable_thinking=False)\n4. Assess performance with the English-only token subset\n5. Analyze the impact of Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n6. Perform detailed error analysis for position and count tasks only\n7. Investigate transfer learning patterns from spelling training to position/count performance\n8. Analyze model's generalization capabilities on position/count tasks\n9. Create visualizations of the position/count results\n10. Compare position/count performance between base and fine-tuned models\n11. Evaluate the effectiveness of the English token filtering approach for position/count tasks\n12. Implement task-specific evaluation metrics and visualization tools for position/count tasks\n13. Ensure proper handling of Qwen3's tokenizer patterns in position/count evaluation\n\nNOTE: Use Lightning.AI Studios for all model evaluation, leveraging their GPU switching feature (CPU → T4 → A100) for cost-effective evaluation.\n\nFile Structure:\n- Main evaluator: `src/evaluation/position_count_evaluator.py`\n- Metrics calculator: `src/evaluation/metrics.py`\n- Error analyzer: `src/evaluation/error_analysis.py`\n- Transfer learning analyzer: `src/evaluation/transfer_analysis.py`\n- Visualization utils: `src/evaluation/visualization.py`\n- Qwen3 evaluator: `src/evaluation/qwen3_evaluator.py`\n- English token analyzer: `src/evaluation/token_analysis.py`\n- Test data: `data/splits/test.json`\n- Challenge sets: `data/splits/challenge_sets/`\n- Edge cases: `data/splits/edge_cases/`\n- Error categories: `data/splits/error_categories/`\n- Evaluation results: `results/evaluation/`\n- Performance metrics: `results/evaluation/metrics/`\n- Error analysis: `results/evaluation/error_analysis/`\n- Transfer learning analysis: `results/evaluation/transfer_analysis/`\n- Token subset analysis: `results/evaluation/token_analysis/`\n- Visualizations: `results/evaluation/plots/`\n\nImplementation:\n```python\nimport torch\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport Levenshtein\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nimport wandb\nfrom scipy.stats import pearsonr, spearmanr\n\n# Load models for comparison\ndef load_models(base_model_name, finetuned_model_path):\n    # Load base model\n    base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n    base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n    base_tokenizer.pad_token = base_tokenizer.eos_token\n    \n    # Load fine-tuned model\n    finetuned_model = AutoModelForCausalLM.from_pretrained(finetuned_model_path)\n    finetuned_tokenizer = AutoTokenizer.from_pretrained(base_model_name)  # Use same tokenizer\n    finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token\n    \n    return {\n        \"base\": (base_model, base_tokenizer),\n        \"finetuned\": (finetuned_model, finetuned_tokenizer)\n    }\n\n# Generate answer from model with support for Qwen3 in non-thinking mode\ndef generate_answer(model, tokenizer, question, max_length=10, \n                   temperature=0.6, top_p=0.95, top_k=20, min_p=0, english_only=False):\n    inputs = tokenizer(question, return_tensors=\"pt\")\n    \n    # Configure generation parameters based on Qwen3 specifications\n    generation_config = {\n        \"max_length\": len(inputs.input_ids[0]) + max_length,\n        \"pad_token_id\": tokenizer.eos_token_id,\n        \"do_sample\": temperature > 0,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"top_k\": top_k,\n    }\n    \n    # Add min_p if supported by the model\n    if hasattr(model.config, \"min_p\"):\n        generation_config[\"min_p\"] = min_p\n    \n    # For Qwen3, explicitly disable thinking mode\n    if \"qwen\" in tokenizer.name_or_path.lower():\n        generation_config[\"thinking\"] = False\n    \n    # Apply English-only token filtering if requested\n    if english_only:\n        # Implementation would depend on the specific model and tokenizer\n        # This is a placeholder for the actual implementation\n        pass\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs.input_ids,\n            **generation_config\n        )\n    \n    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n    \n    # Extract just the first token/character for letter position or first number for letter count\n    if \"How many\" in question:\n        # Extract first number\n        import re\n        numbers = re.findall(r'\\d+', response)\n        return numbers[0] if numbers else response.strip()\n    elif \"What is the letter\" in question:\n        # Extract first character\n        return response.strip()[0] if response.strip() else \"\"\n    else:\n        # For other tasks, return the full response\n        return response.strip()\n\n# Filter to English-only tokens for Qwen3\ndef filter_english_tokens(tokenizer, input_ids):\n    # This is a simplified example - actual implementation would depend on Qwen3's token structure\n    english_token_ids = [id for id in range(tokenizer.vocab_size) if is_english_token(tokenizer, id)]\n    english_token_set = set(english_token_ids)\n    \n    # Filter logits to only allow English tokens\n    def filter_fn(input_ids, scores):\n        for batch_idx in range(scores.shape[0]):\n            for token_idx in range(scores.shape[1]):\n                if token_idx not in english_token_set:\n                    scores[batch_idx, token_idx] = -float('inf')\n        return scores\n    \n    return filter_fn\n\n# Helper function to determine if a token is English\ndef is_english_token(tokenizer, token_id):\n    token = tokenizer.convert_ids_to_tokens(token_id)\n    # Simple heuristic - can be improved with more sophisticated analysis\n    return all(c.isascii() and (c.isalnum() or c.isspace() or c in string.punctuation) for c in token)\n\n# Calculate letter count accuracy\ndef calc_letter_count_accuracy(model, tokenizer, test_dataset, english_only=False):\n    correct = 0\n    total = 0\n    results = []\n    \n    for item in test_dataset:\n        if item[\"question_type\"] != \"letter_count\":\n            continue\n            \n        prediction = generate_answer(model, tokenizer, item[\"question\"], english_only=english_only)\n        is_correct = prediction == item[\"answer\"]\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"correct\": is_correct,\n            \"word\": item[\"word\"],\n            \"english_only\": english_only\n        })\n        \n        correct += int(is_correct)\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"Letter Count Accuracy (English-only: {english_only}): {accuracy:.4f} ({correct}/{total})\")\n    \n    return accuracy, results\n\n# Calculate letter position accuracy\ndef calc_letter_position_accuracy(model, tokenizer, test_dataset, english_only=False):\n    correct = 0\n    total = 0\n    results = []\n    \n    for item in test_dataset:\n        if item[\"question_type\"] != \"letter_position\":\n            continue\n            \n        prediction = generate_answer(model, tokenizer, item[\"question\"], english_only=english_only)\n        is_correct = prediction.lower() == item[\"answer\"].lower()\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"correct\": is_correct,\n            \"word\": item[\"word\"],\n            \"english_only\": english_only\n        })\n        \n        correct += int(is_correct)\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"Letter Position Accuracy (English-only: {english_only}): {accuracy:.4f} ({correct}/{total})\")\n    \n    return accuracy, results\n\n# Calculate character-level accuracy for position/count tasks\ndef calc_character_level_accuracy(model, tokenizer, test_dataset, english_only=False):\n    total_char_accuracy = 0\n    total_samples = 0\n    results = []\n\n    for item in test_dataset:\n        if item[\"question_type\"] not in [\"letter_position\", \"letter_count\"]:\n            continue\n\n        prediction = generate_answer(model, tokenizer, item[\"question\"], english_only=english_only)\n        pred_chars = prediction.strip().lower().replace(\" \", \"\")\n        true_chars = item[\"answer\"].lower()\n\n        # Calculate character-by-character accuracy\n        correct_chars = 0\n        for i, char in enumerate(true_chars):\n            if i < len(pred_chars) and pred_chars[i] == char:\n                correct_chars += 1\n\n        char_accuracy = correct_chars / len(true_chars) if len(true_chars) > 0 else 0\n        \n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": item[\"answer\"],\n            \"prediction\": prediction,\n            \"char_accuracy\": char_accuracy,\n            \"word\": item[\"word\"],\n            \"question_type\": item[\"question_type\"],\n            \"english_only\": english_only\n        })\n        \n        total_char_accuracy += char_accuracy\n        total_samples += 1\n\n    avg_char_accuracy = total_char_accuracy / total_samples if total_samples > 0 else 0\n    print(f\"Character-Level Accuracy (English-only: {english_only}): {avg_char_accuracy:.4f}\")\n    \n    return avg_char_accuracy, results\n\n# Calculate Levenshtein distance for position/count tasks\ndef calc_levenshtein_metrics(model, tokenizer, test_dataset, english_only=False):\n    total_distances = 0\n    total_samples = 0\n    results = []\n\n    for item in test_dataset:\n        if item[\"question_type\"] not in [\"letter_position\", \"letter_count\"]:\n            continue\n\n        prediction = generate_answer(model, tokenizer, item[\"question\"], english_only=english_only)\n        pred_text = prediction.strip().lower()\n        true_text = item[\"answer\"].lower()\n\n        # Calculate Levenshtein distance\n        distance = Levenshtein.distance(pred_text, true_text)\n        normalized_distance = distance / max(len(pred_text), len(true_text)) if max(len(pred_text), len(true_text)) > 0 else 0\n\n        results.append({\n            \"question\": item[\"question\"],\n            \"expected\": true_text,\n            \"prediction\": pred_text,\n            \"levenshtein_distance\": distance,\n            \"normalized_distance\": normalized_distance,\n            \"word\": item[\"word\"],\n            \"question_type\": item[\"question_type\"],\n            \"english_only\": english_only\n        })\n        \n        total_distances += normalized_distance\n        total_samples += 1\n\n    avg_distance = total_distances / total_samples if total_samples > 0 else 0\n    print(f\"Average Normalized Levenshtein Distance (English-only: {english_only}): {avg_distance:.4f}\")\n    \n    return avg_distance, results\n\n# Analyze transfer learning effectiveness from spelling training to position/count tasks\ndef analyze_transfer_learning(model_info, position_count_results):\n    # Extract model training info to understand spelling training impact\n    training_words = model_info.get(\"training_words\", [])\n    training_metrics = model_info.get(\"training_metrics\", {})\n    \n    # Map position/count performance to corresponding training data\n    transfer_data = []\n    for result in position_count_results:\n        word = result[\"word\"]\n        # Check if this word was in the training data\n        was_trained = word in training_words\n        \n        transfer_data.append({\n            \"word\": word,\n            \"was_trained\": was_trained,\n            \"task_performance\": 1 if result[\"correct\"] else 0,\n            \"question_type\": result[\"question_type\"]\n        })\n    \n    # Calculate performance difference between trained and untrained words\n    trained_words_results = [item for item in transfer_data if item[\"was_trained\"]]\n    untrained_words_results = [item for item in transfer_data if not item[\"was_trained\"]]\n    \n    trained_performance = sum([item[\"task_performance\"] for item in trained_words_results]) / len(trained_words_results) if trained_words_results else 0\n    untrained_performance = sum([item[\"task_performance\"] for item in untrained_words_results]) / len(untrained_words_results) if untrained_words_results else 0\n    \n    transfer_effect = trained_performance - untrained_performance\n    \n    # Separate by question type\n    position_data = [item for item in transfer_data if item[\"question_type\"] == \"letter_position\"]\n    count_data = [item for item in transfer_data if item[\"question_type\"] == \"letter_count\"]\n    \n    # Calculate type-specific transfer effects\n    position_trained = [item for item in position_data if item[\"was_trained\"]]\n    position_untrained = [item for item in position_data if not item[\"was_trained\"]]\n    \n    count_trained = [item for item in count_data if item[\"was_trained\"]]\n    count_untrained = [item for item in count_data if not item[\"was_trained\"]]\n    \n    position_trained_perf = sum([item[\"task_performance\"] for item in position_trained]) / len(position_trained) if position_trained else 0\n    position_untrained_perf = sum([item[\"task_performance\"] for item in position_untrained]) / len(position_untrained) if position_untrained else 0\n    position_transfer_effect = position_trained_perf - position_untrained_perf\n    \n    count_trained_perf = sum([item[\"task_performance\"] for item in count_trained]) / len(count_trained) if count_trained else 0\n    count_untrained_perf = sum([item[\"task_performance\"] for item in count_untrained]) / len(count_untrained) if count_untrained else 0\n    count_transfer_effect = count_trained_perf - count_untrained_perf\n    \n    # Identify successful and unsuccessful transfer cases\n    successful_transfer = [item for item in transfer_data if item[\"was_trained\"] and item[\"task_performance\"] == 1]\n    unsuccessful_transfer = [item for item in transfer_data if item[\"was_trained\"] and item[\"task_performance\"] == 0]\n    \n    # Group words by patterns\n    pattern_performance = {}\n    for item in transfer_data:\n        word = item[\"word\"]\n        # Identify patterns (this is a simplified example - expand as needed)\n        patterns = []\n        if 'ie' in word or 'ei' in word:\n            patterns.append('ie_ei_rule')\n        if word.endswith('e') and any(word.endswith(f'{c}e') for c in 'aeiou'):\n            patterns.append('silent_e')\n        if any(c*2 in word for c in 'abcdefghijklmnopqrstuvwxyz'):\n            patterns.append('double_letter')\n        \n        for pattern in patterns:\n            if pattern not in pattern_performance:\n                pattern_performance[pattern] = {\n                    \"position\": [],\n                    \"count\": []\n                }\n            \n            if item[\"question_type\"] == \"letter_position\":\n                pattern_performance[pattern][\"position\"].append(item[\"task_performance\"])\n            elif item[\"question_type\"] == \"letter_count\":\n                pattern_performance[pattern][\"count\"].append(item[\"task_performance\"])\n    \n    # Calculate average performance by pattern\n    for pattern in pattern_performance:                \n        if pattern_performance[pattern][\"position\"]:\n            pattern_performance[pattern][\"avg_position\"] = sum(pattern_performance[pattern][\"position\"]) / len(pattern_performance[pattern][\"position\"])\n        else:\n            pattern_performance[pattern][\"avg_position\"] = 0\n            \n        if pattern_performance[pattern][\"count\"]:\n            pattern_performance[pattern][\"avg_count\"] = sum(pattern_performance[pattern][\"count\"]) / len(pattern_performance[pattern][\"count\"])\n        else:\n            pattern_performance[pattern][\"avg_count\"] = 0\n    \n    transfer_analysis = {\n        \"overall_transfer_effect\": transfer_effect,\n        \"position_transfer_effect\": position_transfer_effect,\n        \"count_transfer_effect\": count_transfer_effect,\n        \"successful_transfer\": {\n            \"count\": len(successful_transfer),\n            \"examples\": successful_transfer[:10]  # Limit to 10 examples\n        },\n        \"unsuccessful_transfer\": {\n            \"count\": len(unsuccessful_transfer),\n            \"examples\": unsuccessful_transfer[:10]  # Limit to 10 examples\n        },\n        \"pattern_performance\": pattern_performance\n    }\n    \n    return transfer_analysis, transfer_data\n\n# Perform error analysis for position/count tasks\ndef perform_error_analysis(results_dict):\n    error_analysis = {}\n    \n    # Analyze letter count errors\n    count_errors = [r for r in results_dict[\"letter_count\"] if not r[\"correct\"]]\n    \n    # Categorize errors\n    error_types = {\n        \"off_by_one\": 0,\n        \"completely_wrong\": 0,\n        \"no_number\": 0,\n        \"other\": 0\n    }\n    \n    for error in count_errors:\n        try:\n            pred = int(error[\"prediction\"])\n            true = int(error[\"expected\"])\n            \n            if abs(pred - true) == 1:\n                error_types[\"off_by_one\"] += 1\n            else:\n                error_types[\"completely_wrong\"] += 1\n        except ValueError:\n            if not error[\"prediction\"].strip():\n                error_types[\"no_number\"] += 1\n            else:\n                error_types[\"other\"] += 1\n    \n    # Analyze letter position errors\n    position_errors = [r for r in results_dict[\"letter_position\"] if not r[\"correct\"]]\n    \n    # Categorize position errors\n    position_error_types = {\n        \"adjacent_letter\": 0,\n        \"wrong_case\": 0,\n        \"no_response\": 0,\n        \"other\": 0\n    }\n    \n    for error in position_errors:\n        if not error[\"prediction\"].strip():\n            position_error_types[\"no_response\"] += 1\n        elif error[\"prediction\"].lower() == error[\"expected\"].lower():\n            position_error_types[\"wrong_case\"] += 1\n        elif error[\"word\"] and error[\"prediction\"] in error[\"word\"]:\n            position_error_types[\"adjacent_letter\"] += 1\n        else:\n            position_error_types[\"other\"] += 1\n    \n    # Analyze by word length\n    word_length_performance = {}\n    \n    for result in results_dict[\"letter_count\"] + results_dict[\"letter_position\"]:\n        word = result[\"word\"]\n        length = len(word)\n        task_type = \"position\" if result in results_dict[\"letter_position\"] else \"count\"\n        \n        if length not in word_length_performance:\n            word_length_performance[length] = {\n                \"position\": {\"correct\": 0, \"total\": 0},\n                \"count\": {\"correct\": 0, \"total\": 0}\n            }\n        \n        word_length_performance[length][task_type][\"total\"] += 1\n        if result[\"correct\"]:\n            word_length_performance[length][task_type][\"correct\"] += 1\n    \n    # Calculate accuracy by word length\n    for length, stats in word_length_performance.items():\n        for task_type in [\"position\", \"count\"]:\n            stats[task_type][\"accuracy\"] = stats[task_type][\"correct\"] / stats[task_type][\"total\"] if stats[task_type][\"total\"] > 0 else 0\n    \n    error_analysis = {\n        \"letter_count_errors\": error_types,\n        \"letter_position_errors\": position_error_types,\n        \"word_length_performance\": word_length_performance\n    }\n    \n    return error_analysis\n\n# Create performance dashboard for position/count tasks\ndef create_performance_dashboard(results, error_analysis, transfer_analysis):\n    # Set up figure\n    fig = plt.figure(figsize=(20, 16))\n    \n    # 1. Accuracy comparison across metrics\n    ax1 = fig.add_subplot(3, 3, 1)\n    metrics = ['letter_count_accuracy', 'letter_position_accuracy', 'character_level_accuracy']\n    \n    # Extract values\n    values = [results.get(m, 0) for m in metrics]\n    \n    ax1.bar(range(len(metrics)), values)\n    ax1.set_ylabel('Accuracy')\n    ax1.set_title('Position/Count Accuracy Comparison')\n    ax1.set_xticks(range(len(metrics)))\n    ax1.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n    \n    # 2. Error analysis for letter count\n    ax2 = fig.add_subplot(3, 3, 2)\n    error_types = list(error_analysis[\"letter_count_errors\"].keys())\n    error_counts = [error_analysis[\"letter_count_errors\"][t] for t in error_types]\n    \n    ax2.bar(error_types, error_counts)\n    ax2.set_title('Letter Count Error Types')\n    ax2.set_ylabel('Count')\n    plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\")\n    \n    # 3. Error analysis for letter position\n    ax3 = fig.add_subplot(3, 3, 3)\n    pos_error_types = list(error_analysis[\"letter_position_errors\"].keys())\n    pos_error_counts = [error_analysis[\"letter_position_errors\"][t] for t in pos_error_types]\n    \n    ax3.bar(pos_error_types, pos_error_counts)\n    ax3.set_title('Letter Position Error Types')\n    ax3.set_ylabel('Count')\n    plt.setp(ax3.get_xticklabels(), rotation=45, ha=\"right\")\n    \n    # 4. Performance by word length\n    ax4 = fig.add_subplot(3, 3, 4)\n    length_perf = error_analysis[\"word_length_performance\"]\n    \n    # Get all word lengths\n    all_lengths = sorted(length_perf.keys())\n    \n    # Get position accuracy by word length\n    pos_acc = [length_perf.get(l, {}).get(\"position\", {}).get(\"accuracy\", 0) for l in all_lengths]\n    \n    ax4.plot(all_lengths, pos_acc, marker='o', label='Position')\n    \n    # Get count accuracy by word length\n    count_acc = [length_perf.get(l, {}).get(\"count\", {}).get(\"accuracy\", 0) for l in all_lengths]\n    \n    ax4.plot(all_lengths, count_acc, marker='^', linestyle='--', label='Count')\n    \n    ax4.set_title('Performance by Word Length')\n    ax4.set_xlabel('Word Length')\n    ax4.set_ylabel('Accuracy')\n    ax4.legend()\n    \n    # 5. Transfer learning effect\n    ax5 = fig.add_subplot(3, 3, 5)\n    \n    # Extract transfer effects\n    transfer_effects = [\n        transfer_analysis.get(\"overall_transfer_effect\", 0),\n        transfer_analysis.get(\"position_transfer_effect\", 0),\n        transfer_analysis.get(\"count_transfer_effect\", 0)\n    ]\n    \n    ax5.bar([\"Overall\", \"Position\", \"Count\"], transfer_effects)\n    ax5.set_title('Transfer Effect from Spelling Training')\n    ax5.set_ylabel('Performance Difference (Trained - Untrained)')\n    \n    # 6. Pattern performance\n    ax6 = fig.add_subplot(3, 3, 6)\n    \n    # Get patterns\n    patterns = transfer_analysis.get(\"pattern_performance\", {})\n    \n    all_patterns = list(patterns.keys())\n    \n    if all_patterns:\n        # Get position performance for each pattern\n        pattern_pos = [patterns.get(p, {}).get(\"avg_position\", 0) for p in all_patterns]\n        \n        # Get count performance for each pattern\n        pattern_count = [patterns.get(p, {}).get(\"avg_count\", 0) for p in all_patterns]\n        \n        # Set up grouped bar chart\n        x = np.arange(len(all_patterns))\n        width = 0.35\n        \n        ax6.bar(x - width/2, pattern_pos, width, label='Position')\n        ax6.bar(x + width/2, pattern_count, width, label='Count')\n        \n        ax6.set_title('Pattern Performance')\n        ax6.set_xticks(x)\n        ax6.set_xticklabels(all_patterns)\n        ax6.set_ylabel('Accuracy')\n        ax6.legend()\n        plt.setp(ax6.get_xticklabels(), rotation=45, ha=\"right\")\n    else:\n        ax6.text(0.5, 0.5, 'No pattern data available', \n                horizontalalignment='center', verticalalignment='center')\n    \n    # 7. Levenshtein distance\n    ax7 = fig.add_subplot(3, 3, 7)\n    lev = results.get('levenshtein_distance', 0)\n    \n    ax7.bar(['Normalized Levenshtein Distance'], [lev])\n    ax7.set_title('Normalized Levenshtein Distance')\n    ax7.set_ylabel('Distance (lower is better)')\n    \n    # 8. Transfer success/failure counts\n    ax8 = fig.add_subplot(3, 3, 8)\n    \n    success = transfer_analysis.get(\"successful_transfer\", {}).get(\"count\", 0)\n    failure = transfer_analysis.get(\"unsuccessful_transfer\", {}).get(\"count\", 0)\n    \n    ax8.bar(['Successful Transfer', 'Unsuccessful Transfer'], [success, failure])\n    ax8.set_title('Transfer Learning Success/Failure')\n    ax8.set_ylabel('Count')\n    \n    # 9. English-only vs All tokens comparison\n    if 'english_only' in results and 'all_tokens' in results:\n        ax9 = fig.add_subplot(3, 3, 9)\n        \n        metrics = ['letter_count_accuracy', 'letter_position_accuracy']\n        english_values = [results['english_only'].get(m, 0) for m in metrics]\n        all_values = [results['all_tokens'].get(m, 0) for m in metrics]\n        \n        x = np.arange(len(metrics))\n        width = 0.35\n        \n        ax9.bar(x - width/2, english_values, width, label='English-only')\n        ax9.bar(x + width/2, all_values, width, label='All tokens')\n        \n        ax9.set_title('Token Filtering Comparison')\n        ax9.set_xticks(x)\n        ax9.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n        ax9.set_ylabel('Accuracy')\n        ax9.legend()\n    \n    plt.tight_layout()\n    plt.savefig('results/evaluation/plots/qwen3_position_count_dashboard.png', dpi=300)\n    \n    return fig\n\n# Analyze English token subset effectiveness for position/count tasks\ndef analyze_english_token_subset(english_results, all_results):\n    token_analysis = {}\n    \n    # Compare metrics between English-only and all tokens\n    metrics = ['letter_count_accuracy', 'letter_position_accuracy', 'character_level_accuracy', 'levenshtein_distance']\n    \n    for metric in metrics:\n        english_value = english_results.get(metric, 0)\n        all_value = all_results.get(metric, 0)\n        \n        # For Levenshtein, lower is better\n        if metric == 'levenshtein_distance':\n            improvement = all_value - english_value\n        else:\n            improvement = english_value - all_value\n            \n        token_analysis[metric] = {\n            'english_only': english_value,\n            'all_tokens': all_value,\n            'improvement': improvement,\n            'percent_change': (improvement / all_value * 100) if all_value != 0 else 0\n        }\n    \n    # Analyze which task types benefit most from English-only tokens\n    task_benefit = {\n        'letter_count': english_results.get('letter_count_accuracy', 0) - all_results.get('letter_count_accuracy', 0),\n        'letter_position': english_results.get('letter_position_accuracy', 0) - all_results.get('letter_position_accuracy', 0)\n    }\n    \n    # Determine which task benefits most\n    max_benefit_task = max(task_benefit.items(), key=lambda x: x[1])[0]\n    task_benefit['most_improved_task'] = max_benefit_task\n    \n    token_analysis['task_benefit'] = task_benefit\n    \n    return token_analysis\n\n# Main evaluation function\ndef evaluate_models(model_path, model_info=None):\n    # Initialize W&B\n    wandb.init(project=\"llm-spelling-finetuning\", name=\"qwen3_position_count_evaluation\")\n    \n    # Load test dataset\n    test_dataset = load_dataset(\"YOUR-USERNAME/llm-spelling-dataset\", split=\"test\")\n    # Alternative: load from local file\n    # with open('data/splits/test.json', 'r') as f:\n    #     test_dataset = json.load(f)\n    \n    # Filter test dataset to only include position and count tasks\n    test_dataset = [item for item in test_dataset if item[\"question_type\"] in [\"letter_position\", \"letter_count\"]]\n    \n    # Load model and tokenizer\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    # Evaluate with different token settings\n    results = {}\n    detailed_results = {}\n    \n    # Token settings to evaluate\n    configs = [\n        {\"english_only\": False},\n        {\"english_only\": True}\n    ]\n    \n    for config in configs:\n        english_only = config[\"english_only\"]\n        \n        token_key = \"english_only\" if english_only else \"all_tokens\"\n        \n        print(f\"\\nEvaluating with english_only={english_only}...\")\n        \n        # Run evaluations with current configuration\n        letter_count_acc, count_results = calc_letter_count_accuracy(model, tokenizer, test_dataset, english_only)\n        letter_position_acc, position_results = calc_letter_position_accuracy(model, tokenizer, test_dataset, english_only)\n        char_acc, char_results = calc_character_level_accuracy(model, tokenizer, test_dataset, english_only)\n        lev_dist, lev_results = calc_levenshtein_metrics(model, tokenizer, test_dataset, english_only)\n        \n        # Store results\n        results[token_key] = {\n            \"letter_count_accuracy\": letter_count_acc,\n            \"letter_position_accuracy\": letter_position_acc,\n            \"character_level_accuracy\": char_acc,\n            \"levenshtein_distance\": lev_dist\n        }\n        \n        detailed_results[token_key] = {\n            \"letter_count\": count_results,\n            \"letter_position\": position_results,\n            \"character_level\": char_results,\n            \"levenshtein\": lev_results\n        }\n    \n    # Use all_tokens results for main analysis\n    main_results = results[\"all_tokens\"]\n    main_detailed_results = detailed_results[\"all_tokens\"]\n    \n    # Perform error analysis\n    error_analysis = perform_error_analysis(main_detailed_results)\n    \n    # Analyze transfer learning effectiveness\n    transfer_result, transfer_data = analyze_transfer_learning(\n        model_info or {},\n        main_detailed_results[\"letter_count\"] + main_detailed_results[\"letter_position\"]\n    )\n    \n    # Analyze English token subset effectiveness\n    token_analysis = analyze_english_token_subset(results[\"english_only\"], results[\"all_tokens\"])\n    \n    # Create performance dashboard\n    dashboard = create_performance_dashboard(main_results, error_analysis, transfer_result)\n    \n    # Log results to W&B\n    wandb.log({\n        \"results\": results,\n        \"error_analysis\": error_analysis,\n        \"transfer_learning\": transfer_result,\n        \"token_analysis\": token_analysis,\n        \"evaluation_dashboard\": wandb.Image(dashboard)\n    })\n    \n    # Save results locally\n    evaluation_results = {\n        \"results\": results,\n        \"detailed_results\": detailed_results,\n        \"error_analysis\": error_analysis,\n        \"transfer_learning\": transfer_result,\n        \"token_analysis\": token_analysis\n    }\n    \n    # Ensure directories exist\n    import os\n    os.makedirs('results/evaluation/metrics', exist_ok=True)\n    os.makedirs('results/evaluation/error_analysis', exist_ok=True)\n    os.makedirs('results/evaluation/transfer_analysis', exist_ok=True)\n    os.makedirs('results/evaluation/token_analysis', exist_ok=True)\n    os.makedirs('results/evaluation/plots', exist_ok=True)\n    \n    # Save results to appropriate locations\n    with open(\"results/evaluation/metrics/qwen3_position_count_evaluation.json\", \"w\") as f:\n        json.dump(evaluation_results, f, indent=2)\n    \n    with open(\"results/evaluation/error_analysis/qwen3_position_count_error_patterns.json\", \"w\") as f:\n        json.dump(error_analysis, f, indent=2)\n        \n    with open(\"results/evaluation/transfer_analysis/qwen3_transfer_learning_analysis.json\", \"w\") as f:\n        json.dump(transfer_result, f, indent=2)\n        \n    with open(\"results/evaluation/token_analysis/qwen3_token_analysis.json\", \"w\") as f:\n        json.dump(token_analysis, f, indent=2)\n    \n    # Close W&B\n    wandb.finish()\n    \n    return evaluation_results\n```",
      "testStrategy": "1. Verify all evaluation metrics are calculated correctly for position/count tasks only\n2. Test Qwen3-4B in non-thinking mode only to ensure proper configuration\n3. Validate English-only token subset filtering works correctly for position/count tasks\n4. Test with Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n5. Confirm error analysis provides meaningful insights for position/count task types\n6. Validate transfer learning analysis metrics for how spelling training affects position/count performance\n7. Check that visualizations clearly show position/count performance metrics\n8. Verify results are properly logged to W&B with appropriate metrics\n9. Confirm performance comparison between different token filtering approaches for position/count tasks\n10. Test with different model checkpoints to ensure consistent evaluation\n11. Verify final evaluation results are saved locally in the correct directories:\n   - `results/evaluation/metrics/`\n   - `results/evaluation/error_analysis/`\n   - `results/evaluation/transfer_analysis/`\n   - `results/evaluation/token_analysis/`\n   - `results/evaluation/plots/`\n12. Test GPU switching functionality in Lightning.AI Studio (CPU → T4 → A100) for optimal resource usage\n13. Verify that all test datasets are properly filtered to include only position/count tasks\n14. Check that the evaluation notebooks can successfully load and analyze the position/count results\n15. Validate the pattern analysis to ensure it correctly identifies which spelling patterns in training lead to better position/count performance\n16. Test Lightning.AI Studio's job system for automated evaluation pipeline execution\n17. Verify proper environment isolation and dependency management in the Lightning.AI Studio\n18. Test the token analysis to ensure it correctly measures the effectiveness of the English token filtering approach for position/count tasks\n19. Verify that thinking mode is never enabled in any part of the evaluation process",
      "subtasks": [
        {
          "id": 1,
          "title": "Multi-metric evaluation framework implementation",
          "description": "Develop and implement a comprehensive evaluation framework with multiple metrics to assess model performance",
          "dependencies": [],
          "details": "Define evaluation goals and success metrics for the model assessment. Select appropriate metrics covering accuracy, precision, recall, F1-score, latency, and domain-specific measures. Create standardized test datasets with diverse examples. Implement automated evaluation pipelines that can process model outputs against ground truth. Establish baseline performance thresholds for each metric.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Base vs. fine-tuned model comparison",
          "description": "Conduct systematic comparison between base models and their fine-tuned versions across all defined metrics",
          "dependencies": [
            1
          ],
          "details": "Design controlled experiments to compare base and fine-tuned models. Ensure identical test conditions and datasets for fair comparison. Measure performance improvements across all metrics defined in subtask 1. Analyze trade-offs between different aspects of performance (e.g., accuracy vs. latency). Document specific improvements attributable to fine-tuning techniques. Identify areas where fine-tuning provided the most significant gains.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Detailed error analysis system",
          "description": "Create a system to categorize, analyze and track different types of model errors",
          "dependencies": [
            1,
            2
          ],
          "details": "Develop error taxonomy specific to the model's domain and tasks. Implement automated error classification system. Perform qualitative analysis of error patterns and edge cases. Create error frequency distribution reports. Identify correlations between specific input characteristics and error types. Develop recommendations for targeted model improvements based on error patterns.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Performance visualization dashboard",
          "description": "Design and implement an interactive dashboard to visualize model performance metrics and comparisons",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Select appropriate visualization types for different metrics and comparisons. Implement interactive features allowing drill-down into specific performance aspects. Create side-by-side visualizations of base vs. fine-tuned model performance. Design time-series views to track performance changes across model iterations. Ensure visualizations are accessible and interpretable for both technical and non-technical stakeholders.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Evaluation report generation",
          "description": "Create comprehensive evaluation reports documenting findings, methodologies, and recommendations",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Develop standardized report templates covering all evaluation aspects. Document evaluation methodology, metrics, and test datasets. Summarize key performance findings and improvements. Include detailed error analysis with examples. Provide actionable recommendations for further model improvements. Create executive summary for non-technical stakeholders. Ensure reports include all relevant visualizations from the dashboard.\n\nNOTE: If evaluating Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local evaluation is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Lightning.AI Studio setup for model evaluation",
          "description": "Configure and set up a dedicated Lightning.AI Studio for model evaluation with GPU switching capabilities",
          "dependencies": [
            1
          ],
          "details": "Create a dedicated evaluation Studio following the \"one Studio, one task\" principle. Configure the Studio with necessary dependencies for model evaluation. Set up GPU switching capabilities (CPU → T4 → A100) for cost-effective resource usage. Install and configure Lightning.AI plugins for experiment tracking and visualization. Set up shared filesystem access for models and datasets. Implement proper environment isolation and dependency management. Create documentation for the Lightning.AI Studio setup and usage. Test the setup with sample models to verify functionality.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "File structure implementation",
          "description": "Set up the file structure for evaluation components and results according to the project organization",
          "dependencies": [],
          "details": "Create the following directory structure:\n- `src/evaluation/` for evaluation code modules\n- `data/splits/` for test datasets and challenge sets\n- `results/evaluation/` for storing evaluation outputs\n- `notebooks/` for analysis notebooks\n- `docs/` for evaluation documentation\n\nEnsure all evaluation code properly uses these paths for loading data and saving results. Update existing code to use the standardized file paths. Create placeholder files and documentation templates as needed.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Analysis notebooks development",
          "description": "Create Jupyter notebooks for analyzing evaluation results and visualizing model performance",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Develop the following notebooks:\n- `notebooks/evaluation_analysis.ipynb`: General analysis of evaluation results\n- `notebooks/error_patterns.ipynb`: Detailed analysis of error patterns and categories\n- `notebooks/model_comparison.ipynb`: Comparative analysis between base and fine-tuned models\n\nEnsure notebooks can load results from the standardized file locations. Implement interactive visualizations and filtering capabilities. Add markdown documentation explaining the analysis methodology and interpretation of results.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Transfer learning analysis implementation",
          "description": "Develop and implement analysis tools to measure transfer learning effectiveness from spelling training to position/count tasks",
          "dependencies": [
            1,
            3
          ],
          "details": "Create a transfer learning analysis module in `src/evaluation/transfer_analysis.py`. Implement metrics to measure how spelling training affects position/count task performance. Develop methods to identify which spelling patterns in training lead to better position/count performance. Create visualizations showing the relationship between spelling training and position/count task success. Implement statistical tests to validate transfer learning effectiveness. Design analysis tools to identify successful and unsuccessful transfer cases.\n\nEnsure results are saved to `results/evaluation/transfer_analysis/` directory and properly visualized in the dashboard.",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "Position/count task evaluation pipeline",
          "description": "Implement dedicated evaluation pipeline for position/count tasks only",
          "dependencies": [
            1
          ],
          "details": "Refactor the evaluation framework to focus exclusively on position/count tasks. Ensure metrics are calculated appropriately for position and count tasks. Implement task-specific error analysis for position/count tasks. Create mechanisms to track performance on different position/count question types. Design the system to identify how spelling training transfers to position/count tasks. Ensure all results can be combined for comprehensive reporting and visualization.\n\nImplement the main evaluator in `src/evaluation/position_count_evaluator.py` to handle position/count evaluation.",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "Lightning.AI automated evaluation pipeline",
          "description": "Set up automated evaluation pipelines using Lightning.AI's job system",
          "dependencies": [
            6,
            10
          ],
          "details": "Configure Lightning.AI's job system for automated model evaluation. Create job templates for different evaluation scenarios. Set up job dependencies to ensure proper execution order. Implement resource optimization to use appropriate GPU tiers for different evaluation stages. Configure job notifications and alerts. Create a dashboard to monitor job status and results. Document the job system setup and usage. Test the automated pipeline with sample models.",
          "status": "pending"
        },
        {
          "id": 12,
          "title": "Cost optimization for Lightning.AI resources",
          "description": "Implement cost optimization strategies for efficient resource usage in Lightning.AI",
          "dependencies": [
            6
          ],
          "details": "Analyze resource requirements for different evaluation stages. Implement automatic GPU switching based on computational needs. Set up resource monitoring and usage alerts. Configure automatic shutdown of idle resources. Implement batch processing for efficient resource utilization. Create cost estimation tools for evaluation runs. Document cost optimization strategies and best practices. Test and validate cost savings through optimized resource usage.",
          "status": "pending"
        },
        {
          "id": 13,
          "title": "Lightning.AI evaluation documentation",
          "description": "Create comprehensive documentation for the Lightning.AI evaluation setup and usage",
          "dependencies": [
            6,
            11,
            12
          ],
          "details": "Document the Lightning.AI Studio setup and configuration. Create user guides for running evaluations in the Studio. Document GPU switching functionality and best practices. Create troubleshooting guides for common issues. Document cost optimization strategies and resource management. Create onboarding materials for new team members. Include examples and tutorials for different evaluation scenarios. Maintain documentation with updates for new Lightning.AI features.",
          "status": "pending"
        },
        {
          "id": 14,
          "title": "Qwen3 non-thinking mode evaluation",
          "description": "Implement evaluation pipeline for Qwen3-4B in non-thinking mode",
          "dependencies": [
            1,
            10
          ],
          "details": "Extend the evaluation framework to support Qwen3-4B in non-thinking mode for position/count tasks. Create evaluation configurations with enable_thinking=False. Implement proper configuration in the generation pipeline. Develop metrics to assess position/count performance. Create visualizations showing performance across different position/count tasks. Analyze transfer learning capabilities from spelling training to position/count tasks. Identify position/count tasks that benefit most from the model. Document findings and recommendations for optimal usage based on task type.",
          "status": "pending"
        },
        {
          "id": 15,
          "title": "English token subset analysis",
          "description": "Implement analysis tools to evaluate position/count performance with English-only token subset",
          "dependencies": [
            1,
            14
          ],
          "details": "Develop token filtering mechanism for Qwen3-4B to restrict generation to English-only tokens. Create analysis tools to compare position/count performance between full vocabulary and English-only subset. Implement metrics to measure the effectiveness of token filtering for position/count tasks. Analyze which position/count tasks benefit most from English token filtering. Create visualizations showing the impact of token filtering on different position/count metrics. Document findings and recommendations for token filtering usage. Ensure proper handling of Qwen3's tokenizer patterns in the filtering process.",
          "status": "pending"
        },
        {
          "id": 16,
          "title": "Qwen3 sampling parameters analysis",
          "description": "Analyze the impact of Qwen3's specific sampling parameters on position/count performance",
          "dependencies": [
            1,
            14
          ],
          "details": "Implement evaluation with Qwen3's recommended sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) for position/count tasks. Create comparison framework to test different parameter combinations for position/count tasks. Analyze how sampling parameters affect performance on position and count task types. Identify optimal parameter settings for position and count tasks. Create visualizations showing parameter sensitivity for position/count tasks. Document findings and recommendations for parameter tuning. Ensure proper implementation of MinP parameter if supported by the model.",
          "status": "pending"
        },
        {
          "id": 17,
          "title": "Performance comparison dashboard",
          "description": "Create interactive dashboard for comparing position/count performance across different configurations",
          "dependencies": [
            4,
            14
          ],
          "details": "Design and implement a dedicated dashboard for position/count performance comparison. Create visualizations of model performance across different configurations for position/count tasks. Implement filtering capabilities to focus on specific position/count metrics. Develop interactive features for exploring position/count error patterns. Create visualizations showing transfer learning characteristics from spelling training to position/count tasks. Ensure the dashboard is accessible and interpretable for both technical and non-technical stakeholders. Save performance comparison visualizations to `results/evaluation/plots/` directory.",
          "status": "pending"
        },
        {
          "id": 18,
          "title": "Qwen3 tokenizer pattern analysis",
          "description": "Analyze and handle Qwen3's specific tokenizer patterns in position/count evaluation",
          "dependencies": [
            1,
            14,
            15
          ],
          "details": "Investigate Qwen3's tokenizer patterns and their impact on position/count evaluation. Develop tools to analyze token distributions in model outputs for position/count tasks. Implement proper handling of Qwen3's tokenizer patterns in position/count evaluation metrics. Create visualizations showing token usage patterns for position/count tasks. Analyze how tokenizer patterns affect performance on position and count task types. Document findings and recommendations for handling tokenizer-specific issues in position/count evaluation. Ensure evaluation metrics properly account for tokenizer characteristics.",
          "status": "pending"
        },
        {
          "id": 19,
          "title": "Non-thinking mode configuration verification",
          "description": "Implement verification checks to ensure thinking mode is never enabled in the evaluation process",
          "dependencies": [
            14
          ],
          "details": "Create validation checks in the evaluation pipeline to verify thinking mode is never enabled. Implement configuration validation in the model loading and generation functions. Add assertions and error handling to prevent accidental use of thinking mode. Create documentation clearly stating the project policy on non-thinking mode usage. Update all code examples and documentation to explicitly show enable_thinking=False. Implement unit tests to verify thinking mode is never enabled in any part of the evaluation process. Create a pre-execution validation step in the Lightning.AI job system to verify configuration compliance.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 10,
      "title": "Model Publishing and Documentation",
      "description": "Prepare the final model, documentation, and publish to Hugging Face with comprehensive model card, focusing on spelling training but exclusively evaluating on position/count capabilities. Include Qwen3-4B specific documentation requirements.",
      "status": "pending",
      "dependencies": [
        9
      ],
      "priority": "medium",
      "details": "1. Prepare model card documentation highlighting transfer learning approach (spelling training to position/count evaluation) and Qwen3-4B specifics\n2. Create detailed README for the project clarifying that training is on spelling but evaluation is exclusively on position/count tasks, including Qwen3-4B features\n3. Upload the best model to Hugging Face\n4. Ensure dataset is properly published\n5. Create final report with results and findings, emphasizing transfer learning metrics (spelling training to position/count evaluation) and Qwen3-4B characteristics\n6. Organize documentation in the specified file structure\n7. Document training endpoints for spelling and evaluation endpoints exclusively for position/count tasks\n8. Document Lightning.AI Studios setup and usage for model publishing and evaluation\n9. Document Qwen3's non-thinking mode configuration (enable_thinking=False) as project policy\n10. Explain the English-only token subset approach and its benefits\n11. Detail Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n12. Document non-thinking mode performance characteristics\n13. Explain token filtering methodology and implementation\n14. Provide guidelines for task optimization with non-thinking mode\n15. Document transfer learning approach (spelling to position/count) in non-thinking mode\n16. Include visualizations of position/count performance results only\n17. Explain tokenizer pattern handling and considerations\n18. Create comprehensive guides for reproducing experiments with Qwen3-4B\n19. Document Lightning.AI Studio configuration for Qwen3 evaluation\n\nNOTE: If publishing Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local publishing is only for CPU-compatible models.\n\nNOTE: Project policy mandates using ONLY Qwen3 non-thinking mode (enable_thinking=False). All documentation and code must reflect this requirement.\n\nNOTE: Training is performed on spelling data, but evaluation is strictly limited to character position and character count tasks. No spelling performance metrics should be reported or documented.\n\nFile Structure:\n- API docs: `docs/api.md`\n- Deployment guide: `docs/deployment.md`\n- Monitoring guide: `docs/monitoring.md`\n- Lightning.AI Studios guide: `docs/lightning_studios.md`\n- Qwen3-4B guide: `docs/qwen3_4b_guide.md`\n- Experiment reproduction guide: `docs/experiment_reproduction.md`",
      "testStrategy": "1. Verify model card is comprehensive and follows Hugging Face guidelines, with clear transfer learning focus (spelling training to position/count evaluation) and Qwen3-4B specifics\n2. Confirm README provides clear instructions for training on spelling but evaluating exclusively on position/count tasks, including Qwen3-4B features\n3. Test model upload to Hugging Face\n4. Verify dataset is properly published and accessible\n5. Check that final report includes all required information, especially transfer learning metrics (spelling to position/count) and Qwen3-4B characteristics\n6. Test model loading from Hugging Face\n7. Verify all success criteria from the PRD are met and documented\n8. For Unsloth or GPU-fine-tuned models, verify the publishing process works in a cloud GPU environment\n9. Validate that all documentation files are created in the correct locations\n10. Test API documentation against actual API implementation, ensuring spelling training endpoints and position/count evaluation endpoints work as documented\n11. Verify deployment instructions work in both Docker and Kubernetes environments, with proper GPU support\n12. Test monitoring setup with Prometheus and Grafana, focusing on position/count evaluation metrics only\n13. Ensure all file paths in documentation match the actual project structure\n14. Test position/count endpoints for performance and accuracy\n15. Verify transfer learning metrics (spelling to position/count) are properly tracked and visualized\n16. Test Lightning.AI Studios setup and configuration according to documentation\n17. Verify GPU switching functionality in Lightning.AI Studios\n18. Test shared filesystem operations for data management\n19. Validate environment isolation and dependency management in Studios\n20. Test cost optimization strategies and automatic shutdown functionality\n21. Verify integration with the position/count evaluation framework in Lightning.AI Studios\n22. Test model publishing to Hugging Face from Lightning.AI Studios\n23. Validate troubleshooting procedures for common issues\n24. Test onboarding process for new team members using the documentation\n25. Verify documentation explicitly states Qwen3's non-thinking mode configuration (enable_thinking=False) as project policy\n26. Test English-only token subset approach and validate its benefits\n27. Verify Qwen3's sampling parameters are correctly documented and implemented\n28. Test non-thinking mode performance characteristics\n29. Validate token filtering methodology and implementation\n30. Test task optimization guidelines for non-thinking mode\n31. Verify documentation of transfer learning approach (spelling to position/count) in non-thinking mode\n32. Check visualizations of position/count performance results for clarity and accuracy\n33. Test tokenizer pattern handling and verify documentation accuracy\n34. Verify experiment reproduction guides can be followed successfully\n35. Test Lightning.AI Studio configuration specifically for Qwen3 evaluation\n36. Validate that all Qwen3-4B specific documentation is accurate and comprehensive\n37. Verify no references to thinking mode or <think> blocks exist in any documentation or code\n38. Test that enable_thinking=False is properly set in all model configurations\n39. Verify that no spelling performance metrics are reported or documented anywhere\n40. Confirm all evaluation scripts and metrics focus exclusively on position and count tasks",
      "subtasks": [
        {
          "id": 1,
          "title": "Model Card Creation",
          "description": "Create a comprehensive model card following Hugging Face guidelines with metadata and detailed sections",
          "dependencies": [],
          "details": "Develop a model card as a Markdown file with YAML metadata section. Include: model description, intended uses & limitations, training parameters, datasets used, evaluation results, biases and ethical considerations. Follow the structure from Mitchell, 2018 paper and use the Hugging Face template. Ensure all metadata supports discovery (license, datasets, language identifiers). Include Qwen3-4B specific information such as non-thinking mode configuration (enable_thinking=False), English-only token subset, and sampling parameters. Clearly state that the model is trained on spelling data but evaluated exclusively on position/count tasks, with no spelling performance metrics reported.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Project README and Documentation",
          "description": "Prepare comprehensive project documentation including installation, usage examples, and technical details",
          "dependencies": [],
          "details": "Create a detailed README.md for the project repository (separate from the model card). Include: project overview, installation instructions, dependency requirements, usage examples with code snippets, architecture diagrams, limitations, and acknowledgments. Document the preprocessing and postprocessing steps to ensure reproducibility. Add inline code comments and generate API documentation if applicable. Include Qwen3-4B specific sections on non-thinking mode configuration (enable_thinking=False), token filtering, and performance characteristics. Clearly distinguish between training (spelling) and evaluation (position/count only) throughout all documentation.\n\nOrganize documentation in the specified file structure:\n- API docs: `docs/api.md`\n- Deployment guide: `docs/deployment.md`\n- Monitoring guide: `docs/monitoring.md`\n- Lightning.AI Studios guide: `docs/lightning_studios.md`\n- Qwen3-4B guide: `docs/qwen3_4b_guide.md`\n- Experiment reproduction guide: `docs/experiment_reproduction.md`",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Hugging Face Model Publishing and Verification",
          "description": "Publish the model to Hugging Face Hub and verify its functionality",
          "dependencies": [
            1
          ],
          "details": "Use the huggingface_hub library to upload the model, tokenizer, and model card. Configure model tags, set appropriate visibility settings, and verify the model card renders correctly. Test the uploaded model with sample inference code to ensure it works as expected for position/count tasks. Validate that all metadata is correctly displayed on the model page and that links to datasets are functional. Include Qwen3-4B specific tags and metadata, ensuring non-thinking mode configuration (enable_thinking=False) is properly documented. Clearly indicate that the model is trained on spelling data but evaluated exclusively on position/count tasks.\n\nNOTE: If publishing Unsloth or GPU-fine-tuned models, use a cloud GPU environment (Google Colab or https://lightning.ai/lars/home). Local publishing is only for CPU-compatible models.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Final Report Generation",
          "description": "Create a comprehensive report summarizing the model development, performance, and publication process",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Generate a final report documenting the entire model development lifecycle. Include: executive summary, methodology, training process details (spelling), evaluation metrics with visualizations (position/count only), comparison to baseline models, limitations discovered during testing, deployment considerations, and future improvement recommendations. Format as a professional document with proper citations and appendices for detailed results. Include specific sections on Qwen3-4B features, non-thinking mode configuration (enable_thinking=False), and transfer learning approach (spelling to position/count) in non-thinking mode. Ensure no spelling performance metrics are reported or documented.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Cloud Environment Setup for Model Publishing",
          "description": "Configure cloud GPU environment for publishing Unsloth or GPU-fine-tuned models",
          "dependencies": [],
          "details": "Set up a cloud GPU environment (Google Colab or https://lightning.ai/lars/home) for publishing models that require GPU resources. Create a notebook or script that handles authentication with Hugging Face, loads the model from local storage or cloud storage, and publishes it to the Hugging Face Hub. Include clear instructions for users on how to use this environment for model publishing. Test the workflow to ensure it works seamlessly with Unsloth-optimized models. Ensure all configurations explicitly set enable_thinking=False for Qwen3-4B models. Include sample code for position/count evaluation (not spelling evaluation).",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "API Documentation Creation",
          "description": "Create detailed API documentation for model inference endpoints",
          "dependencies": [
            2
          ],
          "details": "Develop comprehensive API documentation in `docs/api.md` that includes:\n- Endpoint descriptions and usage examples\n- Request/response formats with JSON examples\n- Authentication requirements\n- Error handling and status codes\n- Rate limiting information\n- Performance considerations\n- Qwen3-4B specific endpoints and parameters, with explicit non-thinking mode configuration (enable_thinking=False)\n\nClearly distinguish between training endpoints (spelling) and evaluation endpoints (position/count only). Ensure documentation matches the actual implementation in `src/api/app.py` and `src/api/routes/`.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Deployment Documentation",
          "description": "Create deployment guide for Docker and Kubernetes environments",
          "dependencies": [
            2
          ],
          "details": "Develop a detailed deployment guide in `docs/deployment.md` covering:\n- Docker deployment instructions\n- Kubernetes deployment configuration\n- Environment variable configuration\n- Resource requirements and scaling recommendations\n- Security considerations\n- Qwen3-4B specific deployment considerations, including non-thinking mode configuration (enable_thinking=False)\n\nReference the actual configuration files in `deployment/docker-compose.yml` and `deployment/k8s/`. Clearly indicate that deployed models should be evaluated only on position/count tasks, not spelling.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Monitoring Documentation",
          "description": "Create monitoring guide for Prometheus and Grafana setup",
          "dependencies": [
            2
          ],
          "details": "Develop a monitoring guide in `docs/monitoring.md` that includes:\n- Prometheus configuration for metrics collection\n- Grafana dashboard setup and import instructions\n- Alert configuration with Alertmanager\n- Log collection and analysis recommendations\n- Performance monitoring best practices\n- Qwen3-4B specific metrics and monitoring considerations for non-thinking mode\n\nReference the actual configuration files in `deployment/monitoring/`. Ensure all monitoring metrics focus exclusively on position/count performance, with no spelling metrics included.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Transfer Learning Documentation and Metrics",
          "description": "Document the transfer learning approach and implement metrics tracking",
          "dependencies": [
            1,
            2,
            6,
            8
          ],
          "details": "Create comprehensive documentation on the transfer learning approach used in the project:\n- Update model card to highlight transfer learning aspects (spelling training to position/count evaluation)\n- Document how training on spelling tasks transfers to position/count performance\n- Create visualization tools for transfer learning metrics (position/count evaluation only)\n- Implement monitoring for transfer learning effectiveness in production\n- Add transfer learning analysis to the final report\n- Document transfer learning approach in Qwen3-4B non-thinking mode\n\nEnsure all documentation clearly explains how training on spelling tasks transfers to position/count tasks, with specific focus on non-thinking mode performance. Do not include any spelling performance metrics in the documentation or visualizations.",
          "status": "pending"
        },
        {
          "id": 10,
          "title": "Dual-Task API Implementation Documentation",
          "description": "Document the implementation of separate endpoints for spelling training and position/count evaluation",
          "dependencies": [
            6
          ],
          "details": "Create detailed documentation for the dual-task API implementation:\n- Document the separate endpoints for spelling training and position/count evaluation\n- Provide examples for both task types\n- Explain how to configure the API for different task types\n- Document error handling specific to each task type\n- Include performance considerations for both tasks\n- Document Qwen3-4B non-thinking mode configuration (enable_thinking=False) for each task type\n\nEnsure the documentation clearly distinguishes between training (spelling) and evaluation (position/count only) endpoints, with specific focus on non-thinking mode optimization. Make it explicit that spelling performance should never be measured or reported.",
          "status": "pending"
        },
        {
          "id": 11,
          "title": "Lightning.AI Studios Documentation",
          "description": "Create comprehensive documentation for Lightning.AI Studios setup and usage",
          "dependencies": [
            2,
            5
          ],
          "details": "Develop detailed documentation in `docs/lightning_studios.md` covering:\n- Account setup and authentication\n- Studio creation and management\n- \"One Studio, one task\" principle implementation\n- GPU configuration and switching options\n- Shared filesystem usage and data management\n- Environment isolation and dependency management\n- Cost optimization guidelines\n- Integration with the position/count evaluation framework\n- Model publishing workflow\n- Troubleshooting and best practices\n- Onboarding process for new team members\n- Qwen3-4B specific considerations for Lightning.AI Studios, including non-thinking mode configuration (enable_thinking=False)\n- Specific configuration requirements for Qwen3 evaluation\n\nInclude practical examples and commands for each section, with clear explanations of the benefits and trade-offs of different approaches. Ensure all evaluation examples focus exclusively on position/count tasks, not spelling.",
          "status": "pending"
        },
        {
          "id": 12,
          "title": "Qwen3-4B Specific Documentation",
          "description": "Create comprehensive documentation for Qwen3-4B specific features and considerations",
          "dependencies": [
            1,
            2,
            6,
            9,
            10
          ],
          "details": "Develop detailed documentation in `docs/qwen3_4b_guide.md` covering:\n- Non-thinking mode configuration (enable_thinking=False) as project policy\n- English-only token subset approach and its benefits\n- Qwen3's specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)\n- Non-thinking mode performance characteristics\n- Token filtering methodology and implementation\n- Guidelines for task optimization with non-thinking mode\n- Transfer learning approach (spelling training to position/count evaluation) in non-thinking mode\n- Visualizations of position/count performance results only\n- Tokenizer pattern handling and considerations\n\nEnsure the documentation provides clear guidance on how to use Qwen3-4B features for optimal performance in various scenarios, with explicit instructions to always use non-thinking mode. Make it clear that while training is on spelling data, evaluation is exclusively on position/count tasks.",
          "status": "pending"
        },
        {
          "id": 13,
          "title": "Experiment Reproduction Guide",
          "description": "Create comprehensive guides for reproducing experiments with Qwen3-4B",
          "dependencies": [
            1,
            2,
            11,
            12
          ],
          "details": "Develop detailed documentation in `docs/experiment_reproduction.md` covering:\n- Step-by-step instructions for reproducing all experiments\n- Environment setup requirements specific to Qwen3-4B\n- Data preparation and preprocessing steps\n- Training command examples with all parameters, including enable_thinking=False\n- Evaluation procedures and metrics calculation for position/count tasks only\n- Expected results and performance benchmarks for position/count tasks\n- Troubleshooting common issues\n- Hardware requirements and optimization tips\n- Time and resource estimates for each experiment\n\nEnsure the guide is comprehensive enough that a new researcher could reproduce all experiments exactly as performed in the original work, with explicit instructions to use non-thinking mode only. Clearly state that while training is on spelling data, evaluation is exclusively on position/count tasks, with no spelling performance metrics to be reported.",
          "status": "pending"
        },
        {
          "id": 14,
          "title": "Non-Thinking Mode Configuration Verification",
          "description": "Verify all code and configurations use Qwen3-4B non-thinking mode only",
          "dependencies": [
            1,
            2,
            3,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13
          ],
          "details": "Perform a comprehensive review of all code, configuration files, and documentation to ensure:\n- All Qwen3-4B model instantiations explicitly set enable_thinking=False\n- No references to thinking mode or <think> blocks exist in any code or documentation\n- All examples, tutorials, and guides consistently use non-thinking mode\n- All API endpoints and inference code enforce non-thinking mode\n- Deployment configurations properly set enable_thinking=False\n- Test scripts verify non-thinking mode is properly configured\n\nCreate a verification checklist and document the results to confirm project policy compliance.",
          "status": "pending"
        },
        {
          "id": 15,
          "title": "Evaluation Metrics Verification",
          "description": "Verify all evaluation metrics focus exclusively on position/count tasks",
          "dependencies": [
            1,
            2,
            3,
            4,
            6,
            8,
            9,
            10,
            11,
            12,
            13
          ],
          "details": "Perform a comprehensive review of all evaluation code, metrics, documentation, and visualizations to ensure:\n- All evaluation metrics focus exclusively on position and count tasks\n- No spelling performance metrics are reported or documented anywhere\n- All evaluation scripts properly implement position/count metrics only\n- Visualizations only show position/count performance\n- Documentation clearly states that evaluation is limited to position/count tasks\n- Monitoring dashboards only track position/count metrics\n- API documentation for evaluation endpoints only mentions position/count capabilities\n\nCreate a verification checklist and document the results to confirm compliance with the evaluation requirements.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 11,
      "title": "Lightning.AI Studio Migration Planning and Setup",
      "description": "Plan and implement the migration of the evaluation framework to Lightning.AI Studios, leveraging features like isolated environments, GPU switching, and the plugin system. This includes initial Studio setup, environment configuration, and structuring components for model training and evaluation, while maintaining data preparation locally on Mac (Apple Silicon).",
      "status": "pending",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "Begin by reviewing Lightning.AI Studio documentation to understand its persistent cloud environment, isolated workspace capabilities, GPU management, and plugin architecture. Design a migration plan that maps components of the current evaluation framework (model training, evaluation, etc.) to a corresponding Studio workspace or plugin, while keeping data preparation local on Mac (Apple Silicon). Set up a new Studio instance, configure the environment with all necessary dependencies (ensuring compatibility with the existing Python setup from Task 1), and enable GPU switching as required. Establish isolated environments for each cloud component to ensure modularity and reproducibility. Integrate Lightning.AI plugins where beneficial, and document the Studio structure, environment variables, and resource allocation strategies. For local data preparation, ensure the setup includes the Qwen3-4B tokenizer with English-only token subset and adheres to the non-thinking mode policy. Document the interface between local data preparation and cloud-based components. Ensure the overall setup supports iterative development and easy scaling for future needs.",
      "testStrategy": "Verify that the Studio instance is accessible and persistent, with all required dependencies installed and functioning. Confirm that cloud components (model training, evaluation) run in isolated environments and can access GPU resources as configured. Test the plugin system by integrating at least one relevant plugin and ensuring it operates as expected. For local data preparation, verify that the Qwen3-4B tokenizer with English-only token subset works correctly on Mac (Apple Silicon) and adheres to the non-thinking mode policy. Test the interface between local data preparation and cloud components to ensure seamless data transfer. Validate that the overall structure supports modular workflows and that documentation is clear and complete. Run end-to-end tests for each workflow to ensure successful execution and reproducibility.",
      "subtasks": [
        {
          "id": 1,
          "title": "Lightning.AI Studio Documentation Review and Migration Planning",
          "description": "Thoroughly review Lightning.AI documentation to understand Studio capabilities and create a comprehensive migration plan for the evaluation framework.",
          "dependencies": [],
          "details": "Review Lightning.AI documentation focusing on persistent cloud environments, isolated workspaces, GPU management, and plugin architecture. Create a detailed migration plan mapping each component of the current evaluation framework to appropriate Studio workspaces following the 'one Studio, one task' principle. Document how data preparation, model training, evaluation, and deployment components will be structured across separate Studios. Include considerations for resource allocation, environment variables, and secrets management.",
          "status": "pending",
          "testStrategy": "Create a comprehensive document outlining the migration plan with clear mappings between current framework components and planned Lightning.AI Studio structure."
        },
        {
          "id": 2,
          "title": "Local Data Preparation Setup on Mac (Apple Silicon)",
          "description": "Set up and configure a local environment on Mac (Apple Silicon) for data preparation tasks with appropriate dependencies and Qwen3-4B tokenizer.",
          "dependencies": [
            1
          ],
          "details": "Configure a local Python environment on Mac (Apple Silicon) dedicated to data preparation tasks. Install all necessary dependencies ensuring compatibility with Apple Silicon architecture. Set up the Qwen3-4B tokenizer with English-only token subset following the project's non-thinking mode policy. Configure appropriate local storage for datasets and processed data. Implement data processing workflows that can be executed locally. Document the local environment configuration, data processing steps, and interface specifications for transferring processed data to Lightning.AI Studios for training and evaluation.",
          "status": "pending",
          "testStrategy": "Verify successful local environment setup with all dependencies installed. Test Qwen3-4B tokenizer functionality with English-only token subset. Validate data processing workflows with sample datasets. Test data export/transfer mechanisms to ensure compatibility with Lightning.AI Studios."
        },
        {
          "id": 3,
          "title": "Model Training Studio Setup with GPU Configuration",
          "description": "Establish a dedicated Lightning.AI Studio for model training with GPU support and appropriate resource allocation.",
          "dependencies": [
            1
          ],
          "details": "Create a separate Lightning.AI Studio specifically for model training tasks. Configure GPU access and switching capabilities based on training requirements. Install all necessary training dependencies and frameworks. Set up environment variables for training parameters. Configure appropriate storage access for training data and model artifacts. Document GPU usage patterns, sleep mode settings, and cost optimization strategies.",
          "status": "pending",
          "testStrategy": "Verify GPU accessibility and performance with a small training job. Confirm proper environment configuration by running existing training scripts."
        },
        {
          "id": 4,
          "title": "Evaluation Framework Studio Implementation",
          "description": "Develop a dedicated Lightning.AI Studio for model evaluation with appropriate metrics tracking and visualization capabilities.",
          "dependencies": [
            1,
            3
          ],
          "details": "Set up a Lightning.AI Studio dedicated to model evaluation tasks. Configure the environment with necessary evaluation dependencies. Implement access to trained models from the model training Studio. Set up metrics tracking and visualization tools. Configure appropriate CPU/GPU resources based on evaluation needs. Document the evaluation workflow, environment configuration, and integration points with other Studios.",
          "status": "pending",
          "testStrategy": "Test the evaluation Studio with sample models to verify metrics calculation and reporting functionality."
        },
        {
          "id": 5,
          "title": "Lightning.AI Plugin Integration and Deployment Studio Setup",
          "description": "Integrate relevant Lightning.AI plugins and establish a deployment Studio for productionizing models with specific focus on Qwen3-4B requirements.",
          "dependencies": [
            1,
            3,
            4
          ],
          "details": "Research and integrate beneficial Lightning.AI plugins across all Studios. Set up a dedicated deployment Studio for serving trained models, with specific configurations for Qwen3-4B. Configure deployment environment for Qwen3-4B with non-thinking mode only (ensure enable_thinking=False in all configurations). Implement proper sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) for Qwen3-4B. Set up English-only token subset filtering in production environments. Optimize deployment for Qwen3's specific tokenizer patterns. Establish endpoints and configurations that strictly enforce non-thinking mode operation. Implement robust error handling for token filtering operations. Configure monitoring systems to track relevant metrics. Implement proper resource allocation strategies based on Qwen3-4B's requirements. Set up efficient model loading, initialization, and caching strategies. Configure scaling mechanisms based on load patterns. Document the plugin architecture, deployment workflow, and maintenance procedures with clear instructions that non-thinking mode is the only supported configuration. Ensure the setup supports iterative development and easy scaling for future needs.",
          "status": "pending",
          "testStrategy": "Verify plugin functionality with test cases. Deploy Qwen3-4B in non-thinking mode only and validate proper configuration of sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0). Verify that enable_thinking=False is set in all configurations. Test English-only token filtering functionality and error handling. Validate endpoints and configurations to ensure they strictly enforce non-thinking mode. Perform load testing to verify proper resource allocation and scaling. Test caching strategies. Verify monitoring systems correctly track relevant metrics. Run comprehensive validation tests to ensure proper functionality and performance in non-thinking mode only."
        },
        {
          "id": 6,
          "title": "Non-Thinking Mode Documentation and Compliance Verification",
          "description": "Ensure all documentation, code, and configurations strictly adhere to the project policy of using only non-thinking mode for Qwen3.",
          "dependencies": [
            1,
            5
          ],
          "details": "Review all documentation, code, and configurations created during the Lightning.AI Studio migration to ensure strict compliance with the project policy of using only non-thinking mode (enable_thinking=False) for Qwen3. Remove or refactor any references to thinking mode or <think> blocks in all materials. Create clear documentation that explicitly states non-thinking mode is the only supported configuration. Implement verification checks in deployment scripts to prevent accidental enabling of thinking mode. Add configuration validation steps to CI/CD pipelines to enforce this policy. Create a compliance checklist for team members to verify adherence to this policy during development and deployment.",
          "status": "pending",
          "testStrategy": "Perform a comprehensive audit of all documentation, code, and configurations to verify no references to thinking mode exist. Test deployment scripts with intentionally incorrect configurations (enable_thinking=True) to ensure they fail appropriately. Validate that CI/CD pipelines correctly identify and prevent configurations that would enable thinking mode. Have team members use the compliance checklist during a test deployment to verify its effectiveness."
        },
        {
          "id": 7,
          "title": "Data Transfer Interface Between Local Environment and Lightning.AI Studios",
          "description": "Develop and document a robust interface for transferring processed data from the local Mac environment to Lightning.AI Studios.",
          "dependencies": [
            2,
            3
          ],
          "details": "Design and implement a reliable mechanism for transferring processed data from the local Mac (Apple Silicon) environment to Lightning.AI Studios for training and evaluation. Consider options such as cloud storage integration (S3, GCS), direct API uploads, or other appropriate transfer methods. Ensure the interface handles large datasets efficiently and maintains data integrity. Implement appropriate authentication and security measures. Create automation scripts to streamline the transfer process. Document the complete data flow from local processing to cloud usage, including file formats, directory structures, and versioning strategies.",
          "status": "pending",
          "testStrategy": "Test data transfer with various dataset sizes to verify reliability and performance. Validate that transferred data maintains integrity and can be correctly loaded in Lightning.AI Studios. Test automation scripts with different scenarios. Verify security measures by attempting unauthorized access. Document transfer times and resource usage for different dataset sizes to establish performance benchmarks."
        },
        {
          "id": 8,
          "title": "Optional: Data Preparation Studio Setup for Future Migration",
          "description": "Create a plan for future migration of data preparation tasks from local environment to Lightning.AI Studio when needed.",
          "dependencies": [
            1,
            2
          ],
          "details": "Develop a detailed plan for future migration of data preparation tasks from the local Mac environment to Lightning.AI Studio when needed. Document the current local data preparation workflow in detail to facilitate future migration. Identify potential challenges in migrating from Apple Silicon to cloud environment, particularly regarding the Qwen3-4B tokenizer and English-only token subset. Create a checklist of requirements and configurations needed for a successful migration. Outline a testing strategy to ensure equivalent results between local and cloud-based data preparation. Document this as an optional future task that can be implemented when scaling requirements exceed local capabilities.",
          "status": "pending",
          "testStrategy": "Review the migration plan with team members to identify potential gaps or challenges. Create a small proof-of-concept test that demonstrates equivalent results between local and cloud-based data preparation using a sample dataset."
        }
      ]
    },
    {
      "id": 12,
      "title": "Migrate Evaluation Framework Components to Lightning.AI Studios",
      "description": "Migrate all core evaluation framework components—including data preparation scripts, training workflows, and evaluation systems—into their respective Lightning.AI Studios, ensuring full functionality and leveraging Lightning.AI features.",
      "status": "pending",
      "dependencies": [
        11
      ],
      "priority": "high",
      "details": "Begin by auditing existing data preparation, training, and evaluation components to identify dependencies and required resources. For each component, refactor code and workflows as needed to fit Lightning.AI Studio paradigms, such as modular Studio apps. Migrate data preparation scripts into a dedicated Studio, ensuring compatibility with Lightning.AI's persistent storage for datasets and intermediate results. Move training workflows into Studios that utilize GPU switching, configuring resource allocation and environment variables for optimal performance. Transition evaluation systems into Studios, integrating with the plugin system for extensibility and automation. Throughout migration, maintain existing functionality and adapt interfaces to Lightning.AI's UI/UX standards. Document all changes, update configuration files, and ensure seamless integration between Studios. Address any deprecated or incompatible features by leveraging Lightning.AI alternatives or plugins. Ensure all implementations strictly adhere to the project policy of using only Qwen3 non-thinking mode (enable_thinking=False), with no references to thinking mode in any code, configuration, or documentation.",
      "testStrategy": "Verify that each migrated component (data preparation, training, evaluation) runs successfully within its respective Lightning.AI Studio, producing expected outputs. Test GPU switching by running training workflows on different GPU types and confirming correct resource usage. Validate persistent storage by saving and retrieving datasets and intermediate results across Studio sessions. Assess plugin integration by installing and running at least one relevant plugin in the evaluation Studio. Perform end-to-end tests to ensure the entire workflow—from data preparation through evaluation—operates as intended. Compare results with pre-migration outputs to confirm parity. Review documentation for completeness and accuracy. Verify that all Qwen3 implementations have enable_thinking=False and that no thinking mode functionality is present in any code or configuration.",
      "subtasks": [
        {
          "id": 1,
          "title": "Audit and Analyze Existing Evaluation Framework Components",
          "description": "Conduct a comprehensive audit of all current data preparation scripts, training workflows, and evaluation systems to identify dependencies, required resources, and potential compatibility issues with Lightning.AI Studios.",
          "dependencies": [],
          "details": "Review codebases, document external dependencies, assess data storage needs, and map out current workflows to inform migration planning. Identify and document any existing code that uses Qwen3 thinking mode for removal or refactoring.",
          "status": "pending",
          "testStrategy": "Validate audit completeness by cross-referencing with existing documentation and stakeholder interviews. Confirm all instances of thinking mode usage have been identified."
        },
        {
          "id": 2,
          "title": "Migrate Data Preparation Scripts to Lightning.AI Studio",
          "description": "Refactor and migrate all data preparation scripts into a dedicated Lightning.AI Studio, ensuring compatibility with Lightning.AI's persistent storage and modular app architecture.",
          "dependencies": [
            1
          ],
          "details": "Adapt scripts to utilize Lightning.AI's storage APIs, modularize code as Studio apps, and ensure intermediate results are correctly handled. Remove any data preparation logic related to Qwen3 thinking mode or <think> blocks.",
          "status": "pending",
          "testStrategy": "Run end-to-end data preparation workflows in the Studio and verify output consistency with pre-migration results. Confirm no thinking mode-related data processing remains."
        },
        {
          "id": 3,
          "title": "Migrate Training Workflows with GPU Switching and Resource Optimization",
          "description": "Transition training workflows into Lightning.AI Studios, implementing GPU switching, configuring resource allocation, and setting environment variables for optimal performance.",
          "dependencies": [
            2
          ],
          "details": "Refactor training code to leverage Lightning.AI's resource management features, ensure compatibility with Studio paradigms, and document environment configurations. Ensure all Qwen3 implementations explicitly set enable_thinking=False and remove any thinking mode-specific code paths.",
          "status": "pending",
          "testStrategy": "Execute training runs in the Studio, monitor resource utilization, and compare training metrics to baseline results. Verify that all Qwen3 instances are configured with enable_thinking=False."
        },
        {
          "id": 4,
          "title": "Migrate and Integrate Evaluation Systems with Plugin Support",
          "description": "Move evaluation systems into Lightning.AI Studios, integrating with the plugin system for extensibility and automation, and adapting interfaces to Lightning.AI's UI/UX standards.",
          "dependencies": [
            3
          ],
          "details": "Refactor evaluation logic as modular Studio apps, implement plugin hooks, and update UI components for seamless user experience. Ensure evaluation metrics and systems are optimized for non-thinking mode only, removing any thinking mode-specific evaluation criteria.",
          "status": "pending",
          "testStrategy": "Perform evaluation runs using migrated systems, test plugin integrations, and validate UI/UX compliance. Confirm all evaluation processes work correctly with non-thinking mode only."
        },
        {
          "id": 5,
          "title": "System Integration, Testing, and Documentation",
          "description": "Ensure seamless integration between all migrated Studios, address deprecated or incompatible features, and document all changes and updated configurations.",
          "dependencies": [
            4
          ],
          "details": "Conduct system-wide integration tests, resolve any migration issues, and update technical documentation and configuration files. Ensure all documentation clearly states that only Qwen3 non-thinking mode is supported and remove any references to thinking mode functionality.",
          "status": "pending",
          "testStrategy": "Run comprehensive integration and regression tests across Studios, verify documentation accuracy, and confirm end-to-end functionality. Validate that all documentation and configuration files only reference non-thinking mode."
        },
        {
          "id": 6,
          "title": "Implement Qwen3-4B Non-Thinking Mode Optimization",
          "description": "Optimize Qwen3-4B non-thinking mode within the Lightning.AI Studios framework.",
          "dependencies": [
            3
          ],
          "details": "Develop and implement optimizations specifically for Qwen3-4B non-thinking mode (enable_thinking=False). Configure resource allocation optimization for non-thinking mode. Implement performance monitoring systems to track efficiency metrics for non-thinking mode operation.",
          "status": "pending",
          "testStrategy": "Benchmark performance of non-thinking mode, verify resource utilization patterns match expectations, and confirm enable_thinking=False is consistently applied."
        },
        {
          "id": 7,
          "title": "Implement English-Only Token Filtering for Qwen3-4B",
          "description": "Develop and integrate efficient English-only token filtering for Qwen3-4B within the Lightning.AI Studios environment.",
          "dependencies": [
            2
          ],
          "details": "Design and implement memory-optimized token filtering algorithms for English-only subset. Develop efficient error handling mechanisms for token filtering edge cases. Optimize tokenizer performance specifically for the English-only subset.",
          "status": "pending",
          "testStrategy": "Test token filtering accuracy on diverse English text samples, measure memory usage during filtering operations, and validate error handling with malformed inputs."
        },
        {
          "id": 8,
          "title": "Optimize Sampling Parameters for Qwen3-4B",
          "description": "Configure and optimize sampling parameter settings (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) for Qwen3-4B in Lightning.AI Studios.",
          "dependencies": [
            3
          ],
          "details": "Implement the specified sampling parameter configurations. Develop efficient batch processing mechanisms that maintain optimal sampling parameters for non-thinking mode operation. Ensure all configurations explicitly set enable_thinking=False.",
          "status": "pending",
          "testStrategy": "Validate output quality with specified sampling parameters, measure inference performance with optimized settings, and compare results against baseline configurations. Verify enable_thinking=False is consistently applied."
        },
        {
          "id": 9,
          "title": "Implement Caching Strategies for Qwen3-4B",
          "description": "Design and implement efficient caching strategies for Qwen3-4B non-thinking mode.",
          "dependencies": [
            6
          ],
          "details": "Develop caching mechanisms optimized for non-thinking mode usage patterns. Optimize model loading and initialization processes to leverage caching for faster startup times. Ensure all cached configurations maintain enable_thinking=False.",
          "status": "pending",
          "testStrategy": "Measure cache hit rates, benchmark startup times with caching enabled, and validate that all cached configurations maintain enable_thinking=False."
        },
        {
          "id": 10,
          "title": "Remove Thinking Mode References from Codebase",
          "description": "Systematically identify and remove all references to Qwen3 thinking mode throughout the codebase and documentation.",
          "dependencies": [
            1
          ],
          "details": "Conduct a comprehensive search for any references to thinking mode, <think> blocks, or enable_thinking parameter. Remove or refactor all identified code, ensuring non-thinking mode is the only supported configuration. Update all documentation to reflect the project policy of using only non-thinking mode.",
          "status": "pending",
          "testStrategy": "Perform static code analysis to verify no thinking mode references remain. Review documentation to confirm all references have been removed or updated. Test system functionality to ensure removal of thinking mode code did not impact performance."
        }
      ]
    },
    {
      "id": 13,
      "title": "Optimize and Fine-Tune Lightning.AI Studios for Performance, Cost, and Scalability",
      "description": "Optimize the Lightning.AI Studios setup for maximum performance, efficient GPU usage, cost management, and maintainability, while implementing best practices for organization, monitoring, and future scalability, with specific focus on Qwen3-4B model requirements in non-thinking mode only.",
      "status": "pending",
      "dependencies": [
        12
      ],
      "priority": "medium",
      "details": "Review the current Lightning.AI Studios configuration post-migration (from Task 12) and identify bottlenecks in data loading, model training, and evaluation workflows. Implement dataset optimizations to minimize GPU idle time and reduce cloud costs by ensuring fast data transfer and leveraging Lightning's optimize operator where appropriate[1][2]. Adjust GPU allocation and sleep settings to balance performance with cost efficiency, using Lightning's built-in controls and best practices. Organize Studios and components following Lightning.AI's recommended structure for clarity and maintainability[3]. Set up comprehensive monitoring tools tailored for Qwen3-4B's specific requirements in non-thinking mode only (enable_thinking=False), including monitoring for English-only token filtering performance, and sampling parameter effectiveness (Temperature=0.6, TopP=0.95, TopK=20, MinP=0). Establish automated alerts for resource usage anomalies. Document and automate regular maintenance procedures, including dependency updates and Studio health checks. Plan for future scaling by enabling multi-GPU support, low-precision training, and efficient configuration management using YAML recipes and parameter-efficient finetuning methods[5].",
      "testStrategy": "Verify that all Studios demonstrate improved performance by benchmarking training and evaluation times before and after optimization. Confirm reduced GPU idle time and lower cloud costs through resource usage reports. Ensure GPU sleep settings are correctly applied and tested for both cost savings and rapid wake-up. Review Studio organization for adherence to best practices and clarity. Validate that monitoring dashboards are operational for non-thinking mode only, with specific metrics for token filtering performance, sampling parameter effectiveness, and performance. Test that alerts trigger appropriately for resource usage issues. Verify comprehensive logging is implemented for non-thinking mode. Validate maintenance scripts and procedures for reliability. Simulate scaling scenarios (e.g., multi-GPU, larger datasets) to confirm readiness for future growth.",
      "subtasks": [
        {
          "id": 1,
          "title": "Optimize Dataset for Performance",
          "description": "Review and optimize datasets to minimize GPU idle time and reduce cloud costs by ensuring fast data transfer and leveraging Lightning's optimize operator.",
          "dependencies": [],
          "details": "Use Lightning's optimize operator to convert datasets into compressed binary files for faster loading and reduced GPU idle time.",
          "status": "pending",
          "testStrategy": "Monitor GPU utilization and data loading speed"
        },
        {
          "id": 2,
          "title": "Tune GPU Allocation and Cost Efficiency",
          "description": "Adjust GPU allocation and sleep settings to balance performance with cost efficiency using Lightning's built-in controls and best practices.",
          "dependencies": [
            1
          ],
          "details": "Configure GPU settings to optimize performance while minimizing unnecessary costs.",
          "status": "pending",
          "testStrategy": "Evaluate cost savings and performance impact"
        },
        {
          "id": 3,
          "title": "Organize Studios for Maintainability",
          "description": "Organize Lightning.AI Studios and components following the recommended structure for clarity and maintainability.",
          "dependencies": [
            2
          ],
          "details": "Implement a structured organization of Studios to enhance clarity and facilitate future updates.",
          "status": "pending",
          "testStrategy": "Assess organizational clarity and ease of maintenance"
        },
        {
          "id": 4,
          "title": "Setup Qwen3-4B Specific Monitoring and Alerts",
          "description": "Set up comprehensive monitoring tools tailored for Qwen3-4B's specific requirements in non-thinking mode only.",
          "dependencies": [
            3
          ],
          "details": "Configure monitoring tools to track performance metrics specific to Qwen3-4B in non-thinking mode only (enable_thinking=False), including English-only token filtering performance, sampling parameter effectiveness (Temperature=0.6, TopP=0.95, TopK=20, MinP=0), performance metrics, token filtering accuracy, and cache effectiveness. Set up alerts for resource usage anomalies.",
          "status": "pending",
          "testStrategy": "Verify alert functionality for resource usage issues and validate monitoring accuracy for non-thinking mode"
        },
        {
          "id": 5,
          "title": "Implement Comprehensive Logging for Qwen3-4B",
          "description": "Implement comprehensive logging system for Qwen3-4B in non-thinking mode only.",
          "dependencies": [
            4
          ],
          "details": "Set up detailed logging for non-thinking mode only, capturing token filtering operations, sampling parameter applications, and error events.",
          "status": "pending",
          "testStrategy": "Verify log completeness, accuracy, and usefulness for non-thinking mode"
        },
        {
          "id": 6,
          "title": "Create Performance Dashboards for Qwen3-4B",
          "description": "Develop dedicated performance dashboards for monitoring Qwen3-4B in non-thinking mode only.",
          "dependencies": [
            4,
            5
          ],
          "details": "Create visual dashboards that display real-time metrics for token filtering performance, sampling parameter effectiveness, performance, resource usage, cache effectiveness, and error rates for non-thinking mode only.",
          "status": "pending",
          "testStrategy": "Validate dashboard functionality, data accuracy, and usefulness for performance optimization"
        },
        {
          "id": 7,
          "title": "Document and Automate Maintenance",
          "description": "Document and automate regular maintenance procedures, including dependency updates and Studio health checks.",
          "dependencies": [
            4,
            5,
            6
          ],
          "details": "Create documentation and automate scripts for regular maintenance tasks to ensure Studio health and efficiency, with specific procedures for Qwen3-4B model maintenance in non-thinking mode only.",
          "status": "pending",
          "testStrategy": "Test automation scripts and review documentation completeness"
        },
        {
          "id": 8,
          "title": "Monitor Token Filtering Accuracy and Performance",
          "description": "Implement specific monitoring for English-only token filtering performance and accuracy.",
          "dependencies": [
            4
          ],
          "details": "Set up metrics to track token filtering accuracy, performance impact, and error rates specifically for English-only filtering in Qwen3-4B.",
          "status": "pending",
          "testStrategy": "Validate accuracy of token filtering metrics and error rate tracking"
        },
        {
          "id": 9,
          "title": "Verify Non-Thinking Mode Configuration",
          "description": "Ensure all Qwen3-4B configurations explicitly set enable_thinking=False and remove any thinking mode references.",
          "dependencies": [
            3
          ],
          "details": "Audit all configuration files, code, and documentation to ensure thinking mode is never enabled. Replace any dual-mode monitoring with non-thinking mode specific monitoring. Remove any references to <think> blocks or thinking mode functionality.",
          "status": "pending",
          "testStrategy": "Verify all configurations have enable_thinking=False set and confirm absence of thinking mode references in code and documentation"
        },
        {
          "id": 10,
          "title": "Update Documentation to Reflect Non-Thinking Mode Only Policy",
          "description": "Update all documentation to clearly state that only non-thinking mode is supported for Qwen3-4B.",
          "dependencies": [
            9
          ],
          "details": "Review and update all documentation, including README files, inline comments, and maintenance guides to explicitly state that only non-thinking mode (enable_thinking=False) is supported per project policy.",
          "status": "pending",
          "testStrategy": "Review documentation for clarity and consistency regarding non-thinking mode only policy"
        }
      ]
    },
    {
      "id": 14,
      "title": "Migrate to Qwen3-4B Model with English Token Filtering and Mode Support",
      "description": "Analyze and integrate the Qwen3-4B model into the codebase, extract English-only tokens from its tokenizer, and update model loading, configuration, and inference logic to support Qwen3-4B's unique features, focusing exclusively on non-thinking mode operation.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "1. Review the Qwen3-4B model documentation and Hugging Face integration requirements, ensuring compatibility with the latest transformers library (>=4.51.0) to avoid loading errors.\n2. Analyze the Qwen3-4B tokenizer to identify and extract the set of English-only tokens. Implement a script or utility to filter these tokens, documenting the extraction process and results.\n3. Refactor the model loading and configuration code to support Qwen3-4B, including device mapping, torch_dtype, and context length settings as per model specs.\n4. Update inference logic to use Qwen3-4B in non-thinking mode only, ensuring the tokenizer's `apply_chat_template` is always called with `enable_thinking=False` parameter.\n5. Ensure that appropriate sampling parameters are set and configurable for non-thinking mode operation.\n6. Update documentation and code comments to reflect all changes, including migration steps, new configuration options, and any model-specific considerations, making it clear that only non-thinking mode is supported per project policy.",
      "testStrategy": "- Verify that the codebase loads Qwen3-4B without errors and that model inference works correctly in non-thinking mode, producing expected outputs.\n- Confirm that the English-only token extraction utility correctly identifies and outputs the relevant tokens, and validate the results against known English token sets.\n- Test that the sampling parameters are correctly applied during inference, and that outputs change as expected when parameters are varied.\n- Ensure that all code paths explicitly set `enable_thinking=False` when using the model's chat template.\n- Review updated documentation for clarity and completeness, confirming no references to thinking mode remain, and perform code review to ensure maintainability and adherence to project standards.",
      "subtasks": [
        {
          "id": 1,
          "title": "Set up Qwen3-4B model with transformers library",
          "description": "Configure the development environment with the latest transformers library (>=4.51.0) and implement basic model loading for Qwen3-4B to ensure compatibility.",
          "dependencies": [],
          "details": "Install or update transformers to version 4.51.0 or higher to avoid the 'KeyError: qwen3' error. Create a basic implementation to load the Qwen3-4B model using AutoModelForCausalLM and AutoTokenizer from the Hugging Face transformers library. Configure proper device mapping and torch_dtype settings for optimal performance. Test basic model loading and simple inference to verify setup.\n<info added on 2025-05-10T09:22:09.195Z>\n## Environment Setup\n- Install or update transformers to version 4.51.0 or higher: `pip install --upgrade transformers>=4.51.0`\n- Verify installation with: `import transformers; print(transformers.__version__)`\n- Install any additional dependencies: `torch`, `accelerate` if needed\n\n## Implementation Plan\n1. Create implementation file at `src/models/qwen3_loader.py`\n2. Use the following code structure:\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ndef load_qwen3_model():\n    model_id = \"Qwen/Qwen3-4B\"\n    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)\n    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    \n    # Load model with appropriate dtype\n    dtype = torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, \n        torch_dtype=dtype,\n        device_map=device\n    )\n    \n    return model, tokenizer, device\n\ndef test_inference(model, tokenizer, device):\n    prompt = \"Hello, world!\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    # Generate text\n    outputs = model.generate(**inputs, max_new_tokens=20)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {prompt}\")\n    print(f\"Output: {generated_text}\")\n    return generated_text\n```\n\n3. Create a test script at `scripts/test_qwen3_loading.py`:\n```python\nfrom src.models.qwen3_loader import load_qwen3_model, test_inference\n\nif __name__ == \"__main__\":\n    print(\"Loading Qwen3-4B model...\")\n    model, tokenizer, device = load_qwen3_model()\n    print(\"Model loaded successfully!\")\n    \n    print(\"\\nTesting basic inference...\")\n    test_inference(model, tokenizer, device)\n```\n\n## Potential Challenges\n- Memory requirements: Qwen3-4B may require 8GB+ RAM\n- If MPS/CUDA fails, fallback to CPU with `device_map=\"auto\"` and `low_cpu_mem_usage=True`\n- May need to use quantization for memory-constrained environments\n\n## Success Criteria\n- Model loads without errors\n- Basic text generation works with a simple prompt\n- Code is modular and reusable for the next subtask (token filtering)\n</info added on 2025-05-10T09:22:09.195Z>\n<info added on 2025-05-10T09:41:51.958Z>\nInstall or update transformers to version 4.51.0 or higher to avoid the 'KeyError: qwen3' error. Create a basic implementation to load the Qwen3-4B model using AutoModelForCausalLM and AutoTokenizer from the Hugging Face transformers library. Configure proper device mapping and torch_dtype settings for optimal performance. Test basic model loading and simple inference to verify setup.\n\n<info added on 2025-05-10T09:22:09.195Z>\n## Environment Setup\n- Install or update transformers to version 4.51.0 or higher: `pip install --upgrade transformers>=4.51.0`\n- Verify installation with: `import transformers; print(transformers.__version__)`\n- Install any additional dependencies: `torch`, `accelerate` if needed\n\n## Implementation Plan\n1. Create implementation file at `src/models/qwen3_loader.py`\n2. Use the following code structure:\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ndef load_qwen3_model():\n    model_id = \"Qwen/Qwen3-4B\"\n    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)\n    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    \n    # Load model with appropriate dtype\n    dtype = torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, \n        torch_dtype=dtype,\n        device_map=device\n    )\n    \n    return model, tokenizer, device\n\ndef test_inference(model, tokenizer, device):\n    prompt = \"Hello, world!\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    # Generate text\n    outputs = model.generate(**inputs, max_new_tokens=20)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {prompt}\")\n    print(f\"Output: {generated_text}\")\n    return generated_text\n```\n\n3. Create a test script at `scripts/test_qwen3_loading.py`:\n```python\nfrom src.models.qwen3_loader import load_qwen3_model, test_inference\n\nif __name__ == \"__main__\":\n    print(\"Loading Qwen3-4B model...\")\n    model, tokenizer, device = load_qwen3_model()\n    print(\"Model loaded successfully!\")\n    \n    print(\"\\nTesting basic inference...\")\n    test_inference(model, tokenizer, device)\n```\n\n## Potential Challenges\n- Memory requirements: Qwen3-4B may require 8GB+ RAM\n- If MPS/CUDA fails, fallback to CPU with `device_map=\"auto\"` and `low_cpu_mem_usage=True`\n- May need to use quantization for memory-constrained environments\n\n## Success Criteria\n- Model loads without errors\n- Basic text generation works with a simple prompt\n- Code is modular and reusable for the next subtask (token filtering)\n</info added on 2025-05-10T09:22:09.195Z>\n\n## Multi-Environment Implementation Strategy\n\nFor this project, we'll use different environments for different stages of the workflow:\n\n1. **Local Development & Data Preparation**: Use Hugging Face transformers\n   - Model ID: 'Qwen/Qwen3-4B'\n   - Use for all data preparation, tokenization analysis, and validation\n   - Ideal for token extraction and filtering work in the next subtask\n   - Provides full access to tokenizer internals needed for English token filtering\n\n2. **Quantized Inference**: Use Ollama\n   - Model: 'qwen:4b'\n   - Implement using the Ollama Python API\n   - Example code:\n   ```python\n   import ollama\n   \n   response = ollama.chat(model='qwen:4b', \n                         messages=[{'role': 'user', 'content': 'Hello, world!'}])\n   print(response['message']['content'])\n   ```\n   - Advantages: Lower memory footprint, faster inference on consumer hardware\n\n3. **Fine-tuning**: Use Unsloth\n   - Environment: Google Colab or Lightning.ai (NOT on Mac)\n   - Provides optimized fine-tuning for Qwen models\n   - Warning: Do not attempt to install or use Unsloth or xformers on Mac systems\n\n4. **Implementation Notes**:\n   - Create separate modules for each environment\n   - Use factory pattern to abstract model interactions\n   - Document environment-specific requirements in README.md\n   - Create environment-specific setup scripts for each workflow\n</info added on 2025-05-10T09:41:51.958Z>\n<info added on 2025-05-10T10:03:45.695Z>\n## Tokenizer-Only Approach for Data Tasks\n\nFor data preparation and token extraction tasks, we should only load the Qwen3-4B tokenizer without loading the full model. This approach significantly improves efficiency since:\n\n1. The tokenizer is much smaller (~100MB) compared to the full model (~8GB)\n2. Tokenization operations run faster without model overhead\n3. Memory requirements are drastically reduced\n4. Token analysis can be performed entirely with the tokenizer\n\n### Updated Implementation Structure\n\n1. Create a separate tokenizer-only loader function in `src/models/qwen3_loader.py`:\n\n```python\ndef load_qwen3_tokenizer_only():\n    model_id = \"Qwen/Qwen3-4B\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    return tokenizer\n\ndef load_qwen3_model_and_tokenizer():\n    \"\"\"Only use this function for evaluation or prompt testing\"\"\"\n    model_id = \"Qwen/Qwen3-4B\"\n    # Determine optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)\n    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    \n    # Load model with appropriate dtype\n    dtype = torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, \n        torch_dtype=dtype,\n        device_map=device\n    )\n    \n    return model, tokenizer, device\n```\n\n2. Update the test script to demonstrate both approaches:\n\n```python\nfrom src.models.qwen3_loader import load_qwen3_tokenizer_only, load_qwen3_model_and_tokenizer, test_inference\n\nif __name__ == \"__main__\":\n    # For data preparation tasks (token extraction, analysis)\n    print(\"Loading Qwen3-4B tokenizer only...\")\n    tokenizer = load_qwen3_tokenizer_only()\n    print(\"Tokenizer loaded successfully!\")\n    \n    # Example tokenization\n    text = \"Hello, world!\"\n    tokens = tokenizer.encode(text)\n    decoded = tokenizer.decode(tokens)\n    print(f\"Tokenized '{text}' to {tokens}\")\n    print(f\"Decoded back to: '{decoded}'\")\n    \n    # Only for evaluation/testing (comment out when not needed)\n    print(\"\\nLoading full Qwen3-4B model (only needed for inference)...\")\n    model, full_tokenizer, device = load_qwen3_model_and_tokenizer()\n    print(\"Model loaded successfully!\")\n    print(\"\\nTesting basic inference...\")\n    test_inference(model, full_tokenizer, device)\n```\n\n### Documentation Updates\n\nAdd the following to project documentation:\n\n- For all data preparation, token extraction, and analysis tasks, use ONLY the tokenizer\n- Full model loading should be restricted to:\n  - Final evaluation of filtered token sets\n  - Testing prompt generation with filtered vocabulary\n  - Benchmarking model performance with modified tokenizer\n\n### Memory and Performance Benefits\n\n- Tokenizer-only: ~100-200MB RAM usage\n- Full model: 8GB+ RAM usage\n- Tokenization speed: 5-10x faster without model loaded\n- Startup time: Near-instant for tokenizer vs. 30+ seconds for model\n\nThis approach will be particularly important for the next subtask \"Analyze and extract English-only tokens from Qwen3-4B tokenizer\" which requires extensive tokenizer operations but no model inference.\n</info added on 2025-05-10T10:03:45.695Z>",
          "status": "done",
          "testStrategy": "Verify successful model loading without errors and confirm basic text generation works with a simple prompt."
        },
        {
          "id": 2,
          "title": "Analyze and extract English-only tokens from Qwen3-4B tokenizer",
          "description": "Develop a methodology to identify and extract English tokens from the Qwen3-4B tokenizer, creating a filtered subset for English-only operations.",
          "dependencies": [
            1
          ],
          "details": "Load the Qwen3-4B tokenizer and analyze its vocabulary. Develop criteria for identifying English tokens (e.g., using regex patterns, Unicode ranges, or language detection algorithms). Create a script that extracts and saves the English token subset. Document the extraction methodology, criteria used, and statistical analysis of the results (total tokens, percentage of English tokens, etc.).\n<info added on 2025-05-10T10:14:27.786Z>\nLoad the Qwen3-4B tokenizer and analyze its vocabulary. Develop criteria for identifying English tokens (e.g., using regex patterns, Unicode ranges, or language detection algorithms). Create a script that extracts and saves the English token subset. Document the extraction methodology, criteria used, and statistical analysis of the results (total tokens, percentage of English tokens, etc.).\n\nImplementation Plan:\n1. Load the Qwen3-4B tokenizer using the existing function `load_qwen3_tokenizer_only()` in `src/models/qwen3_loader.py`.\n2. Access the tokenizer's vocabulary via `tokenizer.get_vocab()`.\n3. Develop criteria for identifying English tokens:\n   - Use regex to match tokens containing only English alphabet characters (A-Z, a-z), common English punctuation, and optionally numbers.\n   - Optionally, use Unicode ranges to further filter out non-English scripts.\n   - Consider using the `regex` library for advanced pattern matching.\n4. Write a script (suggested location: `scripts/extract_english_tokens.py`) that:\n   - Loads the tokenizer\n   - Iterates over the vocabulary\n   - Applies the English-token criteria\n   - Saves the filtered English tokens to a file (e.g., `data/processed/english_tokens_qwen3.txt`)\n   - Outputs statistics: total tokens, number and percentage of English tokens\n5. Document the methodology and criteria in the script docstring and output a summary report.\n6. For validation, encode/decode a sample English sentence using both the full and filtered token sets, and compare results.\n\nSpecial considerations:\n- Handle subwords and tokens with special characters appropriately\n- Account for byte-level or BPE encoding in the tokenizer\n- Apply Unicode normalization for edge cases\n- Ensure the filtered token set maintains coherence for English text processing\n</info added on 2025-05-10T10:14:27.786Z>",
          "status": "done",
          "testStrategy": "Validate the extracted English token set by encoding/decoding sample English texts and comparing results with the full tokenizer."
        },
        {
          "id": 3,
          "title": "Implement non-thinking mode support",
          "description": "Update the inference logic to ensure Qwen3-4B is always used in non-thinking mode, following project policy.",
          "dependencies": [
            1
          ],
          "details": "Modify the chat template application to explicitly set 'enable_thinking=False' in the tokenizer's apply_chat_template method. Create utility functions for generating responses that enforce non-thinking mode. Add safeguards to prevent accidental use of thinking mode. Document the implementation to make it clear that only non-thinking mode is supported per project policy.",
          "status": "done",
          "testStrategy": "Test that all inference calls correctly use non-thinking mode and verify that generated responses do not contain thinking content."
        },
        {
          "id": 4,
          "title": "Configure model-specific sampling parameters",
          "description": "Implement and configure appropriate sampling parameters for Qwen3-4B in non-thinking mode operation.",
          "dependencies": [
            3
          ],
          "details": "Create a configuration system for Qwen3-4B sampling parameters with reasonable defaults for non-thinking mode. Make these parameters configurable through API or configuration files. Implement parameter validation to ensure values are within acceptable ranges. Update the model generation code to apply these parameters during inference. Document recommended parameter settings for different use cases, all within non-thinking mode operation.\n<info added on 2025-05-10T12:00:04.119Z>\nCreate a configuration system for Qwen3-4B sampling parameters with reasonable defaults for non-thinking mode. Make these parameters configurable through API or configuration files. Implement parameter validation to ensure values are within acceptable ranges. Update the model generation code to apply these parameters during inference. Document recommended parameter settings for different use cases, all within non-thinking mode operation.\n\nImplementation plan based on codebase exploration and official Qwen3/Unsloth recommendations:\n\n1. Add support for all recommended sampling parameters for Qwen3-4B non-thinking mode:\n   - temperature: Controls randomness in generation (higher = more random)\n   - top_p: Nucleus sampling parameter (cumulative probability threshold)\n   - top_k: Limits vocabulary to top k tokens during sampling\n   - min_p: Minimum probability filtering relative to the most likely token\n\n2. Update the EvaluationConfig dataclass and YAML configuration format to include these parameters with the following defaults:\n   - temperature: 0.6\n   - top_p: 0.95\n   - top_k: 20\n   - min_p: 0.0\n\n3. Modify the model inference code to explicitly pass all sampling parameters to model.generate() function calls.\n\n4. Implement parameter validation logic to ensure all values fall within recommended ranges:\n   - temperature: 0.0 to 2.0 (0.0 = deterministic)\n   - top_p: 0.0 to 1.0\n   - top_k: 0 to infinity (0 = disabled)\n   - min_p: 0.0 to 1.0 (0.0 = disabled)\n\n5. Add comprehensive documentation for each parameter in both code comments and configuration templates, with references to Unsloth and Qwen3 official documentation.\n\n6. Ensure all implementation is strictly focused on non-thinking mode operation, maintaining alignment with project requirements.\n\nReferences:\n- Unsloth documentation: https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune\n- Qwen3 example notebook: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb\n</info added on 2025-05-10T12:00:04.119Z>",
          "status": "done",
          "testStrategy": "Verify that changing sampling parameters produces expected variations in model outputs while maintaining non-thinking mode operation."
        },
        {
          "id": 5,
          "title": "Integrate Qwen3-4B into existing codebase and document changes",
          "description": "Refactor the existing codebase to fully support Qwen3-4B in non-thinking mode, including model loading, configuration, and inference logic, with comprehensive documentation.",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Update model factory or loading mechanisms to recognize and properly initialize Qwen3-4B. Modify configuration schemas to include Qwen3-4B specific parameters (context length of 32,768, GQA attention heads configuration, etc.). Create migration guides for developers explaining how to transition to Qwen3-4B. Document all new features, parameters, and configuration options with examples, ensuring all documentation explicitly states that only non-thinking mode is supported. Update API documentation to reflect changes in behavior and capabilities.",
          "status": "done",
          "testStrategy": "Perform end-to-end testing of the integrated solution, verifying all components work together correctly with both simple and complex prompts, and that non-thinking mode is consistently enforced."
        },
        {
          "id": 6,
          "title": "Audit codebase for thinking mode references",
          "description": "Perform a comprehensive audit of the codebase to ensure all thinking mode references are removed or disabled.",
          "dependencies": [
            3,
            4,
            5
          ],
          "details": "Scan the entire codebase for any references to thinking mode, <think> blocks, or enable_thinking parameter. Create a checklist of all files and functions that need modification. Implement changes to ensure thinking mode is never enabled. Update comments, documentation, and variable names to reflect the non-thinking mode policy. Create automated tests that verify thinking mode is never enabled in any code path.\n<info added on 2025-05-10T12:38:51.017Z>\nThe audit of the codebase for thinking mode references has been completed with the following findings:\n\n1. Codebase Audit Results:\n   - Conducted a comprehensive search across all code, documentation, and configuration files for:\n     - `enable_thinking` parameter, config, or variable references\n     - `<think>` blocks\n     - \"thinking mode\" mentions in comments, documentation, or code\n\n2. Key Findings:\n   - All code paths, particularly in `src/models/qwen3_loader.py` and `src/evaluation/framework.py`, strictly enforce `enable_thinking=False`\n   - Any attempt to use `enable_thinking=True` triggers a `ValueError` through the `apply_qwen3_chat_template_non_thinking` function\n   - All `<think>` blocks and related logic have been completely removed\n   - Documentation (README, docs/analysis.md, docs/templates.md, docs/data_format.md, docs/token_extraction.md) clearly states that only non-thinking mode is supported\n   - All configuration files and example code explicitly set `enable_thinking=False`\n   - No references to thinking mode remain except as warnings or policy clarifications\n\n3. Files Audited:\n   - `src/models/qwen3_loader.py` - Verified enforcement of non-thinking mode with error handling\n   - `src/evaluation/framework.py` - Confirmed policy comments with no thinking mode logic\n   - All documentation files - Updated to reflect non-thinking mode only policy\n   - All task description files (`tasks/task_*.txt` and `tasks/tasks.json`) - Verified policy compliance\n\n4. Conclusion:\n   - The codebase is fully compliant with the non-thinking mode policy\n   - No code, configuration, or documentation supports thinking mode except to explicitly prohibit it\n   - All examples, comments, and documentation consistently reflect the project policy\n\n5. Next Steps:\n   - Continue monitoring for potential regressions through code review and CI processes\n   - Proceed to the next subtask for removing GPT-2 references and completing migration to Qwen3-4B\n</info added on 2025-05-10T12:38:51.017Z>",
          "status": "done",
          "testStrategy": "Use static code analysis tools to verify no thinking mode references remain. Create test cases that attempt to enable thinking mode and verify they fail or default to non-thinking mode."
        },
        {
          "id": 7,
          "title": "Remove GPT-2 references and fully migrate to Qwen3-4B",
          "description": "Update all configuration files, scripts, and code to remove references to GPT-2 and ensure Qwen3-4B is the default model for all workflows. This includes updating the model name in configs, ensuring model loaders use the config value, updating documentation, and cleaning up any GPT-2-specific scripts or tokenizers.",
          "details": "- Change model.name in configs/evaluation/base_config.yaml to 'Qwen/Qwen3-4B'.\n- Update model loading code to use the config value, not hardcoded model names.\n- Remove or refactor scripts that use GPT-2-specific tokenizers (e.g., src/data/token_extractor.py).\n- Update documentation and comments to reference Qwen3-4B as the default model.\n- Ensure all test and evaluation scripts work with Qwen3-4B.\n- Clean up any remaining GPT-2 artifacts or dependencies.\n<info added on 2025-05-10T12:48:52.765Z>\n## Implementation Plan for Qwen3-4B Migration\n\n### 1. Remove/Replace All GPT-2-Specific Code and Artifacts\n- Delete src/data/token_extractor.py as it's entirely GPT-2-specific (all extraction now uses Qwen3-4B)\n- Remove or rewrite src/data/create_notebook.py to validate Qwen3-4B English token extraction with english_tokens.json\n- Update notebooks/token_validation.ipynb:\n  - Change title and markdown to reference Qwen3-4B\n  - Update code to use english_tokens.json instead of gpt2_letter_tokens.json\n- Update scripts/test_evaluation.py:\n  - Replace all instances of 'sshleifer/tiny-gpt2' with 'Qwen/Qwen3-4B'\n  - Ensure model/tokenizer loading uses config values, not hardcoded references\n- In configs/evaluation/base_config.yaml:\n  - Change model.name to 'Qwen/Qwen3-4B'\n  - Update wandb.run_name from 'gpt2_evaluation' to 'qwen3_evaluation'\n- Delete data/processed/gpt2_letter_tokens.json if present\n- Update tasks/tasks.json to remove or update any tasks referencing GPT-2\n\n### 2. Update Documentation\n- Review and update README.md to remove GPT-2 references\n- Update docs/token_extraction.md to describe only Qwen3-4B workflows\n\n### 3. Update/Remove Downstream References\n- Identify and update all code referencing gpt2_letter_tokens.json to use english_tokens.json\n- Check for hardcoded paths or logic specific to GPT-2 that need refactoring\n- Verify downstream analysis or data processing works with the new token file format\n\n### 4. Clean Up and Verification\n- Remove any remaining GPT-2 artifacts, outputs, or dependencies\n- Run all scripts to ensure they work with Qwen3-4B\n- Verify all configs and documentation are Qwen3-4B-only\n- Document any breaking changes for other team members\n</info added on 2025-05-10T12:48:52.765Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 14
        }
      ]
    },
    {
      "id": 15,
      "title": "Convert Existing Work and Datasets for Qwen3-4B Compatibility",
      "description": "Update training data format, evaluation framework, and scripts to ensure compatibility with Qwen3-4B, including support for its tokenizer and non-thinking mode only, while validating the integrity of the original experiment.",
      "status": "done",
      "dependencies": [
        14
      ],
      "priority": "high",
      "details": "This task involves converting all existing training datasets and evaluation frameworks to be compatible with the Qwen3-4B model. Key steps include: (1) Updating the training data format to align with Qwen3's tokenizer, ensuring that only English tokens are used as per the English-only subset; (2) Adapting the evaluation framework to support Qwen3-4B in non-thinking mode only (enable_thinking=False), and ensuring that all evaluation metrics and scripts are updated to reflect this configuration; (3) Modifying all existing scripts and utilities to work seamlessly with the new tokenizer and model configuration, removing any logic related to thinking mode or <think> blocks; (4) Validating that the conversion process maintains the integrity of the original experiment's goals by comparing results before and after the migration. Special attention should be paid to the tokenizer's handling of English-only data, as this is critical for accurate and fair evaluation. Documentation should be updated to reflect all changes, explicitly state that only non-thinking mode is allowed per project policy, and provide clear guidance for future maintenance.",
      "testStrategy": "To verify task completion, perform the following: (1) Run the updated training pipeline with the converted datasets and confirm that the model trains without errors; (2) Execute the evaluation framework with non-thinking mode only (enable_thinking=False), ensuring that all metrics are reported correctly and that the results are consistent with the original experiment; (3) Validate that all scripts and utilities function as expected with the new tokenizer and model configuration, with no references to thinking mode; (4) Compare the results of the original and converted pipelines to ensure that the integrity of the experiment is maintained; (5) Review updated documentation for accuracy and completeness, confirming it clearly states that only non-thinking mode is allowed. Automated tests should be added where possible to catch regressions in data processing, tokenization, and evaluation logic, including tests that verify thinking mode is never enabled.",
      "subtasks": [
        {
          "id": 1,
          "title": "Convert Training Datasets to Qwen3-4B English-Only Tokenizer Format",
          "description": "Transform all existing training datasets to be compatible with the Qwen3-4B tokenizer, ensuring only English tokens are included as required by the English-only subset.",
          "dependencies": [],
          "details": "Analyze the Qwen3-4B tokenizer specifications and preprocess datasets to filter out non-English tokens, reformatting data as needed for model ingestion.",
          "status": "done",
          "testStrategy": "Validate that all tokens in the converted datasets are recognized by the Qwen3-4B tokenizer and that sample batches can be tokenized without errors."
        },
        {
          "id": 2,
          "title": "Update Evaluation Framework for Qwen3-4B Non-Thinking Mode",
          "description": "Adapt the evaluation framework to support Qwen3-4B in non-thinking mode only, and update evaluation metrics and scripts accordingly.",
          "dependencies": [
            1
          ],
          "details": "Review Qwen3-4B's API and modify evaluation scripts to always use non-thinking mode (enable_thinking=False), removing any code that references or enables thinking mode or <think> blocks, and ensure metrics capture relevant model behaviors in non-thinking mode only.",
          "status": "done",
          "testStrategy": "Run evaluation on sample data with non-thinking mode and verify that outputs and metrics are correctly captured. Confirm that no thinking mode functionality is present in the code."
        },
        {
          "id": 3,
          "title": "Refactor Scripts and Utilities for Qwen3-4B Compatibility",
          "description": "Modify all existing scripts and utilities to work seamlessly with the Qwen3-4B model configuration and tokenizer, ensuring thinking mode is never enabled.",
          "dependencies": [
            1
          ],
          "details": "Update data loading, preprocessing, and inference scripts to use the latest Qwen3-4B APIs with enable_thinking=False explicitly set. Remove any code that processes or generates <think> blocks or thinking mode outputs. Ensure compatibility with the model's configuration and tokenizer requirements.",
          "status": "done",
          "testStrategy": "Execute end-to-end training and inference pipelines using the updated scripts and confirm successful runs without compatibility errors. Verify that enable_thinking=False is consistently applied throughout all code."
        },
        {
          "id": 4,
          "title": "Validate Integrity of Experimental Results Post-Conversion",
          "description": "Ensure that the conversion process maintains the integrity of the original experiment by comparing results before and after migration to Qwen3-4B in non-thinking mode.",
          "dependencies": [
            2,
            3
          ],
          "details": "Design and run comparative experiments using both the original and converted setups, analyzing key metrics to confirm that experimental goals and data integrity are preserved when using Qwen3-4B in non-thinking mode only.",
          "status": "done",
          "testStrategy": "Statistically compare pre- and post-conversion results, documenting any discrepancies and verifying that performance and evaluation outcomes remain consistent when using non-thinking mode."
        },
        {
          "id": 5,
          "title": "Update Documentation and Provide Maintenance Guidance",
          "description": "Revise all relevant documentation to reflect changes made for Qwen3-4B compatibility in non-thinking mode only and offer clear guidance for future maintenance.",
          "dependencies": [
            4
          ],
          "details": "Document new data formats, evaluation procedures, script changes, and best practices for working with Qwen3-4B in non-thinking mode only. Explicitly state that project policy requires enable_thinking=False for all Qwen3-4B usage. Include handling of English-only data and ensure no references to thinking mode remain in documentation.",
          "status": "done",
          "testStrategy": "Review documentation for completeness and clarity, ensuring it explicitly states the non-thinking mode requirement. Solicit feedback from team members to ensure usability for future updates."
        },
        {
          "id": 6,
          "title": "Add Safeguards Against Thinking Mode Usage",
          "description": "Implement safeguards in code to prevent accidental use of thinking mode in any part of the system.",
          "dependencies": [
            2,
            3
          ],
          "details": "Add validation checks in model initialization and inference code to ensure enable_thinking is always set to False. Create CI/CD tests that verify no thinking mode code exists in the codebase. Consider adding runtime assertions that will fail if thinking mode is detected.",
          "status": "done",
          "testStrategy": "Create test cases that attempt to enable thinking mode and verify they fail appropriately. Run static code analysis to detect any remaining references to thinking mode or <think> blocks."
        }
      ]
    },
    {
      "id": 16,
      "title": "Update Environment Setup Scripts and Onboarding Docs for Qwen3-4B and Token Extraction",
      "description": "Revise all environment setup scripts and onboarding documentation to ensure compatibility with Qwen3-4B and the new English-only token extraction process, including updated installation instructions for transformers (>=4.51.0) and usage guidance for new token extraction scripts. Clearly distinguish between local (Mac/Apple Silicon) and cloud-based workflows.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "Review all existing environment setup scripts (e.g., shell scripts, Python setup files) and onboarding documentation to ensure they reference and support Qwen3-4B as the default model. Update instructions to include installation or upgrade steps for the transformers library (version 4.51.0 or higher), specifying any required flags or compatibility notes. \n\nClearly document the split between local and cloud workflows:\n- Local Mac (Apple Silicon) setup: Specify that only Mac-compatible packages should be installed. Explicitly note that Unsloth and xformers should NOT be installed locally. Include instructions to use `uv pip install ollama` for the Ollama Python API. Document using Hugging Face transformers for data preparation and tokenization.\n- Cloud workflow: Clarify that all fine-tuning and Unsloth steps should be performed in cloud environments only (Google Colab or Lightning).\n\nAdd explicit instructions for using Ollama for local quantized inference and transformers for data preparation. Integrate clear, step-by-step guidance for using the new English-only token extraction scripts, including prerequisites, example commands, and troubleshooting tips. Reference the new workflow and model requirements throughout, ensuring that all documentation is consistent and unambiguous. Coordinate with the team to verify that all changes align with the latest project standards and dependencies established in Task 1.",
      "testStrategy": "Verify that a new developer can follow the updated scripts and documentation to set up a working environment from scratch on both local Mac (Apple Silicon) and cloud environments. Test that developers can successfully install transformers (>=4.51.0) and Ollama locally, and run the new token extraction scripts with Qwen3-4B. Verify that the documentation clearly distinguishes which operations should be performed locally versus in the cloud. Confirm that all references to model requirements and workflows are accurate and that no outdated instructions remain. Test the local quantized inference workflow with Ollama. Solicit feedback from at least one team member not involved in the update to ensure clarity and completeness.",
      "subtasks": [
        {
          "id": 1,
          "title": "Update README and main onboarding documentation",
          "description": "Revise the main README.md and onboarding documentation to reflect current environment setup requirements for Qwen3-4B compatibility",
          "dependencies": [],
          "details": "Review and update the main project documentation to ensure it accurately describes the current setup process, prerequisites, and environment configuration. Include clear sections on project purpose, high-level architecture, and getting started steps. Ensure all links are valid and documentation reflects the current state of the project.\n<info added on 2025-05-10T14:07:49.788Z>\nReview and update the main project documentation to ensure it accurately describes the current setup process, prerequisites, and environment configuration. Include clear sections on project purpose, high-level architecture, and getting started steps. Ensure all links are valid and documentation reflects the current state of the project.\n\nThe documentation update should focus on creating a clear distinction between local (Mac/Apple Silicon) workflows and cloud workflows. This includes:\n\n1. Review all existing documentation:\n   - README.md in the project root\n   - All onboarding documentation in the docs/ directory\n   - Any setup scripts in the scripts/ directory\n\n2. Create or update the following sections:\n   - \"Local (Mac/Apple Silicon) Workflow\" - focusing on data preparation using transformers and inference with Ollama\n   - \"Cloud Workflow\" - covering fine-tuning, Unsloth, xformers, and training processes\n   - Add a comparison table or bullet list clearly showing which steps should be performed locally vs. in the cloud\n\n3. Add explicit warnings about environment-specific dependencies:\n   - Highlight that Unsloth and xformers are cloud-only and should never be installed locally\n   - Provide clear installation instructions for local environment: `uv pip install ollama`, transformers for data preparation\n   - Document cloud-specific setup for training with Qwen3-4B\n\n4. Include practical examples:\n   - Example commands for each environment\n   - Troubleshooting section addressing common mistakes (especially attempting to install cloud-only packages locally)\n   - References to the updated requirements from Task 16\n\n5. Ensure consistency across all documentation:\n   - Remove or update any outdated instructions\n   - Verify that all documentation presents a unified approach to the local/cloud workflow split\n   - Make the distinction between environments unambiguous for new users\n</info added on 2025-05-10T14:07:49.788Z>",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Create Mac/Apple Silicon specific installation instructions",
          "description": "Develop detailed installation instructions specifically for Mac users with Apple Silicon processors",
          "dependencies": [
            1
          ],
          "details": "Document the specific steps, packages, and configurations needed for Apple Silicon Macs. Include troubleshooting for common issues, performance optimization tips, and any workarounds needed for compatibility. Test the instructions on an M1/M2 machine if possible to verify accuracy.\n<info added on 2025-05-10T14:14:39.024Z>\nDocument the specific steps, packages, and configurations needed for Apple Silicon Macs. Include troubleshooting for common issues, performance optimization tips, and any workarounds needed for compatibility. Test the instructions on an M1/M2 machine if possible to verify accuracy.\n\nThe Mac/Apple Silicon installation instructions should include:\n\n1. Step-by-step environment setup:\n   - Detailed instructions for using `uv` and virtualenv specifically on Apple Silicon\n   - Clear guidance on installing core dependencies with proper architecture (arm64)\n   - Instructions for Ollama installation for local quantized inference\n\n2. Troubleshooting section for common Apple Silicon issues:\n   - Solutions for torch/transformers installation errors (ensuring proper Apple Silicon wheels)\n   - Fixing Homebrew and system dependency issues (libomp, openssl)\n   - Resolving Jupyter/Matplotlib backend issues specific to macOS\n\n3. Performance optimization tips:\n   - Instructions for obtaining and using Apple Silicon-optimized wheels for torch/transformers\n   - Guidance on avoiding Python under Rosetta 2 emulation\n   - Best practices for using `uv` to create faster, reproducible environments\n   - Tips for leveraging MPS (Metal Performance Shaders) acceleration\n\n4. Verification steps:\n   - A test script to verify torch, transformers, and Ollama imports\n   - Commands to confirm correct Python architecture (arm64)\n   - Instructions to print torch device info to confirm MPS availability\n   - Simple inference test to verify end-to-end functionality\n\n5. Reference table:\n   - Common errors and their solutions\n   - Required vs. optional dependencies\n   - Version compatibility information\n\nThe documentation should be concise yet comprehensive, with clear distinction between required steps and optional optimizations. Consider creating a dedicated Mac-specific document or a clearly labeled section in the README.md.\n</info added on 2025-05-10T14:14:39.024Z>",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Document Ollama integration for local inference",
          "description": "Add comprehensive documentation on setting up and using Ollama for local model inference",
          "dependencies": [
            1
          ],
          "details": "Create step-by-step instructions for installing Ollama, configuring it for the project, and running local inference. Include examples of common commands, performance expectations, resource requirements, and troubleshooting tips. Document how to verify successful setup and integration with the project.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Update transformers documentation for data preparation",
          "description": "Revise documentation related to using transformers library for data preparation and token extraction scripts",
          "dependencies": [
            1
          ],
          "details": "Update all documentation related to the transformers library usage, focusing on data preparation workflows and token extraction scripts. Include code examples, expected inputs/outputs, and guidance on handling different data formats. Ensure compatibility with the latest transformers version and Qwen3-4B model.",
          "status": "done"
        },
        {
          "id": 5,
          "title": "Create cloud workflow guidance documentation",
          "description": "Develop clear documentation distinguishing between local and cloud workflows with specific guidance for cloud environments",
          "dependencies": [
            1,
            3,
            4
          ],
          "details": "Create documentation that clearly separates local development workflows from cloud-based production workflows. Include cloud setup instructions, environment variables, security considerations, and cost management. Document the differences in configuration, performance expectations, and recommended practices for each environment.",
          "status": "done"
        }
      ]
    },
    {
      "id": 17,
      "title": "Refactor Token Extraction Pipeline for Qwen3-4B English Subset",
      "description": "Replace all GPT-2 token extraction scripts and outputs with a new process that extracts the English-only token subset from the Qwen3-4B tokenizer. Validate, document, and update all downstream references to use the new extraction process and outputs.",
      "details": "1. Analyze the Qwen3-4B tokenizer using the latest Hugging Face 'transformers' library to extract its full vocabulary. 2. Define clear criteria for identifying 'English-only' tokens (e.g., tokens composed exclusively of ASCII letters and common English punctuation, excluding tokens with non-English characters or symbols). 3. Implement a new extraction script (preferably in Python) that filters the Qwen3-4B vocabulary to produce the English-only token subset, saving the result in a well-documented JSON format. 4. Validate the extracted subset by sampling and manually inspecting tokens, and by running automated checks for non-English characters. 5. Update all downstream scripts, references, and documentation to use the new Qwen3-4B English token subset and extraction process, removing all GPT-2-specific logic and outputs. 6. Provide comprehensive documentation describing the extraction criteria, process, and usage of the new subset.",
      "testStrategy": "- Run the new extraction script and verify that the output JSON contains only tokens matching the defined English-only criteria. - Manually inspect a random sample of extracted tokens to confirm correctness. - Execute automated tests to ensure no non-English characters are present in the subset. - Confirm that all downstream scripts and documentation reference the new Qwen3-4B English token subset and that no GPT-2-specific code or outputs remain. - Review documentation for completeness and clarity regarding the new extraction process and criteria.",
      "status": "pending",
      "dependencies": [
        2
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement the New Token Extraction Script",
          "description": "Develop and refactor the token extraction pipeline to use the new extraction script, ensuring it is optimized for efficiency and maintains functional consistency with the previous implementation.",
          "dependencies": [],
          "details": "This involves rewriting or updating the extraction logic, integrating any new requirements, and ensuring the script is ready for validation. Focus on minimizing token usage and maintaining coherent context as per the new model's needs.",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Validate and Document the Extraction Output",
          "description": "Test the new extraction script to ensure its output matches expected results and document the extraction process, output format, and any changes from the previous version.",
          "dependencies": [
            1
          ],
          "details": "Perform thorough validation using representative data, compare outputs with the previous pipeline, and document the new script's behavior, usage instructions, and output specifications for future reference.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Update Downstream References and Scripts",
          "description": "Identify and update all downstream scripts and references that depend on the token extraction pipeline to ensure compatibility with the new extraction script and output format.",
          "dependencies": [
            2
          ],
          "details": "Audit all dependent codebases, update integration points, and perform end-to-end testing to confirm that downstream processes function correctly with the refactored pipeline.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 18,
      "title": "Recreate All Dataset Splits Using Qwen3-4B English-Only Token Subset",
      "description": "Regenerate the training, validation, and test datasets using the Qwen3-4B English-only token subset, ensuring compatibility with the new tokenizer and updating all metadata and data structures to match the revised format.",
      "details": "Leverage the updated token extraction pipeline from Task 17 to process all raw data and regenerate the training, validation, and test splits exclusively using the Qwen3-4B English-only token subset. For each split, re-tokenize all examples with the Qwen3-4B tokenizer (as available via Hugging Face Transformers) and verify that every example is fully compatible—i.e., all tokens are present in the English-only subset, and no legacy or out-of-vocabulary tokens remain. Update all dataset metadata (e.g., token counts, vocabulary statistics, data format descriptors) to reflect the new tokenization. Ensure that the data structures, file formats, and split boundaries strictly follow the updated requirements, and that all downstream consumers of the data are notified of the changes. Document any edge cases or data exclusions resulting from incompatibility with the new tokenizer.",
      "testStrategy": "1. For each dataset split, confirm that all examples are tokenized exclusively with the Qwen3-4B English-only token subset and that no out-of-vocabulary or legacy tokens are present. 2. Validate that the number of examples and split boundaries match the intended design and that no data leakage occurs between splits. 3. Check that all metadata fields (e.g., token counts, vocabulary lists, format descriptors) are accurate and up-to-date. 4. Run automated compatibility tests to ensure all downstream scripts and pipelines can load and process the new datasets without errors. 5. Manually inspect a sample of examples from each split to confirm correct tokenization and data structure. 6. Document and review any data exclusions or format changes with the team before final sign-off.",
      "status": "pending",
      "dependencies": [
        3,
        17
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Regenerate Dataset Splits with New Token Subset",
          "description": "Process the dataset with the new tokenizer subset and create appropriate train/test/validation splits",
          "dependencies": [],
          "details": "Use the train_test_split() function to create appropriate dataset splits with the new tokenization. Consider using a 80/10/10 split for train/validation/test or adjust based on project requirements. Ensure proper shuffling of data before splitting. For large datasets, consider using sharding techniques to manage memory efficiently.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Validate Dataset Compatibility and Metadata",
          "description": "Verify the regenerated dataset splits maintain compatibility with models and contain correct metadata",
          "dependencies": [
            1
          ],
          "details": "Compare token distributions between old and new datasets. Verify schema consistency across all splits. Check for potential data leakage between splits. Validate that tokenization is consistent and properly cached to avoid redundant processing. Run basic model inference tests to ensure compatibility with existing models.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Update Downstream Consumers and Documentation",
          "description": "Communicate changes and update all systems that consume the dataset",
          "dependencies": [
            2
          ],
          "details": "Update pipeline configurations to use the new dataset splits. Modify any hardcoded references to the old dataset structure. Update documentation with new dataset statistics and tokenization details. Communicate changes to all stakeholders, including information about potential impacts on model performance. Create examples showing how to properly load and use the new dataset splits.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 19,
      "title": "Revalidate and Update Template-Generated Training Data for Qwen3-4B English Token Subset",
      "description": "Audit all template-generated training data to ensure strict compatibility with the Qwen3-4B English-only token subset, updating templates, separators, and formatting scripts as necessary. Regenerate non-compliant data and revise documentation to reflect all changes, clearly distinguishing between local data processing and cloud-based training workflows.",
      "status": "pending",
      "dependencies": [
        4,
        18
      ],
      "priority": "high",
      "details": "Begin by auditing all existing template-generated training data, focusing on tokenization compatibility with the Qwen3-4B English-only token subset as established in Task 18. Review and update all data generation templates, separator styles, and formatting scripts to ensure they produce outputs that exclusively use valid English tokens recognized by the Qwen3-4B tokenizer. For any data that fails validation, regenerate it using the updated templates and scripts. Ensure that all formatting (including separators, prompt structures, and metadata) adheres to the new requirements. All template generation and validation should be performed locally using Hugging Face transformers and the Qwen3-4B tokenizer. Update internal documentation to describe the revised templates, formatting conventions, and validation procedures, with clear distinction between local data processing workflows and cloud-based training workflows. Explicitly document that fine-tuning and LoRA optimizations are cloud-only operations (Colab/Lightning), and that Unsloth or xformers should not be used in local environments. Coordinate with the teams responsible for dataset splits and downstream consumers to ensure seamless integration of the updated data.",
      "testStrategy": "1. Run automated tokenization checks on all template-generated data to confirm exclusive use of Qwen3-4B English-only tokens using local Hugging Face transformers. 2. Manually inspect a representative sample of data for correct template application, separator usage, and formatting. 3. Verify that all scripts and templates have been updated and are producing compliant outputs. 4. Confirm that all previously non-compliant data has been regenerated and replaced. 5. Review updated documentation for completeness and clarity, especially regarding the separation between local data processing and cloud-based training workflows. 6. Ensure downstream data consumers can ingest and process the revised datasets without errors. 7. Validate that documentation clearly indicates which operations should be performed locally versus in cloud environments.",
      "subtasks": [
        {
          "id": 1,
          "title": "Audit and Update Templates/Scripts for Tokenizer Compatibility",
          "description": "Review all existing templates and scripts to identify non-compliant data generation patterns. Update them to ensure compatibility with the tokenizer and maintain audit trails for changes.",
          "dependencies": [],
          "details": "This includes examining current data handling processes, identifying sensitive or non-compliant data streams, and updating templates/scripts to align with tokenizer requirements. Ensure all changes are logged for compliance and future audits.",
          "status": "pending"
        },
        {
          "id": 2,
          "title": "Regenerate Non-Compliant Data",
          "description": "Use the updated templates/scripts to regenerate any previously generated data that does not meet tokenizer compatibility standards.",
          "dependencies": [
            1
          ],
          "details": "After updating the templates/scripts, identify all instances of non-compliant data and systematically regenerate them using the new, compliant processes. Ensure that regenerated data is properly mapped and stored according to tokenization best practices.",
          "status": "pending"
        },
        {
          "id": 3,
          "title": "Revise Documentation and Downstream Integration",
          "description": "Update all relevant documentation and notify or adjust downstream systems and integrations to reflect the changes in data generation and tokenization processes.",
          "dependencies": [
            2
          ],
          "details": "Revise user guides, technical documentation, and integration instructions to reflect the new processes. Communicate changes to stakeholders and ensure downstream systems are tested and updated as needed to handle the revised data formats.",
          "status": "pending"
        },
        {
          "id": 4,
          "title": "Document Local vs. Cloud Workflow Separation",
          "description": "Create clear documentation distinguishing between local data processing and cloud-based training workflows.",
          "dependencies": [
            1
          ],
          "details": "Document that all template generation and validation can be performed locally using Hugging Face transformers and the Qwen3-4B tokenizer. Explicitly note that fine-tuning and LoRA optimizations are cloud-only operations (Colab/Lightning), and that Unsloth or xformers should not be used in local environments. Create workflow diagrams showing the separation between local and cloud processes for data generation, validation, and training.",
          "status": "pending"
        },
        {
          "id": 5,
          "title": "Implement Local Validation Tools Using HF Transformers",
          "description": "Develop and document local validation tools using Hugging Face transformers to verify token compatibility.",
          "dependencies": [
            1
          ],
          "details": "Create scripts that leverage the Hugging Face transformers library to locally validate that all generated data uses only tokens from the Qwen3-4B English-only subset. Document the installation and usage of these tools, ensuring they can be run in standard local environments without cloud-specific optimizations like Unsloth or xformers.",
          "status": "pending"
        }
      ]
    }
  ]
}