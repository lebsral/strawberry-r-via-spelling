# Task ID: 8
# Title: Model Fine-tuning and Experimentation
# Status: pending
# Dependencies: 4, 6, 7
# Priority: high
# Description: Implement the training loop and run experiments with different hyperparameters to find the optimal configuration for effective transfer learning using cloud GPU environments.
# Details:
**IMPORTANT NOTE: This task requires a cloud GPU environment for Unsloth-based fine-tuning. Do not attempt on local Mac.**

1. Create a reusable training script that accepts hyperparameter configs (`src/training/train.py`)
2. Implement the training loop using Unsloth on Google Colab or https://lightning.ai/lars/home
3. Set up checkpoint saving and loading system (`src/training/checkpointing.py`)
4. Implement early stopping based on validation metrics
5. Run experiments with different hyperparameters focused on transfer learning effectiveness:
   - LoRA rank (r): [4, 8, 16, 32]
   - LoRA alpha: [8, 16, 32, 64]
   - Learning rate: [1e-4, 2e-4, 5e-4, 1e-3]
   - Batch size: [4, 8, 16, 32]
   - Gradient accumulation steps: [1, 2, 4, 8]
   - Training steps: [500, 1000, 2000, 5000]
6. Track all experiments in W&B with both spelling metrics and transfer metrics
7. Analyze correlation between spelling improvement and transfer performance
8. Identify training patterns that lead to better transfer learning

**Transfer Learning Focus:**
- Primary training on spelling variation tasks
- Monitor transfer learning effectiveness to position/count tasks
- Track correlation between spelling performance and transfer capabilities
- Identify which training approaches generalize better across task types

**File Structure:**
- Training Infrastructure:
  - Training script: `src/training/train.py`
  - Training utilities: `src/training/utils.py`
  - Data loaders: `src/training/data_loaders.py`
  - Model checkpointing: `src/training/checkpointing.py`
  - Transfer metrics: `src/training/transfer_metrics.py`

- Model Components:
  - Model architecture: `src/models/spelling_model.py`
  - Loss functions: `src/models/losses.py`
  - Metrics tracking: `src/models/metrics.py`
  - Model utilities: `src/models/utils.py`
  - Transfer evaluation: `src/models/transfer_eval.py`

- Deployment Components:
  - Model export: `src/deployment/model_export.py`
  - API implementation: `src/deployment/api.py`
  - Performance monitoring: `src/deployment/monitoring.py`
  - Load testing: `src/deployment/benchmark.py`
  - Performance visualization: `src/deployment/visualization.py`
  - Report generation: `src/deployment/report.py`
  - Transfer analysis: `src/deployment/transfer_analysis.py`

- Configurations:
  - Training config: `configs/training/config.yaml`
  - Model config: `configs/models/model_config.yaml`
  - Optimizer config: `configs/training/optimizer.yaml`
  - Scheduler config: `configs/training/scheduler.yaml`
  - Transfer config: `configs/training/transfer_config.yaml`

- Results and Checkpoints:
  - Model checkpoints: `checkpoints/`
  - Training logs: `results/training_logs/`
  - Performance metrics: `results/metrics/`
  - Error analysis: `results/error_analysis/`
  - Transfer analysis: `results/transfer_analysis/`
  - Deployment results: `results/deployment/`
    - Figures: `results/deployment/figures/`
    - Reports: `results/deployment/reports/`
    - Data: `results/deployment/data/`
    - Models: `results/deployment/models/`

- Documentation:
  - Training guide: `docs/training.md`
  - Model architecture: `docs/model.md`
  - Results analysis: `docs/results.md`
  - Transfer learning analysis: `docs/transfer_learning.md`

Implementation:
```python
import os
import yaml
import torch
import wandb
import numpy as np
from unsloth import FastLanguageModel
from datasets import load_dataset
from transformers import TrainingArguments, Trainer, EarlyStoppingCallback
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Main experiment runner
def run_experiment(config_path):
    # Load config
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    
    # Initialize W&B
    run = wandb.init(
        project="llm-spelling-finetuning",
        name=config["experiment_name"],
        config=config,
        reinit=True
    )
    
    # Load model and tokenizer with Unsloth
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="gpt2",
        max_seq_length=512,
        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        load_in_4bit=True
    )
    
    # Add LoRA adapters
    model = FastLanguageModel.get_peft_model(
        model,
        r=config["lora_config"]["r"],
        target_modules=config["lora_config"]["target_modules"],
        lora_alpha=config["lora_config"]["alpha"],
        lora_dropout=config["lora_config"]["dropout"],
        bias="none",
        use_gradient_checkpointing=True,
        random_state=42
    )
    
    # Load datasets - both spelling and transfer task datasets
    spelling_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset")
    position_dataset = load_dataset("YOUR-USERNAME/llm-position-dataset")
    count_dataset = load_dataset("YOUR-USERNAME/llm-count-dataset")
    
    # Format dataset for instruction fine-tuning
    def formatting_func(examples):
        questions = examples["question"]
        answers = examples["answer"]
        
        prompts = [f"<human>: {q}\n<assistant>: " for q in questions]
        completions = [f"{a}{tokenizer.eos_token}" for a in answers]
        
        return {"prompt": prompts, "completion": completions}
    
    train_dataset = spelling_dataset["train"].map(formatting_func, batched=True)
    eval_dataset = spelling_dataset["validation"].map(formatting_func, batched=True)
    
    # Format transfer task datasets for evaluation
    position_eval_dataset = position_dataset["validation"].map(formatting_func, batched=True)
    count_eval_dataset = count_dataset["validation"].map(formatting_func, batched=True)
    
    # Set up output directory
    output_dir = f"./results/{config['experiment_name']}_{config['timestamp']}"
    os.makedirs(output_dir, exist_ok=True)
    
    # Create training arguments
    training_args = FastLanguageModel.get_train_args(
        output_dir=output_dir,
        per_device_train_batch_size=config["training_config"]["per_device_train_batch_size"],
        gradient_accumulation_steps=config["training_config"]["gradient_accumulation_steps"],
        warmup_steps=config["training_config"]["warmup_steps"],
        max_steps=config["training_config"]["max_steps"],
        learning_rate=config["training_config"]["learning_rate"],
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=200,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        optim="adamw_torch",
        max_grad_norm=0.3,
        report_to="wandb"
    )
    
    # Define compute metrics function for transfer learning evaluation
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=2)
        
        # Only consider non-padding tokens
        mask = labels != -100
        labels = labels[mask]
        predictions = predictions[mask]
        
        accuracy = accuracy_score(labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
        
        return {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
    
    # Create trainer with early stopping
    trainer = FastLanguageModel.get_trainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=FastLanguageModel.get_data_collator(tokenizer),
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )
    
    # Train model
    trainer.train()
    
    # Save final model
    trainer.save_model(f"{output_dir}/final")
    
    # Evaluate on spelling validation set
    spelling_eval_results = trainer.evaluate()
    
    # Evaluate on transfer task datasets
    position_eval_results = trainer.evaluate(eval_dataset=position_eval_dataset)
    count_eval_results = trainer.evaluate(eval_dataset=count_eval_dataset)
    
    # Calculate transfer metrics
    transfer_metrics = {
        "position_transfer_score": position_eval_results["eval_accuracy"],
        "count_transfer_score": count_eval_results["eval_accuracy"],
        "avg_transfer_score": (position_eval_results["eval_accuracy"] + count_eval_results["eval_accuracy"]) / 2,
        "spelling_to_position_ratio": spelling_eval_results["eval_accuracy"] / position_eval_results["eval_accuracy"] if position_eval_results["eval_accuracy"] > 0 else 0,
        "spelling_to_count_ratio": spelling_eval_results["eval_accuracy"] / count_eval_results["eval_accuracy"] if count_eval_results["eval_accuracy"] > 0 else 0
    }
    
    # Log all results
    wandb.log({
        **spelling_eval_results,
        "position_eval_results": position_eval_results,
        "count_eval_results": count_eval_results,
        **transfer_metrics
    })
    
    # Save evaluation results
    all_results = {
        "spelling_eval_results": spelling_eval_results,
        "position_eval_results": position_eval_results,
        "count_eval_results": count_eval_results,
        "transfer_metrics": transfer_metrics
    }
    
    with open(f"{output_dir}/eval_results.yaml", "w") as f:
        yaml.dump(all_results, f)
    
    # Close wandb run
    wandb.finish()
    
    return output_dir, all_results

# Run multiple experiments
def run_experiments(config_paths):
    results = {}
    for config_path in config_paths:
        print(f"Running experiment with config: {config_path}")
        output_dir, eval_results = run_experiment(config_path)
        
        # Extract config name
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        
        results[config["experiment_name"]] = {
            "output_dir": output_dir,
            "eval_results": eval_results
        }
    
    # Analyze transfer learning effectiveness across experiments
    analyze_transfer_effectiveness(results)
    
    # Save all results
    with open("experiment_results_summary.yaml", "w") as f:
        yaml.dump(results, f)
    
    return results

# Analyze transfer learning effectiveness
def analyze_transfer_effectiveness(results):
    """Analyze which training patterns lead to better transfer learning"""
    # Extract key metrics for analysis
    experiment_metrics = []
    for exp_name, exp_data in results.items():
        with open(f"{exp_data['output_dir']}/config.yaml", "r") as f:
            config = yaml.safe_load(f)
        
        metrics = {
            "experiment_name": exp_name,
            "lora_rank": config["lora_config"]["r"],
            "lora_alpha": config["lora_config"]["alpha"],
            "learning_rate": config["training_config"]["learning_rate"],
            "batch_size": config["training_config"]["per_device_train_batch_size"],
            "grad_accum_steps": config["training_config"]["gradient_accumulation_steps"],
            "training_steps": config["training_config"]["max_steps"],
            "spelling_accuracy": exp_data["eval_results"]["spelling_eval_results"]["eval_accuracy"],
            "position_accuracy": exp_data["eval_results"]["position_eval_results"]["eval_accuracy"],
            "count_accuracy": exp_data["eval_results"]["count_eval_results"]["eval_accuracy"],
            "avg_transfer_score": exp_data["eval_results"]["transfer_metrics"]["avg_transfer_score"]
        }
        experiment_metrics.append(metrics)
    
    # Calculate correlations between spelling performance and transfer tasks
    import pandas as pd
    df = pd.DataFrame(experiment_metrics)
    correlation = df[["spelling_accuracy", "position_accuracy", "count_accuracy", "avg_transfer_score"]].corr()
    
    # Identify top performing configurations for transfer learning
    df_sorted = df.sort_values(by="avg_transfer_score", ascending=False)
    top_configs = df_sorted.head(5)
    
    # Save analysis results
    os.makedirs("results/transfer_analysis", exist_ok=True)
    correlation.to_csv("results/transfer_analysis/metric_correlations.csv")
    top_configs.to_csv("results/transfer_analysis/top_transfer_configs.csv")
    
    # Generate visualization of transfer learning effectiveness
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 6))
    plt.scatter(df["spelling_accuracy"], df["avg_transfer_score"])
    plt.xlabel("Spelling Task Accuracy")
    plt.ylabel("Average Transfer Task Accuracy")
    plt.title("Correlation between Spelling Performance and Transfer Learning")
    plt.savefig("results/transfer_analysis/spelling_vs_transfer.png")
    
    # Log findings to wandb
    wandb.init(project="llm-spelling-finetuning", name="transfer_analysis", reinit=True)
    wandb.log({
        "correlation_matrix": wandb.Table(dataframe=correlation),
        "top_transfer_configs": wandb.Table(dataframe=top_configs),
        "spelling_vs_transfer_plot": wandb.Image("results/transfer_analysis/spelling_vs_transfer.png")
    })
    wandb.finish()
```

# Test Strategy:
1. Verify training script (`src/training/train.py`) runs without errors on Google Colab or lightning.ai
2. Confirm experiments are properly tracked in W&B with both spelling and transfer metrics
3. Check that checkpoints are saved correctly to `checkpoints/` directory
4. Verify early stopping works as expected
5. Test that the best model is loaded at the end of training
6. Compare performance across different hyperparameter configurations using Python scripts
7. Ensure all experiment results are properly saved to `results/metrics/` and `results/training_logs/`
8. Verify the environment is properly set up with GPU access before starting experiments
9. Validate that all configuration files in `configs/` directory are properly loaded and applied
10. Test transfer learning evaluation on position and count tasks
11. Verify correlation analysis between spelling performance and transfer capabilities
12. Test the transfer analysis visualization generation
13. Validate the identification of optimal training patterns for transfer learning
14. Test deployment scripts in `src/deployment/` directory:
    - Verify model export functionality in `model_export.py`
    - Test API implementation in `api.py`
    - Validate monitoring capabilities in `monitoring.py`
    - Check benchmark functionality in `benchmark.py`
    - Test visualization generation in `visualization.py`
    - Verify HTML report generation in `report.py`
    - Test transfer analysis in `transfer_analysis.py`
15. Ensure all deployment and analysis results are correctly saved to the appropriate directories

# Subtasks:
## 1. Training Script Implementation [pending]
### Dependencies: None
### Description: Develop a robust training script that handles the fine-tuning process for pre-trained models
### Details:
Create a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.
<info added on 2025-05-07T14:48:16.314Z>
Create a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.

This task can be worked on independently and in parallel with others. The training script implementation has no dependencies and is parallelizable (parallelizable: true).
</info added on 2025-05-07T14:48:16.314Z>

## 2. Checkpoint Management System [pending]
### Dependencies: 8.1
### Description: Implement a comprehensive checkpoint system to save and restore model states
### Details:
Design a checkpoint manager in `src/training/checkpointing.py` that: saves model weights at configurable intervals, stores optimizer states, implements versioning for checkpoints, provides functionality to resume training from any checkpoint, includes cleanup mechanisms for old checkpoints, and ensures compatibility across different hardware configurations. All checkpoint operations should be compatible with Google Colab or lightning.ai cloud environments. Checkpoints should be saved to the `checkpoints/` directory with appropriate naming conventions.

## 3. Early Stopping Mechanism [pending]
### Dependencies: 8.1, 8.2
### Description: Develop an early stopping system to prevent overfitting and optimize training time
### Details:
Implement a configurable early stopping mechanism that: monitors validation metrics (loss, accuracy), applies patience parameters to allow for fluctuations, saves best model states when improvements occur, provides restoration of best model after training, includes visualization of stopping point, and allows for custom stopping criteria definition. Ensure the implementation works reliably in cloud GPU environments like Google Colab or lightning.ai. The early stopping configuration should be defined in `configs/training/config.yaml` and the implementation should be integrated with the checkpoint system in `src/training/checkpointing.py`.

## 4. Hyperparameter Experimentation Framework [pending]
### Dependencies: 8.1, 8.2, 8.3
### Description: Create a framework for systematic hyperparameter tuning and experimentation
### Details:
Develop a hyperparameter experimentation system that: supports grid search and random search methods, enables parallel experiment execution, provides configuration management for experiments, implements parameter scheduling (learning rate decay), integrates with checkpoint system, and includes mechanisms to handle failed experiments gracefully. Design the framework to work efficiently in cloud GPU environments (Google Colab or lightning.ai) and to handle potential session timeouts or disconnections. Configuration files should be stored in the `configs/` directory with appropriate organization. Implement utilities in `src/training/utils.py` to support experiment management.

## 5. Results Tracking and Analysis System [pending]
### Dependencies: 8.4
### Description: Build a comprehensive system to track, visualize and compare experiment results
### Details:
Implement a results management system that: stores metrics for all experiments in `results/metrics/`, generates comparative visualizations using Python scripts, calculates statistical significance of improvements, exports results in standard formats, provides filtering and sorting capabilities, and integrates with external visualization tools if needed. Ensure all results are properly saved to persistent storage accessible after cloud GPU sessions end. Implement error analysis functionality in `src/deployment/visualization.py` and `src/deployment/report.py` to help understand model performance and limitations. Use the deployment scripts to generate HTML reports and visualizations that will be saved to `results/deployment/reports/` and `results/deployment/figures/` respectively.

## 6. Cloud Environment Setup Guide [pending]
### Dependencies: None
### Description: Create documentation for setting up the required cloud GPU environment
### Details:
Develop a comprehensive guide in `docs/training.md` for setting up the training environment on Google Colab or lightning.ai, including: step-by-step instructions for accessing GPU resources, installing Unsloth and other dependencies, configuring W&B integration, handling file storage and persistence, and troubleshooting common issues. Include examples of notebook configurations that work well for this specific fine-tuning task. Create additional documentation in `docs/model.md` for model architecture details and in `docs/results.md` for analyzing training results.

## 7. Deployment Scripts Implementation [pending]
### Dependencies: 8.5
### Description: Develop Python scripts for model deployment and performance analysis
### Details:
Create a suite of Python scripts in the `src/deployment/` directory to handle all aspects of model deployment and analysis:

1. `model_export.py`: Implement model export and conversion functionality with command-line interface
2. `api.py`: Create a FastAPI implementation for model serving
3. `monitoring.py`: Develop performance monitoring capabilities
4. `benchmark.py`: Implement load testing functionality
5. `visualization.py`: Create performance visualization tools
6. `report.py`: Develop HTML report generation

Ensure all scripts have proper command-line interfaces for flexibility and can be run independently. Implement proper Python packaging with clear separation of concerns. All output from these scripts should be saved to the appropriate directories under `results/deployment/`:
- Figures: `results/deployment/figures/`
- Reports: `results/deployment/reports/`
- Performance data: `results/deployment/data/`
- Exported models: `results/deployment/models/`

This approach will improve maintainability, version control, and integration with the rest of the codebase while maintaining all deployment capabilities.

## 8. Transfer Learning Evaluation System [pending]
### Dependencies: 8.1, 8.4
### Description: Implement a system to evaluate transfer learning effectiveness across different tasks
### Details:
Develop a transfer learning evaluation system in `src/training/transfer_metrics.py` and `src/models/transfer_eval.py` that: evaluates models trained on spelling tasks against position and count tasks, calculates transfer metrics and correlation scores, identifies which training patterns lead to better transfer, visualizes the relationship between spelling performance and transfer capabilities, and generates comprehensive reports on transfer learning effectiveness. The system should integrate with the existing experimentation framework and results tracking system. All transfer analysis results should be saved to `results/transfer_analysis/` directory.

## 9. Transfer Learning Documentation [pending]
### Dependencies: 8.8
### Description: Create comprehensive documentation on transfer learning analysis and findings
### Details:
Develop detailed documentation in `docs/transfer_learning.md` that explains: the transfer learning evaluation methodology, metrics used to assess transfer effectiveness, analysis of correlation between spelling performance and transfer capabilities, identification of optimal training patterns for transfer learning, visualization of key findings, and recommendations for maximizing transfer learning effectiveness. Include examples and case studies from the experiments to illustrate important concepts and findings.

