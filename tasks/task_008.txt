# Task ID: 8
# Title: Model Fine-tuning and Experimentation
# Status: pending
# Dependencies: 4, 6, 7
# Priority: high
# Description: Implement the training loop and run experiments with different hyperparameters to find the optimal configuration for effective transfer learning using Qwen3-4B model in cloud GPU environments.
# Details:
**IMPORTANT NOTE: This task requires a cloud GPU environment for Qwen3-4B fine-tuning. Do not attempt on local Mac.**

**CLARIFICATION: Training is performed on spelling data, but evaluation is strictly limited to character position and character count tasks. All evaluation metrics, scripts, and documentation must focus exclusively on position and count. Spelling is never used as an evaluation metric.**

1. Create a reusable training script that accepts hyperparameter configs (`src/training/train.py`)
2. Implement the training loop using Unsloth with Qwen3-4B on Google Colab or https://lightning.ai/lars/home
3. Set up checkpoint saving and loading system (`src/training/checkpointing.py`)
4. Implement early stopping based on validation metrics
5. Configure Qwen3-4B specific features:
   - Non-thinking mode only (enable_thinking=False) as per project policy
   - Sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)
   - English-only token subset handling during training
   - Adaptation to Qwen3's tokenizer patterns
6. Run experiments with different hyperparameters focused on transfer learning effectiveness:
   - LoRA rank (r): [4, 8, 16, 32]
   - LoRA alpha: [8, 16, 32, 64]
   - Learning rate: [1e-4, 2e-4, 5e-4, 1e-3]
   - Batch size: [4, 8, 16, 32]
   - Gradient accumulation steps: [1, 2, 4, 8]
   - Training steps: [500, 1000, 2000, 5000]
7. Track all experiments in W&B with position and character count metrics only
8. Implement evaluation metrics for non-thinking mode
9. Analyze transfer performance on position and character count tasks
10. Identify training patterns that lead to better transfer learning

**Transfer Learning Focus:**
- Primary training on spelling variation tasks
- Evaluate transfer learning effectiveness exclusively on position/count tasks
- Track performance on position and character count tasks only
- Identify which training approaches generalize better to position and count tasks
- Never use spelling as an evaluation metric

**File Structure:**
- Training Infrastructure:
  - Training script: `src/training/train.py`
  - Training utilities: `src/training/utils.py`
  - Data loaders: `src/training/data_loaders.py`
  - Model checkpointing: `src/training/checkpointing.py`
  - Transfer metrics: `src/training/transfer_metrics.py`
  - Qwen3 utilities: `src/training/qwen3_utils.py`

- Model Components:
  - Model architecture: `src/models/spelling_model.py`
  - Loss functions: `src/models/losses.py`
  - Metrics tracking: `src/models/metrics.py`
  - Model utilities: `src/models/utils.py`
  - Transfer evaluation: `src/models/transfer_eval.py`

- Deployment Components:
  - Model export: `src/deployment/model_export.py`
  - Lightning.AI Studio setup: `src/deployment/lightning_studio.py`
  - API implementation: `src/deployment/api.py`
  - Performance monitoring: `src/deployment/monitoring.py`
  - Load testing: `src/deployment/benchmark.py`
  - Performance visualization: `src/deployment/visualization.py`
  - Report generation: `src/deployment/report.py`
  - Transfer analysis: `src/deployment/transfer_analysis.py`
  - Auto-scaling config: `src/deployment/scaling_config.py`

- Configurations:
  - Training config: `configs/training/config.yaml`
  - Model config: `configs/models/model_config.yaml`
  - Optimizer config: `configs/training/optimizer.yaml`
  - Scheduler config: `configs/training/scheduler.yaml`
  - Transfer config: `configs/training/transfer_config.yaml`
  - Qwen3 config: `configs/models/qwen3_config.yaml`
  - Lightning.AI deployment config: `configs/deployment/lightning_config.yaml`

- Results and Checkpoints:
  - Model checkpoints: `checkpoints/`
  - Training logs: `results/training_logs/`
  - Performance metrics: `results/metrics/`
  - Transfer analysis: `results/transfer_analysis/`
  - Deployment results: `results/deployment/`
    - Figures: `results/deployment/figures/`
    - Reports: `results/deployment/reports/`
    - Data: `results/deployment/data/`
    - Models: `results/deployment/models/`

- Documentation:
  - Training guide: `docs/training.md`
  - Model architecture: `docs/model.md`
  - Results analysis: `docs/results.md`
  - Transfer learning analysis: `docs/transfer_learning.md`
  - Lightning.AI deployment guide: `docs/lightning_deployment.md`
  - Qwen3 specific guide: `docs/qwen3_guide.md`

Implementation:
```python
import os
import yaml
import torch
import wandb
import numpy as np
from unsloth import FastLanguageModel
from datasets import load_dataset
from transformers import TrainingArguments, Trainer, EarlyStoppingCallback
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Main experiment runner
def run_experiment(config_path):
    # Load config
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    
    # Initialize W&B
    run = wandb.init(
        project="llm-spelling-finetuning",
        name=config["experiment_name"],
        config=config,
        reinit=True
    )
    
    # Load model and tokenizer with Unsloth
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="Qwen/Qwen3-4B",
        max_seq_length=512,
        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        load_in_4bit=True
    )
    
    # Configure Qwen3-specific sampling parameters
    generation_config = model.generation_config
    generation_config.temperature = 0.6
    generation_config.top_p = 0.95
    generation_config.top_k = 20
    generation_config.min_p = 0.0
    
    # Add LoRA adapters
    model = FastLanguageModel.get_peft_model(
        model,
        r=config["lora_config"]["r"],
        target_modules=config["lora_config"]["target_modules"],
        lora_alpha=config["lora_config"]["alpha"],
        lora_dropout=config["lora_config"]["dropout"],
        bias="none",
        use_gradient_checkpointing=True,
        random_state=42
    )
    
    # Load datasets - both training and transfer task datasets
    spelling_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset")
    position_dataset = load_dataset("YOUR-USERNAME/llm-position-dataset")
    count_dataset = load_dataset("YOUR-USERNAME/llm-count-dataset")
    
    # Format dataset for instruction fine-tuning with Qwen3's format
    def formatting_func(examples):
        questions = examples["question"]
        answers = examples["answer"]
        
        # Format with standard mode (non-thinking mode only as per project policy)
        prompts = [f"<|im_start|>user\n{q}\n<|im_end|>\n<|im_start|>assistant\n" for q in questions]
        completions = [f"{a}<|im_end|>" for a in answers]
        
        return {"prompt": prompts, "completion": completions}
    
    # Apply formatting
    train_dataset = spelling_dataset["train"].map(formatting_func, batched=True)
    
    # Format transfer task datasets for evaluation
    position_eval_dataset = position_dataset["validation"].map(formatting_func, batched=True)
    count_eval_dataset = count_dataset["validation"].map(formatting_func, batched=True)
    
    # Set up output directory
    output_dir = f"./results/{config['experiment_name']}_{config['timestamp']}"
    os.makedirs(output_dir, exist_ok=True)
    
    # Create training arguments
    training_args = FastLanguageModel.get_train_args(
        output_dir=output_dir,
        per_device_train_batch_size=config["training_config"]["per_device_train_batch_size"],
        gradient_accumulation_steps=config["training_config"]["gradient_accumulation_steps"],
        warmup_steps=config["training_config"]["warmup_steps"],
        max_steps=config["training_config"]["max_steps"],
        learning_rate=config["training_config"]["learning_rate"],
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=200,
        load_best_model_at_end=True,
        metric_for_best_model="eval_position_accuracy",  # Use position task accuracy as primary metric
        greater_is_better=True,
        optim="adamw_torch",
        max_grad_norm=0.3,
        report_to="wandb"
    )
    
    # Define compute metrics function for transfer learning evaluation
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=2)
        
        # Only consider non-padding tokens
        mask = labels != -100
        labels = labels[mask]
        predictions = predictions[mask]
        
        accuracy = accuracy_score(labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
        
        metrics = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
        
        return metrics
    
    # Create trainer with early stopping
    trainer = FastLanguageModel.get_trainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=position_eval_dataset,  # Use position task as primary validation dataset
        data_collator=FastLanguageModel.get_data_collator(tokenizer),
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )
    
    # Train model
    trainer.train()
    
    # Save final model
    trainer.save_model(f"{output_dir}/final")
    
    # Evaluate on transfer task datasets
    position_eval_results = trainer.evaluate(eval_dataset=position_eval_dataset)
    count_eval_results = trainer.evaluate(eval_dataset=count_eval_dataset)
    
    # Calculate transfer metrics
    transfer_metrics = {
        "position_accuracy": position_eval_results["eval_accuracy"],
        "count_accuracy": count_eval_results["eval_accuracy"],
        "avg_transfer_score": (position_eval_results["eval_accuracy"] + count_eval_results["eval_accuracy"]) / 2,
        "position_to_count_ratio": position_eval_results["eval_accuracy"] / count_eval_results["eval_accuracy"] if count_eval_results["eval_accuracy"] > 0 else 0
    }
    
    # Log all results
    wandb.log({
        "position_eval_results": position_eval_results,
        "count_eval_results": count_eval_results,
        **transfer_metrics
    })
    
    # Save evaluation results
    all_results = {
        "position_eval_results": position_eval_results,
        "count_eval_results": count_eval_results,
        "transfer_metrics": transfer_metrics
    }
    
    with open(f"{output_dir}/eval_results.yaml", "w") as f:
        yaml.dump(all_results, f)
    
    # Close wandb run
    wandb.finish()
    
    return output_dir, all_results

# Run multiple experiments
def run_experiments(config_paths):
    results = {}
    for config_path in config_paths:
        print(f"Running experiment with config: {config_path}")
        output_dir, eval_results = run_experiment(config_path)
        
        # Extract config name
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        
        results[config["experiment_name"]] = {
            "output_dir": output_dir,
            "eval_results": eval_results
        }
    
    # Analyze transfer learning effectiveness across experiments
    analyze_transfer_effectiveness(results)
    
    # Save all results
    with open("experiment_results_summary.yaml", "w") as f:
        yaml.dump(results, f)
    
    return results

# Analyze transfer learning effectiveness
def analyze_transfer_effectiveness(results):
    """Analyze which training patterns lead to better transfer learning"""
    # Extract key metrics for analysis
    experiment_metrics = []
    for exp_name, exp_data in results.items():
        with open(f"{exp_data['output_dir']}/config.yaml", "r") as f:
            config = yaml.safe_load(f)
        
        metrics = {
            "experiment_name": exp_name,
            "lora_rank": config["lora_config"]["r"],
            "lora_alpha": config["lora_config"]["alpha"],
            "learning_rate": config["training_config"]["learning_rate"],
            "batch_size": config["training_config"]["per_device_train_batch_size"],
            "grad_accum_steps": config["training_config"]["gradient_accumulation_steps"],
            "training_steps": config["training_config"]["max_steps"],
            "position_accuracy": exp_data["eval_results"]["position_eval_results"]["eval_accuracy"],
            "count_accuracy": exp_data["eval_results"]["count_eval_results"]["eval_accuracy"],
            "avg_transfer_score": exp_data["eval_results"]["transfer_metrics"]["avg_transfer_score"]
        }
        experiment_metrics.append(metrics)
    
    # Calculate correlations between position and count tasks
    import pandas as pd
    df = pd.DataFrame(experiment_metrics)
    correlation = df[["position_accuracy", "count_accuracy", "avg_transfer_score"]].corr()
    
    # Identify top performing configurations for transfer learning
    df_sorted = df.sort_values(by="avg_transfer_score", ascending=False)
    top_configs = df_sorted.head(5)
    
    # Save analysis results
    os.makedirs("results/transfer_analysis", exist_ok=True)
    correlation.to_csv("results/transfer_analysis/metric_correlations.csv")
    top_configs.to_csv("results/transfer_analysis/top_transfer_configs.csv")
    
    # Generate visualization of transfer learning effectiveness
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 6))
    plt.scatter(df["position_accuracy"], df["count_accuracy"])
    plt.xlabel("Position Task Accuracy")
    plt.ylabel("Count Task Accuracy")
    plt.title("Correlation between Position and Count Task Performance")
    plt.savefig("results/transfer_analysis/position_vs_count.png")
    
    # Log findings to wandb
    wandb.init(project="llm-spelling-finetuning", name="transfer_analysis", reinit=True)
    wandb.log({
        "correlation_matrix": wandb.Table(dataframe=correlation),
        "top_transfer_configs": wandb.Table(dataframe=top_configs),
        "position_vs_count_plot": wandb.Image("results/transfer_analysis/position_vs_count.png")
    })
    wandb.finish()
```

# Test Strategy:
1. Verify training script (`src/training/train.py`) runs without errors on Google Colab or lightning.ai with Qwen3-4B
2. Confirm non-thinking mode is properly implemented and no thinking mode code exists in the codebase
3. Confirm experiments are properly tracked in W&B with position and character count metrics only
4. Verify Qwen3-specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) are correctly applied
5. Test English-only token subset handling during training
6. Check that checkpoints are saved correctly to `checkpoints/` directory and can be loaded properly
7. Verify early stopping works as expected
8. Test that the best model is loaded at the end of training
9. Compare performance across different hyperparameter configurations using Python scripts
10. Ensure all experiment results are properly saved to `results/metrics/` and `results/training_logs/`
11. Verify the environment is properly set up with GPU access before starting experiments
12. Validate that all configuration files in `configs/` directory are properly loaded and applied
13. Test transfer learning evaluation on position and count tasks only
14. Verify correlation analysis between position and count task performance
15. Test the transfer analysis visualization generation
16. Validate the identification of optimal training patterns for transfer learning
17. Test deployment scripts in `src/deployment/` directory with Qwen3-4B models:
    - Verify model export functionality in `model_export.py`
    - Test Lightning.AI Studio setup in `lightning_studio.py`
    - Test API implementation in `api.py`
    - Validate monitoring capabilities in `monitoring.py`
    - Check benchmark functionality in `benchmark.py`
    - Test visualization generation in `visualization.py`
    - Verify HTML report generation in `report.py`
    - Test transfer analysis in `transfer_analysis.py`
    - Validate auto-scaling configuration in `scaling_config.py`
18. Ensure all deployment and analysis results are correctly saved to the appropriate directories
19. Test Lightning.AI deployment with Qwen3-4B models:
    - Verify dedicated deployment Studio creation
    - Test environment isolation and dependency management
    - Validate auto-scaling and load balancing configuration
    - Check monitoring and logging setup
    - Test cost optimization mechanisms
    - Verify automated testing and validation pipeline
20. Verify that no thinking mode references exist in any documentation, code, or configuration files
21. Confirm that no spelling evaluation metrics or code exists in the implementation
22. Verify that all evaluation focuses exclusively on position and character count tasks
23. Run a comprehensive check to ensure no spelling evaluation metrics are tracked in W&B or any other logging system
24. Verify all documentation clearly states that training is on spelling data but evaluation is exclusively on position and count tasks

# Subtasks:
## 1. Training Script Implementation [pending]
### Dependencies: None
### Description: Develop a robust training script that handles the fine-tuning process for pre-trained models
### Details:
Create a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.
<info added on 2025-05-07T14:48:16.314Z>
Create a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.

This task can be worked on independently and in parallel with others. The training script implementation has no dependencies and is parallelizable (parallelizable: true).
</info added on 2025-05-07T14:48:16.314Z>

## 2. Checkpoint Management System [pending]
### Dependencies: 8.1
### Description: Implement a comprehensive checkpoint system to save and restore model states
### Details:
Design a checkpoint manager in `src/training/checkpointing.py` that: saves model weights at configurable intervals, stores optimizer states, implements versioning for checkpoints, provides functionality to resume training from any checkpoint, includes cleanup mechanisms for old checkpoints, and ensures compatibility across different hardware configurations. All checkpoint operations should be compatible with Google Colab or lightning.ai cloud environments. Checkpoints should be saved to the `checkpoints/` directory with appropriate naming conventions.

## 3. Early Stopping Mechanism [pending]
### Dependencies: 8.1, 8.2
### Description: Develop an early stopping system to prevent overfitting and optimize training time
### Details:
Implement a configurable early stopping mechanism that: monitors validation metrics (loss, accuracy), applies patience parameters to allow for fluctuations, saves best model states when improvements occur, provides restoration of best model after training, includes visualization of stopping point, and allows for custom stopping criteria definition. Ensure the implementation works reliably in cloud GPU environments like Google Colab or lightning.ai. The early stopping configuration should be defined in `configs/training/config.yaml` and the implementation should be integrated with the checkpoint system in `src/training/checkpointing.py`.

## 4. Hyperparameter Experimentation Framework [pending]
### Dependencies: 8.1, 8.2, 8.3
### Description: Create a framework for systematic hyperparameter tuning and experimentation
### Details:
Develop a hyperparameter experimentation system that: supports grid search and random search methods, enables parallel experiment execution, provides configuration management for experiments, implements parameter scheduling (learning rate decay), integrates with checkpoint system, and includes mechanisms to handle failed experiments gracefully. Design the framework to work efficiently in cloud GPU environments (Google Colab or lightning.ai) and to handle potential session timeouts or disconnections. Configuration files should be stored in the `configs/` directory with appropriate organization. Implement utilities in `src/training/utils.py` to support experiment management.

## 5. Results Tracking and Analysis System [pending]
### Dependencies: 8.4
### Description: Build a comprehensive system to track, visualize and compare experiment results
### Details:
Implement a results management system that: stores metrics for all experiments in `results/metrics/`, generates comparative visualizations using Python scripts, calculates statistical significance of improvements, exports results in standard formats, provides filtering and sorting capabilities, and integrates with external visualization tools if needed. Ensure all results are properly saved to persistent storage accessible after cloud GPU sessions end. Implement error analysis functionality in `src/deployment/visualization.py` and `src/deployment/report.py` to help understand model performance and limitations. Use the deployment scripts to generate HTML reports and visualizations that will be saved to `results/deployment/reports/` and `results/deployment/figures/` respectively. Focus exclusively on position and character count task metrics in all visualizations and reports. Ensure no spelling evaluation metrics are tracked or displayed in any reports or visualizations.

## 6. Cloud Environment Setup Guide [pending]
### Dependencies: None
### Description: Create documentation for setting up the required cloud GPU environment
### Details:
Develop a comprehensive guide in `docs/training.md` for setting up the training environment on Google Colab or lightning.ai, including: step-by-step instructions for accessing GPU resources, installing Unsloth and other dependencies, configuring W&B integration, handling file storage and persistence, and troubleshooting common issues. Include examples of notebook configurations that work well for this specific fine-tuning task. Create additional documentation in `docs/model.md` for model architecture details and in `docs/results.md` for analyzing training results. Clearly document that while training is performed on spelling data, all evaluation is strictly limited to position and count tasks.

## 7. Deployment Scripts Implementation [pending]
### Dependencies: 8.5
### Description: Develop Python scripts for model deployment and performance analysis
### Details:
Create a suite of Python scripts in the `src/deployment/` directory to handle all aspects of model deployment and analysis using Lightning.AI Studios:

1. `model_export.py`: Implement model export and conversion functionality with command-line interface
2. `lightning_studio.py`: Create dedicated deployment Studio setup following the "one Studio, one task" principle
3. `api.py`: Create an API implementation using Lightning.AI's serving engine
4. `monitoring.py`: Develop performance monitoring and logging capabilities
5. `benchmark.py`: Implement load testing functionality
6. `visualization.py`: Create performance visualization tools
7. `report.py`: Develop HTML report generation
8. `scaling_config.py`: Configure auto-scaling and load balancing for the deployment

Ensure all scripts have proper command-line interfaces for flexibility and can be run independently. Implement proper Python packaging with clear separation of concerns. Configure environment isolation and dependency management for the Lightning.AI Studio. Implement cost optimization through efficient resource usage. Set up automated testing and validation in the deployment pipeline.

All output from these scripts should be saved to the appropriate directories under `results/deployment/`:
- Figures: `results/deployment/figures/`
- Reports: `results/deployment/reports/`
- Performance data: `results/deployment/data/`
- Exported models: `results/deployment/models/`

Create comprehensive deployment documentation in `docs/lightning_deployment.md` including Lightning.AI-specific setup instructions. Ensure all monitoring, visualization, and reporting focus exclusively on position and count task metrics, with no spelling evaluation metrics included.

## 8. Transfer Learning Evaluation System [pending]
### Dependencies: 8.1, 8.4
### Description: Implement a system to evaluate transfer learning effectiveness across different tasks
### Details:
Develop a transfer learning evaluation system in `src/training/transfer_metrics.py` and `src/models/transfer_eval.py` that: evaluates models trained on spelling tasks against position and count tasks only, calculates transfer metrics between position and count tasks, identifies which training patterns lead to better transfer, visualizes the relationship between position and count task performance, and generates comprehensive reports on transfer learning effectiveness. The system should integrate with the existing experimentation framework and results tracking system. All transfer analysis results should be saved to `results/transfer_analysis/` directory. Ensure the system focuses exclusively on position and character count tasks as per project policy. Implement strict validation to ensure no spelling evaluation metrics are tracked or reported.

## 9. Transfer Learning Documentation [pending]
### Dependencies: 8.8
### Description: Create comprehensive documentation on transfer learning analysis and findings
### Details:
Develop detailed documentation in `docs/transfer_learning.md` that explains: the transfer learning evaluation methodology, metrics used to assess transfer effectiveness between position and count tasks, analysis of correlation between position and count task performance, identification of optimal training patterns for transfer learning, visualization of key findings, and recommendations for maximizing transfer learning effectiveness. Include examples and case studies from the experiments to illustrate important concepts and findings. Ensure all documentation focuses exclusively on position and character count tasks as per project policy. Clearly document that while training is performed on spelling data, all evaluation is strictly limited to position and count tasks, and spelling is never used as an evaluation metric.

## 12. Qwen3-4B Non-Thinking Mode Implementation [pending]
### Dependencies: 8.1
### Description: Implement support for Qwen3-4B's non-thinking mode only
### Details:
Create utilities in `src/training/qwen3_utils.py` to support Qwen3-specific features including handling of the English-only token subset during training and adapting to Qwen3's tokenizer patterns. Ensure all implementations strictly follow the project policy of using non-thinking mode only (enable_thinking=False). Create configuration options in `configs/models/qwen3_config.yaml` to control Qwen3-specific parameters. Document the implementation details and usage guidelines in `docs/qwen3_guide.md`, emphasizing that only non-thinking mode is supported.

## 13. Qwen3-4B Sampling Parameters Configuration [pending]
### Dependencies: 8.1, 8.12
### Description: Implement configuration for Qwen3-4B's specific sampling parameters
### Details:
Create a configuration system for Qwen3-4B's sampling parameters in `configs/models/qwen3_config.yaml` that includes Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 as default values. Implement utilities in `src/training/qwen3_utils.py` to apply these parameters during model initialization and inference. Add functionality to experiment with different sampling parameter combinations and analyze their impact on model performance. Document the sampling parameters and their effects in `docs/qwen3_guide.md`.

## 14. Code Review for Thinking Mode References [pending]
### Dependencies: 8.1, 8.12, 8.13
### Description: Review all code to ensure no thinking mode references remain
### Details:
Perform a comprehensive review of all code, configuration files, and documentation to ensure that no references to thinking mode remain. This includes checking for any code that might enable thinking mode, any configuration options related to thinking mode, and any documentation that mentions thinking mode. Create a verification script that can scan the codebase for thinking mode references and report any findings. Document the verification process and results to confirm compliance with the project policy of using non-thinking mode only.

## 15. Remove Spelling Evaluation References [pending]
### Dependencies: 8.1, 8.5, 8.8
### Description: Ensure all evaluation focuses exclusively on position and character count tasks
### Details:
Perform a comprehensive review of all code, configuration files, and documentation to ensure that no references to spelling evaluation remain. This includes removing any code that evaluates spelling performance, any configuration options related to spelling metrics, and any documentation that mentions spelling evaluation. Create a verification script that can scan the codebase for spelling evaluation references and report any findings. Update all evaluation scripts, metrics tracking, and visualization code to focus exclusively on position and character count tasks as per project policy. Ensure that W&B logging, reports, and all analysis focus only on position and count task metrics.

## 16. Training vs. Evaluation Documentation [pending]
### Dependencies: 8.6, 8.9
### Description: Clearly document the distinction between training and evaluation tasks
### Details:
Create clear documentation that explicitly states the distinction between training data (spelling tasks) and evaluation metrics (position and count tasks only). Update all existing documentation to reflect this distinction. Add prominent notes in all relevant files including README.md, training guides, evaluation scripts, and configuration files. Create a dedicated section in the documentation that explains the transfer learning approach: training on spelling tasks but evaluating exclusively on position and count tasks. Ensure this distinction is clear to all users of the codebase.

## 17. Metrics Validation System [pending]
### Dependencies: 8.5, 8.8, 8.15
### Description: Implement a validation system to ensure no spelling metrics are tracked
### Details:
Develop a validation system that checks all metrics being tracked and reported to ensure they are exclusively related to position and count tasks. Implement this as a pre-commit hook and as part of the CI/CD pipeline. Create a whitelist of allowed metrics and validate that only these metrics are being tracked in W&B, saved to result files, or displayed in visualizations. Add assertions in the code to prevent accidental tracking of spelling metrics. Create a comprehensive test suite that verifies no spelling metrics are being tracked or reported.

