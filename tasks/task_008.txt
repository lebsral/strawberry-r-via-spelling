# Task ID: 8
# Title: Model Fine-tuning and Experimentation
# Status: pending
# Dependencies: 4, 6, 7
# Priority: high
# Description: Implement the training loop and run experiments with different hyperparameters to find the optimal configuration for effective transfer learning using Qwen3-4B model in cloud GPU environments.
# Details:
**IMPORTANT NOTE: This task requires a cloud GPU environment for Qwen3-4B fine-tuning. Do not attempt on local Mac.**

1. Create a reusable training script that accepts hyperparameter configs (`src/training/train.py`)
2. Implement the training loop using Unsloth with Qwen3-4B on Google Colab or https://lightning.ai/lars/home
3. Set up checkpoint saving and loading system (`src/training/checkpointing.py`)
4. Implement early stopping based on validation metrics
5. Configure Qwen3-4B specific features:
   - Support for thinking/non-thinking modes
   - Sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0)
   - English-only token subset handling during training
   - Adaptation to Qwen3's tokenizer patterns
6. Run experiments with different hyperparameters focused on transfer learning effectiveness:
   - LoRA rank (r): [4, 8, 16, 32]
   - LoRA alpha: [8, 16, 32, 64]
   - Learning rate: [1e-4, 2e-4, 5e-4, 1e-3]
   - Batch size: [4, 8, 16, 32]
   - Gradient accumulation steps: [1, 2, 4, 8]
   - Training steps: [500, 1000, 2000, 5000]
   - Thinking mode: [enabled, disabled]
7. Track all experiments in W&B with both spelling metrics and transfer metrics
8. Implement evaluation metrics suitable for both thinking and non-thinking modes
9. Analyze correlation between spelling improvement and transfer performance
10. Identify training patterns that lead to better transfer learning

**Transfer Learning Focus:**
- Primary training on spelling variation tasks
- Monitor transfer learning effectiveness to position/count tasks
- Track correlation between spelling performance and transfer capabilities
- Identify which training approaches generalize better across task types
- Compare performance between thinking and non-thinking modes

**File Structure:**
- Training Infrastructure:
  - Training script: `src/training/train.py`
  - Training utilities: `src/training/utils.py`
  - Data loaders: `src/training/data_loaders.py`
  - Model checkpointing: `src/training/checkpointing.py`
  - Transfer metrics: `src/training/transfer_metrics.py`
  - Qwen3 utilities: `src/training/qwen3_utils.py`

- Model Components:
  - Model architecture: `src/models/spelling_model.py`
  - Loss functions: `src/models/losses.py`
  - Metrics tracking: `src/models/metrics.py`
  - Model utilities: `src/models/utils.py`
  - Transfer evaluation: `src/models/transfer_eval.py`
  - Thinking mode handler: `src/models/thinking_mode.py`

- Deployment Components:
  - Model export: `src/deployment/model_export.py`
  - Lightning.AI Studio setup: `src/deployment/lightning_studio.py`
  - API implementation: `src/deployment/api.py`
  - Performance monitoring: `src/deployment/monitoring.py`
  - Load testing: `src/deployment/benchmark.py`
  - Performance visualization: `src/deployment/visualization.py`
  - Report generation: `src/deployment/report.py`
  - Transfer analysis: `src/deployment/transfer_analysis.py`
  - Auto-scaling config: `src/deployment/scaling_config.py`

- Configurations:
  - Training config: `configs/training/config.yaml`
  - Model config: `configs/models/model_config.yaml`
  - Optimizer config: `configs/training/optimizer.yaml`
  - Scheduler config: `configs/training/scheduler.yaml`
  - Transfer config: `configs/training/transfer_config.yaml`
  - Qwen3 config: `configs/models/qwen3_config.yaml`
  - Lightning.AI deployment config: `configs/deployment/lightning_config.yaml`

- Results and Checkpoints:
  - Model checkpoints: `checkpoints/`
  - Training logs: `results/training_logs/`
  - Performance metrics: `results/metrics/`
  - Error analysis: `results/error_analysis/`
  - Transfer analysis: `results/transfer_analysis/`
  - Deployment results: `results/deployment/`
    - Figures: `results/deployment/figures/`
    - Reports: `results/deployment/reports/`
    - Data: `results/deployment/data/`
    - Models: `results/deployment/models/`

- Documentation:
  - Training guide: `docs/training.md`
  - Model architecture: `docs/model.md`
  - Results analysis: `docs/results.md`
  - Transfer learning analysis: `docs/transfer_learning.md`
  - Lightning.AI deployment guide: `docs/lightning_deployment.md`
  - Qwen3 specific guide: `docs/qwen3_guide.md`

Implementation:
```python
import os
import yaml
import torch
import wandb
import numpy as np
from unsloth import FastLanguageModel
from datasets import load_dataset
from transformers import TrainingArguments, Trainer, EarlyStoppingCallback
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Main experiment runner
def run_experiment(config_path):
    # Load config
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    
    # Initialize W&B
    run = wandb.init(
        project="llm-spelling-finetuning",
        name=config["experiment_name"],
        config=config,
        reinit=True
    )
    
    # Load model and tokenizer with Unsloth
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="Qwen/Qwen3-4B",
        max_seq_length=512,
        dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        load_in_4bit=True
    )
    
    # Configure Qwen3-specific sampling parameters
    generation_config = model.generation_config
    generation_config.temperature = 0.6
    generation_config.top_p = 0.95
    generation_config.top_k = 20
    generation_config.min_p = 0.0
    
    # Add LoRA adapters
    model = FastLanguageModel.get_peft_model(
        model,
        r=config["lora_config"]["r"],
        target_modules=config["lora_config"]["target_modules"],
        lora_alpha=config["lora_config"]["alpha"],
        lora_dropout=config["lora_config"]["dropout"],
        bias="none",
        use_gradient_checkpointing=True,
        random_state=42
    )
    
    # Load datasets - both spelling and transfer task datasets
    spelling_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset")
    position_dataset = load_dataset("YOUR-USERNAME/llm-position-dataset")
    count_dataset = load_dataset("YOUR-USERNAME/llm-count-dataset")
    
    # Format dataset for instruction fine-tuning with Qwen3's format
    def formatting_func(examples, thinking_mode=False):
        questions = examples["question"]
        answers = examples["answer"]
        
        if thinking_mode:
            # Format with thinking mode enabled
            prompts = [f"<|im_start|>user\n{q}\n<|im_end|>\n<|im_start|>assistant\n<|thinking|>" for q in questions]
            completions = [f"Let me solve this step by step...\n{a}\n<|endthinking|>\n{a}<|im_end|>" for a in answers]
        else:
            # Format with standard mode (no thinking)
            prompts = [f"<|im_start|>user\n{q}\n<|im_end|>\n<|im_start|>assistant\n" for q in questions]
            completions = [f"{a}<|im_end|>" for a in answers]
        
        return {"prompt": prompts, "completion": completions}
    
    # Apply formatting based on thinking mode configuration
    thinking_mode = config.get("thinking_mode", False)
    train_dataset = spelling_dataset["train"].map(lambda x: formatting_func(x, thinking_mode), batched=True)
    eval_dataset = spelling_dataset["validation"].map(lambda x: formatting_func(x, thinking_mode), batched=True)
    
    # Format transfer task datasets for evaluation
    position_eval_dataset = position_dataset["validation"].map(lambda x: formatting_func(x, thinking_mode), batched=True)
    count_eval_dataset = count_dataset["validation"].map(lambda x: formatting_func(x, thinking_mode), batched=True)
    
    # Set up output directory
    output_dir = f"./results/{config['experiment_name']}_{config['timestamp']}"
    os.makedirs(output_dir, exist_ok=True)
    
    # Create training arguments
    training_args = FastLanguageModel.get_train_args(
        output_dir=output_dir,
        per_device_train_batch_size=config["training_config"]["per_device_train_batch_size"],
        gradient_accumulation_steps=config["training_config"]["gradient_accumulation_steps"],
        warmup_steps=config["training_config"]["warmup_steps"],
        max_steps=config["training_config"]["max_steps"],
        learning_rate=config["training_config"]["learning_rate"],
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=200,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        optim="adamw_torch",
        max_grad_norm=0.3,
        report_to="wandb"
    )
    
    # Define compute metrics function for transfer learning evaluation
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=2)
        
        # Only consider non-padding tokens
        mask = labels != -100
        labels = labels[mask]
        predictions = predictions[mask]
        
        accuracy = accuracy_score(labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
        
        # Add thinking mode specific metrics if applicable
        metrics = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
        
        if thinking_mode:
            # Add thinking-specific metrics
            # This would analyze the quality of the thinking process
            # Implementation depends on specific requirements
            pass
        
        return metrics
    
    # Create trainer with early stopping
    trainer = FastLanguageModel.get_trainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=FastLanguageModel.get_data_collator(tokenizer),
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )
    
    # Train model
    trainer.train()
    
    # Save final model
    trainer.save_model(f"{output_dir}/final")
    
    # Evaluate on spelling validation set
    spelling_eval_results = trainer.evaluate()
    
    # Evaluate on transfer task datasets
    position_eval_results = trainer.evaluate(eval_dataset=position_eval_dataset)
    count_eval_results = trainer.evaluate(eval_dataset=count_eval_dataset)
    
    # Calculate transfer metrics
    transfer_metrics = {
        "position_transfer_score": position_eval_results["eval_accuracy"],
        "count_transfer_score": count_eval_results["eval_accuracy"],
        "avg_transfer_score": (position_eval_results["eval_accuracy"] + count_eval_results["eval_accuracy"]) / 2,
        "spelling_to_position_ratio": spelling_eval_results["eval_accuracy"] / position_eval_results["eval_accuracy"] if position_eval_results["eval_accuracy"] > 0 else 0,
        "spelling_to_count_ratio": spelling_eval_results["eval_accuracy"] / count_eval_results["eval_accuracy"] if count_eval_results["eval_accuracy"] > 0 else 0
    }
    
    # Log all results
    wandb.log({
        **spelling_eval_results,
        "position_eval_results": position_eval_results,
        "count_eval_results": count_eval_results,
        **transfer_metrics,
        "thinking_mode": thinking_mode
    })
    
    # Save evaluation results
    all_results = {
        "spelling_eval_results": spelling_eval_results,
        "position_eval_results": position_eval_results,
        "count_eval_results": count_eval_results,
        "transfer_metrics": transfer_metrics,
        "thinking_mode": thinking_mode
    }
    
    with open(f"{output_dir}/eval_results.yaml", "w") as f:
        yaml.dump(all_results, f)
    
    # Close wandb run
    wandb.finish()
    
    return output_dir, all_results

# Run multiple experiments
def run_experiments(config_paths):
    results = {}
    for config_path in config_paths:
        print(f"Running experiment with config: {config_path}")
        output_dir, eval_results = run_experiment(config_path)
        
        # Extract config name
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        
        results[config["experiment_name"]] = {
            "output_dir": output_dir,
            "eval_results": eval_results
        }
    
    # Analyze transfer learning effectiveness across experiments
    analyze_transfer_effectiveness(results)
    
    # Save all results
    with open("experiment_results_summary.yaml", "w") as f:
        yaml.dump(results, f)
    
    return results

# Analyze transfer learning effectiveness
def analyze_transfer_effectiveness(results):
    """Analyze which training patterns lead to better transfer learning"""
    # Extract key metrics for analysis
    experiment_metrics = []
    for exp_name, exp_data in results.items():
        with open(f"{exp_data['output_dir']}/config.yaml", "r") as f:
            config = yaml.safe_load(f)
        
        metrics = {
            "experiment_name": exp_name,
            "lora_rank": config["lora_config"]["r"],
            "lora_alpha": config["lora_config"]["alpha"],
            "learning_rate": config["training_config"]["learning_rate"],
            "batch_size": config["training_config"]["per_device_train_batch_size"],
            "grad_accum_steps": config["training_config"]["gradient_accumulation_steps"],
            "training_steps": config["training_config"]["max_steps"],
            "thinking_mode": config.get("thinking_mode", False),
            "spelling_accuracy": exp_data["eval_results"]["spelling_eval_results"]["eval_accuracy"],
            "position_accuracy": exp_data["eval_results"]["position_eval_results"]["eval_accuracy"],
            "count_accuracy": exp_data["eval_results"]["count_eval_results"]["eval_accuracy"],
            "avg_transfer_score": exp_data["eval_results"]["transfer_metrics"]["avg_transfer_score"]
        }
        experiment_metrics.append(metrics)
    
    # Calculate correlations between spelling performance and transfer tasks
    import pandas as pd
    df = pd.DataFrame(experiment_metrics)
    correlation = df[["spelling_accuracy", "position_accuracy", "count_accuracy", "avg_transfer_score"]].corr()
    
    # Analyze impact of thinking mode on transfer learning
    thinking_vs_standard = df.groupby("thinking_mode")[["spelling_accuracy", "position_accuracy", 
                                                      "count_accuracy", "avg_transfer_score"]].mean()
    
    # Identify top performing configurations for transfer learning
    df_sorted = df.sort_values(by="avg_transfer_score", ascending=False)
    top_configs = df_sorted.head(5)
    
    # Save analysis results
    os.makedirs("results/transfer_analysis", exist_ok=True)
    correlation.to_csv("results/transfer_analysis/metric_correlations.csv")
    thinking_vs_standard.to_csv("results/transfer_analysis/thinking_mode_impact.csv")
    top_configs.to_csv("results/transfer_analysis/top_transfer_configs.csv")
    
    # Generate visualization of transfer learning effectiveness
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 6))
    plt.scatter(df["spelling_accuracy"], df["avg_transfer_score"], 
                c=df["thinking_mode"].map({True: 'red', False: 'blue'}))
    plt.xlabel("Spelling Task Accuracy")
    plt.ylabel("Average Transfer Task Accuracy")
    plt.title("Correlation between Spelling Performance and Transfer Learning")
    plt.legend(["Thinking Mode", "Standard Mode"])
    plt.savefig("results/transfer_analysis/spelling_vs_transfer.png")
    
    # Log findings to wandb
    wandb.init(project="llm-spelling-finetuning", name="transfer_analysis", reinit=True)
    wandb.log({
        "correlation_matrix": wandb.Table(dataframe=correlation),
        "thinking_vs_standard": wandb.Table(dataframe=thinking_vs_standard),
        "top_transfer_configs": wandb.Table(dataframe=top_configs),
        "spelling_vs_transfer_plot": wandb.Image("results/transfer_analysis/spelling_vs_transfer.png")
    })
    wandb.finish()
```

# Test Strategy:
1. Verify training script (`src/training/train.py`) runs without errors on Google Colab or lightning.ai with Qwen3-4B
2. Test both thinking and non-thinking modes to ensure proper formatting and processing
3. Confirm experiments are properly tracked in W&B with both spelling and transfer metrics
4. Verify Qwen3-specific sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) are correctly applied
5. Test English-only token subset handling during training
6. Check that checkpoints are saved correctly to `checkpoints/` directory and can be loaded properly
7. Verify early stopping works as expected
8. Test that the best model is loaded at the end of training
9. Compare performance across different hyperparameter configurations using Python scripts
10. Specifically test the impact of thinking mode vs. non-thinking mode on performance
11. Ensure all experiment results are properly saved to `results/metrics/` and `results/training_logs/`
12. Verify the environment is properly set up with GPU access before starting experiments
13. Validate that all configuration files in `configs/` directory are properly loaded and applied
14. Test transfer learning evaluation on position and count tasks with both thinking and non-thinking modes
15. Verify correlation analysis between spelling performance and transfer capabilities
16. Test the transfer analysis visualization generation, including thinking mode comparisons
17. Validate the identification of optimal training patterns for transfer learning
18. Test deployment scripts in `src/deployment/` directory with Qwen3-4B models:
    - Verify model export functionality in `model_export.py`
    - Test Lightning.AI Studio setup in `lightning_studio.py`
    - Test API implementation in `api.py`
    - Validate monitoring capabilities in `monitoring.py`
    - Check benchmark functionality in `benchmark.py`
    - Test visualization generation in `visualization.py`
    - Verify HTML report generation in `report.py`
    - Test transfer analysis in `transfer_analysis.py`
    - Validate auto-scaling configuration in `scaling_config.py`
19. Ensure all deployment and analysis results are correctly saved to the appropriate directories
20. Test Lightning.AI deployment with Qwen3-4B models:
    - Verify dedicated deployment Studio creation
    - Test environment isolation and dependency management
    - Validate auto-scaling and load balancing configuration
    - Check monitoring and logging setup
    - Test cost optimization mechanisms
    - Verify automated testing and validation pipeline

# Subtasks:
## 1. Training Script Implementation [pending]
### Dependencies: None
### Description: Develop a robust training script that handles the fine-tuning process for pre-trained models
### Details:
Create a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.
<info added on 2025-05-07T14:48:16.314Z>
Create a modular training script that includes: data loading pipeline, model initialization with pre-trained weights, loss function definition, optimization algorithm setup (SGD/Adam), training loop with batch processing, validation steps, and proper error handling. Implement logging for training metrics and ensure GPU/CPU compatibility.

This task can be worked on independently and in parallel with others. The training script implementation has no dependencies and is parallelizable (parallelizable: true).
</info added on 2025-05-07T14:48:16.314Z>

## 2. Checkpoint Management System [pending]
### Dependencies: 8.1
### Description: Implement a comprehensive checkpoint system to save and restore model states
### Details:
Design a checkpoint manager in `src/training/checkpointing.py` that: saves model weights at configurable intervals, stores optimizer states, implements versioning for checkpoints, provides functionality to resume training from any checkpoint, includes cleanup mechanisms for old checkpoints, and ensures compatibility across different hardware configurations. All checkpoint operations should be compatible with Google Colab or lightning.ai cloud environments. Checkpoints should be saved to the `checkpoints/` directory with appropriate naming conventions.

## 3. Early Stopping Mechanism [pending]
### Dependencies: 8.1, 8.2
### Description: Develop an early stopping system to prevent overfitting and optimize training time
### Details:
Implement a configurable early stopping mechanism that: monitors validation metrics (loss, accuracy), applies patience parameters to allow for fluctuations, saves best model states when improvements occur, provides restoration of best model after training, includes visualization of stopping point, and allows for custom stopping criteria definition. Ensure the implementation works reliably in cloud GPU environments like Google Colab or lightning.ai. The early stopping configuration should be defined in `configs/training/config.yaml` and the implementation should be integrated with the checkpoint system in `src/training/checkpointing.py`.

## 4. Hyperparameter Experimentation Framework [pending]
### Dependencies: 8.1, 8.2, 8.3
### Description: Create a framework for systematic hyperparameter tuning and experimentation
### Details:
Develop a hyperparameter experimentation system that: supports grid search and random search methods, enables parallel experiment execution, provides configuration management for experiments, implements parameter scheduling (learning rate decay), integrates with checkpoint system, and includes mechanisms to handle failed experiments gracefully. Design the framework to work efficiently in cloud GPU environments (Google Colab or lightning.ai) and to handle potential session timeouts or disconnections. Configuration files should be stored in the `configs/` directory with appropriate organization. Implement utilities in `src/training/utils.py` to support experiment management.

## 5. Results Tracking and Analysis System [pending]
### Dependencies: 8.4
### Description: Build a comprehensive system to track, visualize and compare experiment results
### Details:
Implement a results management system that: stores metrics for all experiments in `results/metrics/`, generates comparative visualizations using Python scripts, calculates statistical significance of improvements, exports results in standard formats, provides filtering and sorting capabilities, and integrates with external visualization tools if needed. Ensure all results are properly saved to persistent storage accessible after cloud GPU sessions end. Implement error analysis functionality in `src/deployment/visualization.py` and `src/deployment/report.py` to help understand model performance and limitations. Use the deployment scripts to generate HTML reports and visualizations that will be saved to `results/deployment/reports/` and `results/deployment/figures/` respectively.

## 6. Cloud Environment Setup Guide [pending]
### Dependencies: None
### Description: Create documentation for setting up the required cloud GPU environment
### Details:
Develop a comprehensive guide in `docs/training.md` for setting up the training environment on Google Colab or lightning.ai, including: step-by-step instructions for accessing GPU resources, installing Unsloth and other dependencies, configuring W&B integration, handling file storage and persistence, and troubleshooting common issues. Include examples of notebook configurations that work well for this specific fine-tuning task. Create additional documentation in `docs/model.md` for model architecture details and in `docs/results.md` for analyzing training results.

## 7. Deployment Scripts Implementation [pending]
### Dependencies: 8.5
### Description: Develop Python scripts for model deployment and performance analysis
### Details:
Create a suite of Python scripts in the `src/deployment/` directory to handle all aspects of model deployment and analysis using Lightning.AI Studios:

1. `model_export.py`: Implement model export and conversion functionality with command-line interface
2. `lightning_studio.py`: Create dedicated deployment Studio setup following the "one Studio, one task" principle
3. `api.py`: Create an API implementation using Lightning.AI's serving engine
4. `monitoring.py`: Develop performance monitoring and logging capabilities
5. `benchmark.py`: Implement load testing functionality
6. `visualization.py`: Create performance visualization tools
7. `report.py`: Develop HTML report generation
8. `scaling_config.py`: Configure auto-scaling and load balancing for the deployment

Ensure all scripts have proper command-line interfaces for flexibility and can be run independently. Implement proper Python packaging with clear separation of concerns. Configure environment isolation and dependency management for the Lightning.AI Studio. Implement cost optimization through efficient resource usage. Set up automated testing and validation in the deployment pipeline.

All output from these scripts should be saved to the appropriate directories under `results/deployment/`:
- Figures: `results/deployment/figures/`
- Reports: `results/deployment/reports/`
- Performance data: `results/deployment/data/`
- Exported models: `results/deployment/models/`

Create comprehensive deployment documentation in `docs/lightning_deployment.md` including Lightning.AI-specific setup instructions.

## 8. Transfer Learning Evaluation System [pending]
### Dependencies: 8.1, 8.4
### Description: Implement a system to evaluate transfer learning effectiveness across different tasks
### Details:
Develop a transfer learning evaluation system in `src/training/transfer_metrics.py` and `src/models/transfer_eval.py` that: evaluates models trained on spelling tasks against position and count tasks, calculates transfer metrics and correlation scores, identifies which training patterns lead to better transfer, visualizes the relationship between spelling performance and transfer capabilities, and generates comprehensive reports on transfer learning effectiveness. The system should integrate with the existing experimentation framework and results tracking system. All transfer analysis results should be saved to `results/transfer_analysis/` directory.

## 9. Transfer Learning Documentation [pending]
### Dependencies: 8.8
### Description: Create comprehensive documentation on transfer learning analysis and findings
### Details:
Develop detailed documentation in `docs/transfer_learning.md` that explains: the transfer learning evaluation methodology, metrics used to assess transfer effectiveness, analysis of correlation between spelling performance and transfer capabilities, identification of optimal training patterns for transfer learning, visualization of key findings, and recommendations for maximizing transfer learning effectiveness. Include examples and case studies from the experiments to illustrate important concepts and findings.

## 10. Qwen3-4B Thinking Mode Implementation [pending]
### Dependencies: 8.1
### Description: Implement support for Qwen3-4B's thinking and non-thinking modes
### Details:
Create a dedicated module in `src/models/thinking_mode.py` that handles Qwen3-4B's thinking and non-thinking modes. Implement functionality to format prompts correctly for each mode, process model outputs appropriately, and evaluate performance differences. Develop utilities in `src/training/qwen3_utils.py` to support Qwen3-specific features including handling of the English-only token subset during training and adapting to Qwen3's tokenizer patterns. Create configuration options in `configs/models/qwen3_config.yaml` to control thinking mode behavior and other Qwen3-specific parameters. Document the implementation details and usage guidelines in `docs/qwen3_guide.md`.

## 11. Qwen3-4B Sampling Parameters Configuration [pending]
### Dependencies: 8.1, 8.10
### Description: Implement configuration for Qwen3-4B's specific sampling parameters
### Details:
Create a configuration system for Qwen3-4B's sampling parameters in `configs/models/qwen3_config.yaml` that includes Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 as default values. Implement utilities in `src/training/qwen3_utils.py` to apply these parameters during model initialization and inference. Add functionality to experiment with different sampling parameter combinations and analyze their impact on model performance. Document the sampling parameters and their effects in `docs/qwen3_guide.md`.

