# Task ID: 5
# Title: Baseline Model Evaluation
# Status: pending
# Dependencies: 3
# Priority: medium
# Description: Evaluate the Qwen3-4B model on letter count and position tasks to establish a performance baseline for transfer learning effectiveness, leveraging Lightning.AI Studios for efficient computation and experiment tracking.
# Details:
1. Set up DsPy for multi-shot prompting
2. Create Python scripts to evaluate the Qwen3-4B model
3. Test ONLY on position and character count metrics using multi-token words
4. Ensure output format is correct (single integer for character count or single letter for character position)
5. Document the baseline performance for comparison

Qwen3-4B Configuration:
- Configure model with enable_thinking=False (non-thinking mode only)
- Focus evaluation on English-only token subset

Lightning.AI Studios Integration:
- Create a dedicated evaluation Studio following the "one Studio, one task" principle
- Utilize GPU switching feature (CPU → T4 → A100) for cost-effective evaluation
- Ensure sufficient memory allocation for Qwen3-4B model requirements
- Leverage Lightning.AI plugins for experiment tracking and visualization
- Use shared filesystem for accessing models and datasets

Data Sources:
- Training data: Spelling variations with multicharacter tokens (for training only)
- Evaluation data: Multi-token words for position and character count questions ONLY

Evaluation Approach:
- Focus on measuring transfer learning effectiveness from spelling training to position/count tasks
- No traditional train/val/test split since evaluation uses different data
- Separate evaluation pipeline for position and count metrics ONLY
- Implement evaluation metrics using TorchMetrics for optimized distributed evaluation

File Structure:
- Main framework: `src/evaluation/framework.py`
- Metrics definitions: `src/evaluation/metrics.py`
- Evaluation config: `configs/evaluation/base_config.yaml`
- Lightning.AI Studio config: `configs/evaluation/lightning_studio.yaml`
- Letter count evaluator: `src/evaluation/letter_count.py`
- Letter position evaluator: `src/evaluation/letter_position.py`
- Common utilities: `src/evaluation/utils.py`
- Visualization utilities: `src/evaluation/visualization.py`
- Report generation: `src/evaluation/report.py`
- Results directory: `results/evaluation/`
- Visualizations: `results/evaluation/figures/`
- HTML reports: `results/evaluation/reports/`
- Processed data: `results/evaluation/data/`
- Raw metrics: `results/evaluation/data/metrics.json`
- Detailed analysis: `results/evaluation/data/analysis.json`

Implementation:
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import dspy
import wandb
import argparse
import os
import json
import pandas as pd
import torchmetrics
from lightning.pytorch import loggers as pl_loggers
from lightning.app import LightningApp, LightningWork
from lightning.app.components import TracerPythonScript

def parse_args():
    parser = argparse.ArgumentParser(description="Evaluate Qwen3-4B model on letter count and position tasks")
    parser.add_argument("--model", type=str, default="Qwen/Qwen3-4B", help="Model to evaluate")
    parser.add_argument("--output_dir", type=str, default="results/evaluation", help="Directory to save results")
    parser.add_argument("--log_wandb", action="store_true", help="Whether to log results to W&B")
    parser.add_argument("--use_lightning", action="store_true", help="Whether to use Lightning.AI for distributed evaluation")
    parser.add_argument("--gpu_tier", type=str, default="T4", choices=["CPU", "T4", "A100"], help="GPU tier to use for evaluation")
    return parser.parse_args()

class EvaluationWork(LightningWork):
    def __init__(self, model_name, output_dir, log_wandb=False, gpu_tier="T4"):
        super().__init__(cloud_compute={"gpu_type": gpu_tier.lower() if gpu_tier != "CPU" else None})
        self.model_name = model_name
        self.output_dir = output_dir
        self.log_wandb = log_wandb
        
    def run(self):
        # Create output directories
        os.makedirs(f"{self.output_dir}/data", exist_ok=True)
        os.makedirs(f"{self.output_dir}/figures", exist_ok=True)
        os.makedirs(f"{self.output_dir}/reports", exist_ok=True)
        
        # Initialize W&B if requested
        if self.log_wandb:
            wandb.init(project="llm-spelling-finetuning", name="qwen3-baseline-evaluation")
        
        # Load Qwen3-4B model and tokenizer
        model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map="auto", trust_remote_code=True)
        model.eval()
        tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
        
        # Load evaluation dataset with multi-token words
        from datasets import load_dataset
        eval_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset", split="evaluation")
        
        # Initialize TorchMetrics for evaluation
        letter_count_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=20)  # Assuming max 20 characters
        letter_position_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=26)  # A-Z positions
        
        # Define generation function with Qwen3-specific parameters
        def generate_answer(model, tokenizer, question, max_length=10):
            inputs = tokenizer(question, return_tensors="pt").to(model.device)
            
            # Configure generation parameters (non-thinking mode only)
            generation_config = {
                "max_new_tokens": max_length,
                "do_sample": False,
                "pad_token_id": tokenizer.pad_token_id,
                "enable_thinking": False  # Explicitly disable thinking mode
            }
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    **generation_config
                )
            
            response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
            
            # Extract just the first token/character for letter position or first number for letter count
            if "How many" in question:
                # Extract first number
                import re
                numbers = re.findall(r'\d+', response)
                return numbers[0] if numbers else response.strip()
            else:
                # Extract first character
                return response.strip()[0] if response.strip() else ""
        
        # Evaluate on letter count questions
        def evaluate_letter_count(model, tokenizer, dataset):
            correct = 0
            total = 0
            results = []
            
            for item in dataset:
                if item["question_type"] != "letter_count":
                    continue
                    
                prediction = generate_answer(model, tokenizer, item["question"])
                is_correct = prediction == item["answer"]
                
                results.append({
                    "question": item["question"],
                    "expected": item["answer"],
                    "prediction": prediction,
                    "correct": is_correct
                })
                
                correct += int(is_correct)
                total += 1
            
            accuracy = correct / total if total > 0 else 0
            print(f"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})")
            
            return accuracy, results
        
        # Evaluate on letter position questions
        def evaluate_letter_position(model, tokenizer, dataset):
            correct = 0
            total = 0
            results = []
            
            for item in dataset:
                if item["question_type"] != "letter_position":
                    continue
                    
                prediction = generate_answer(model, tokenizer, item["question"])
                is_correct = prediction.lower() == item["answer"].lower()
                
                results.append({
                    "question": item["question"],
                    "expected": item["answer"],
                    "prediction": prediction,
                    "correct": is_correct
                })
                
                correct += int(is_correct)
                total += 1
            
            accuracy = correct / total if total > 0 else 0
            print(f"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})")
            
            return accuracy, results
        
        # Run evaluation - only for count and position tasks
        count_accuracy, count_results = evaluate_letter_count(model, tokenizer, eval_dataset)
        position_accuracy, position_results = evaluate_letter_position(model, tokenizer, eval_dataset)
        
        # Log results to W&B if requested
        if self.log_wandb:
            wandb.log({
                "letter_count_accuracy": count_accuracy,
                "letter_position_accuracy": position_accuracy,
                "count_examples": wandb.Table(dataframe=pd.DataFrame(count_results[:20])),
                "position_examples": wandb.Table(dataframe=pd.DataFrame(position_results[:20]))
            })
        
        # Save results locally
        metrics_data = {
            "model": self.model_name,
            "letter_count_accuracy": count_accuracy,
            "letter_position_accuracy": position_accuracy,
            "count_results": count_results,
            "position_results": position_results
        }
        
        with open(f"{self.output_dir}/data/metrics.json", "w") as f:
            json.dump(metrics_data, f, indent=2)
        
        # Generate visualizations
        from src.evaluation.visualization import create_accuracy_chart, create_error_analysis
        create_accuracy_chart(metrics_data, f"{self.output_dir}/figures/accuracy.png")
        create_error_analysis(metrics_data, f"{self.output_dir}/figures/error_analysis.png")
        
        # Generate HTML report
        from src.evaluation.report import generate_html_report
        generate_html_report(metrics_data, f"{self.output_dir}/reports/baseline_report.html")
        
        print(f"Evaluation complete. Results saved to {self.output_dir}")

def main():
    args = parse_args()
    
    if args.use_lightning:
        # Use Lightning.AI for distributed evaluation
        eval_work = EvaluationWork(
            model_name=args.model,
            output_dir=args.output_dir,
            log_wandb=args.log_wandb,
            gpu_tier=args.gpu_tier
        )
        app = LightningApp(eval_work)
        app.run()
    else:
        # Create output directories
        os.makedirs(f"{args.output_dir}/data", exist_ok=True)
        os.makedirs(f"{args.output_dir}/figures", exist_ok=True)
        os.makedirs(f"{args.output_dir}/reports", exist_ok=True)
        
        # Initialize W&B if requested
        if args.log_wandb:
            wandb.init(project="llm-spelling-finetuning", name="qwen3-baseline-evaluation")
        
        # Load Qwen3-4B model and tokenizer
        model_name = args.model
        model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", trust_remote_code=True)
        model.eval()
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        
        # Load evaluation dataset with multi-token words
        from datasets import load_dataset
        eval_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset", split="evaluation")
        
        # Define generation function with Qwen3-specific parameters
        def generate_answer(model, tokenizer, question, max_length=10):
            inputs = tokenizer(question, return_tensors="pt").to(model.device)
            
            # Configure generation parameters (non-thinking mode only)
            generation_config = {
                "max_new_tokens": max_length,
                "do_sample": False,
                "pad_token_id": tokenizer.pad_token_id,
                "enable_thinking": False  # Explicitly disable thinking mode
            }
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    **generation_config
                )
            
            response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
            
            # Extract just the first token/character for letter position or first number for letter count
            if "How many" in question:
                # Extract first number
                import re
                numbers = re.findall(r'\d+', response)
                return numbers[0] if numbers else response.strip()
            else:
                # Extract first character
                return response.strip()[0] if response.strip() else ""
        
        # Evaluate on letter count questions
        def evaluate_letter_count(model, tokenizer, dataset):
            correct = 0
            total = 0
            results = []
            
            for item in dataset:
                if item["question_type"] != "letter_count":
                    continue
                    
                prediction = generate_answer(model, tokenizer, item["question"])
                is_correct = prediction == item["answer"]
                
                results.append({
                    "question": item["question"],
                    "expected": item["answer"],
                    "prediction": prediction,
                    "correct": is_correct
                })
                
                correct += int(is_correct)
                total += 1
            
            accuracy = correct / total if total > 0 else 0
            print(f"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})")
            
            return accuracy, results
        
        # Evaluate on letter position questions
        def evaluate_letter_position(model, tokenizer, dataset):
            correct = 0
            total = 0
            results = []
            
            for item in dataset:
                if item["question_type"] != "letter_position":
                    continue
                    
                prediction = generate_answer(model, tokenizer, item["question"])
                is_correct = prediction.lower() == item["answer"].lower()
                
                results.append({
                    "question": item["question"],
                    "expected": item["answer"],
                    "prediction": prediction,
                    "correct": is_correct
                })
                
                correct += int(is_correct)
                total += 1
            
            accuracy = correct / total if total > 0 else 0
            print(f"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})")
            
            return accuracy, results
        
        # Run evaluation - only for count and position tasks
        count_accuracy, count_results = evaluate_letter_count(model, tokenizer, eval_dataset)
        position_accuracy, position_results = evaluate_letter_position(model, tokenizer, eval_dataset)
        
        # Log results to W&B if requested
        if args.log_wandb:
            wandb.log({
                "letter_count_accuracy": count_accuracy,
                "letter_position_accuracy": position_accuracy,
                "count_examples": wandb.Table(dataframe=pd.DataFrame(count_results[:20])),
                "position_examples": wandb.Table(dataframe=pd.DataFrame(position_results[:20]))
            })
        
        # Save results locally
        metrics_data = {
            "model": model_name,
            "letter_count_accuracy": count_accuracy,
            "letter_position_accuracy": position_accuracy,
            "count_results": count_results,
            "position_results": position_results
        }
        
        with open(f"{args.output_dir}/data/metrics.json", "w") as f:
            json.dump(metrics_data, f, indent=2)
        
        # Generate visualizations
        from src.evaluation.visualization import create_accuracy_chart, create_error_analysis
        create_accuracy_chart(metrics_data, f"{args.output_dir}/figures/accuracy.png")
        create_error_analysis(metrics_data, f"{args.output_dir}/figures/error_analysis.png")
        
        # Generate HTML report
        from src.evaluation.report import generate_html_report
        generate_html_report(metrics_data, f"{args.output_dir}/reports/baseline_report.html")
        
        print(f"Evaluation complete. Results saved to {args.output_dir}")

if __name__ == "__main__":
    main()
```

# Test Strategy:
1. Verify the evaluation scripts run without errors with Qwen3-4B model
2. Test command-line arguments for flexibility, including Lightning.AI specific arguments
3. Confirm output format is correct (single integer for count, single letter for position)
4. Check that results are properly logged to W&B when specified
5. Verify baseline performance metrics are saved to `results/evaluation/data/metrics.json`
6. Ensure visualizations are correctly generated in `results/evaluation/figures/`
7. Validate HTML reports are generated in `results/evaluation/reports/`
8. Test error handling for edge cases
9. Analyze error patterns in Qwen3-4B model predictions and document in `results/evaluation/data/analysis.json`
10. Ensure documentation is complete in `docs/evaluation.md`, `docs/metrics.md`, `docs/baseline_results.md`, and `docs/lightning_studio_setup.md`
11. Verify that the Python scripts can be imported and used as modules by other components
12. Confirm the evaluation correctly uses multi-token words for position and count tasks ONLY
13. Validate that the evaluation framework properly measures transfer learning effectiveness
14. Test for correlation analysis between spelling training and position/count task performance
15. Verify Lightning.AI Studio setup and configuration works correctly with Qwen3-4B's memory requirements
16. Test GPU switching functionality (CPU → T4 → A100) for cost optimization
17. Validate TorchMetrics integration for distributed evaluation
18. Test automated job submission for batch evaluations
19. Verify environment isolation and dependency management in Lightning.AI Studio
20. Verify that thinking mode is never enabled in any configuration or code path
21. Test English-only token subset focus in evaluation
22. Verify no spelling evaluation is included in any part of the evaluation process
23. Confirm all metrics, reports, and visualizations focus exclusively on position and count tasks

# Subtasks:
## 1. Evaluation Framework Setup [pending]
### Dependencies: None
### Description: Establish the hierarchical evaluation framework structure for NLP model assessment
### Details:
Create a modular evaluation framework that supports both automated and human evaluation components. Implement a transfer learning evaluation approach using multi-token words for position and count tasks only. Ensure the framework can handle diverse linguistic structures and edge cases. Set up configuration files for evaluation parameters and thresholds.

## 2. Metrics Definition and Implementation [pending]
### Dependencies: 5.1
### Description: Define and implement comprehensive evaluation metrics for model assessment
### Details:
Implement position accuracy and count accuracy metrics for multi-token words. Add specialized metrics for transfer learning effectiveness evaluation. Create a metrics registry system that allows for easy addition of new metrics. Ensure all metrics are properly documented with mathematical formulations. Implement analysis of error patterns and potential correlation with spelling training.

## 3. Letter Count Evaluator Implementation [pending]
### Dependencies: 5.2
### Description: Develop specialized evaluator for letter count assessment
### Details:
Implement a dedicated evaluator that analyzes the model's ability to count letters in multi-token words. Create test cases with varying complexity levels. Implement error tolerance thresholds. Design the evaluator to track performance across different text lengths. Include detailed logging of evaluation results for later analysis of transfer learning effectiveness.

## 4. Position Evaluator Implementation [pending]
### Dependencies: 5.2
### Description: Develop specialized evaluator for letter position assessment
### Details:
Create an evaluator that tests the model's ability to identify letter positions within multi-token words. Implement position-based metrics including absolute and relative position accuracy. Design test cases with varying complexity. Include support for different character sets. Implement detailed error tracking for position-based mistakes to analyze transfer learning effectiveness.

## 5. Results Visualization System [pending]
### Dependencies: 5.2, 5.3, 5.4
### Description: Develop comprehensive visualization tools for evaluation results
### Details:
Create interactive dashboards showing performance across position and count metrics only. Implement comparison visualizations between model versions. Design visualizations to show transfer learning effectiveness from spelling training to position/count tasks. Include error distribution visualizations. Ensure all visualizations are exportable in multiple formats (PNG, PDF, interactive HTML).

## 6. Error Analysis Framework [pending]
### Dependencies: 5.3, 5.4, 5.5
### Description: Implement systematic error analysis capabilities
### Details:
Develop tools to categorize and analyze error patterns for position and count tasks only. Create clustering algorithms to group similar errors. Implement analysis of correlation between spelling training and position/count task performance. Design interfaces for domain experts to review and annotate errors. Include recommendation generation for model improvements based on error patterns.

## 7. Documentation and Reporting [pending]
### Dependencies: 5.1, 5.2, 5.5, 5.6
### Description: Create comprehensive documentation and automated reporting
### Details:
Document the entire evaluation framework architecture with focus on transfer learning approach from spelling training to position/count tasks. Create user guides for running evaluations. Implement automated report generation with executive summaries and detailed technical appendices. Include benchmark comparisons against industry standards. Design templates for different stakeholder audiences (technical, management, etc.).

## 8. Integration with External Components [pending]
### Dependencies: 5.1, 5.2, 5.5, 5.7
### Description: Ensure seamless integration with other system components
### Details:
Develop APIs for integration with model training pipelines. Implement webhooks for continuous evaluation triggers. Create data exchange formats for evaluation results. Design integration with CI/CD pipelines for automated testing. Implement monitoring capabilities to track transfer learning effectiveness over time.

## 9. Transfer Learning Effectiveness Analysis [pending]
### Dependencies: 5.3, 5.4, 5.6
### Description: Implement analysis of transfer learning from spelling training to position/count tasks
### Details:
Develop metrics to quantify transfer learning effectiveness. Create visualization tools to show correlation between spelling training and position/count task performance. Implement statistical analysis to identify significant patterns. Design experiments to test different transfer learning hypotheses. Document findings in comprehensive reports.

## 10. Lightning.AI Studio Setup [pending]
### Dependencies: 5.1
### Description: Configure dedicated Lightning.AI Studio for evaluation tasks
### Details:
Create a dedicated evaluation Studio following the "one Studio, one task" principle. Configure proper environment isolation and dependency management. Set up shared filesystem access for models and datasets. Implement GPU switching functionality (CPU → T4 → A100) for cost optimization. Create configuration files for Lightning.AI Studio setup.

## 11. TorchMetrics Integration [pending]
### Dependencies: 5.2, 5.10
### Description: Implement evaluation metrics using TorchMetrics for optimized distributed evaluation
### Details:
Refactor existing metrics to use TorchMetrics for better performance in distributed environments. Implement custom TorchMetrics classes for specialized evaluation needs. Ensure metrics are properly synchronized across distributed processes. Add support for metric serialization and deserialization for result persistence.

## 12. Automated Job Submission [pending]
### Dependencies: 5.10, 5.11
### Description: Set up automated job submission for batch evaluations
### Details:
Implement batch job submission system for running multiple evaluations. Create job templates for different evaluation scenarios. Set up job scheduling and prioritization. Implement notification system for job completion. Design job monitoring dashboard for tracking evaluation progress.

## 13. Lightning.AI Documentation [pending]
### Dependencies: 5.10, 5.11, 5.12
### Description: Create comprehensive documentation for Lightning.AI Studio setup and usage
### Details:
Document Lightning.AI Studio setup process. Create tutorials for running evaluations in Lightning.AI. Document GPU switching functionality and cost optimization strategies. Include troubleshooting guides for common issues. Create reference documentation for all Lightning.AI specific configuration options.

## 16. Non-Thinking Mode Configuration [pending]
### Dependencies: 5.1, 5.2
### Description: Ensure Qwen3-4B is properly configured with non-thinking mode
### Details:
Configure Qwen3-4B with enable_thinking=False to ensure compliance with project policy. Verify all code paths explicitly disable thinking mode. Update all configuration files to reflect non-thinking mode only. Document the non-thinking mode configuration in all relevant documentation. Create validation tests to ensure thinking mode is never enabled.

## 15. English-only Token Subset Focus [pending]
### Dependencies: 5.1, 5.16
### Description: Configure evaluation to focus on English-only token subset
### Details:
Implement filtering mechanisms to focus evaluation on English-only token subset. Create analysis tools to measure performance differences between full vocabulary and English-only subset. Document impact of token subset focus on evaluation results. Implement visualization tools to highlight performance differences across token subsets.

## 17. Position and Count Task Filtering [pending]
### Dependencies: 5.1, 5.2
### Description: Ensure evaluation focuses exclusively on position and count tasks
### Details:
Implement strict filtering to ensure only position and count tasks are evaluated. Create validation checks to verify no spelling evaluation is included. Update all configuration files to exclude spelling tasks. Document the focus on position and count tasks in all relevant documentation. Create validation tests to ensure no spelling evaluation occurs.

## 18. Evaluation Report Refactoring [pending]
### Dependencies: 5.5, 5.7
### Description: Refactor reporting to focus exclusively on position and count metrics
### Details:
Update all reporting templates to focus exclusively on position and count metrics. Remove any references to spelling evaluation from reports. Create new visualization templates specific to position and count tasks. Ensure all metrics and analyses in reports relate only to position and count tasks. Update documentation to reflect the focus on position and count tasks only.

