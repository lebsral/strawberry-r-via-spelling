# Task ID: 5
# Title: Baseline Model Evaluation
# Status: pending
# Dependencies: 3
# Priority: medium
# Description: Evaluate the base GPT-2 model on letter count and position tasks to establish a performance baseline.
# Details:
1. Set up DsPy for multi-shot prompting
2. Create a Jupyter notebook to evaluate the base GPT-2 model
3. Test on both primary metrics (letter count, letter position)
4. Ensure output format is correct (single integer for character count or single letter for character position)
5. Document the baseline performance for comparison

File Structure:
- Main framework: `src/evaluation/framework.py`
- Metrics definitions: `src/evaluation/metrics.py`
- Evaluation config: `configs/evaluation/base_config.yaml`
- Letter count evaluator: `src/evaluation/letter_count.py`
- Letter position evaluator: `src/evaluation/letter_position.py`
- Common utilities: `src/evaluation/utils.py`
- Results directory: `results/baseline_evaluation/`
- Visualizations: `results/baseline_evaluation/figures/`
- Raw metrics: `results/baseline_evaluation/metrics.json`
- Detailed analysis: `results/baseline_evaluation/analysis.json`
- Results analysis notebook: `notebooks/baseline_analysis.ipynb`
- Performance visualization notebook: `notebooks/baseline_visualization.ipynb`

Implementation:
```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import dspy
import wandb

# Initialize W&B
wandb.init(project="llm-spelling-finetuning", name="baseline-evaluation")

# Load base model and tokenizer
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
model.eval()
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Load test dataset
from datasets import load_dataset
test_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset", split="test")

# Define generation function
def generate_answer(model, tokenizer, question, max_length=10):
    inputs = tokenizer(question, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_length=len(inputs.input_ids[0]) + max_length,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=False
        )
    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
    # Extract just the first token/character for letter position or first number for letter count
    if "How many" in question:
        # Extract first number
        import re
        numbers = re.findall(r'\d+', response)
        return numbers[0] if numbers else response.strip()
    else:
        # Extract first character
        return response.strip()[0] if response.strip() else ""

# Evaluate on letter count questions
def evaluate_letter_count(model, tokenizer, dataset):
    correct = 0
    total = 0
    results = []
    
    for item in dataset:
        if item["question_type"] != "letter_count":
            continue
            
        prediction = generate_answer(model, tokenizer, item["question"])
        is_correct = prediction == item["answer"]
        
        results.append({
            "question": item["question"],
            "expected": item["answer"],
            "prediction": prediction,
            "correct": is_correct
        })
        
        correct += int(is_correct)
        total += 1
    
    accuracy = correct / total if total > 0 else 0
    print(f"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})")
    
    return accuracy, results

# Evaluate on letter position questions
def evaluate_letter_position(model, tokenizer, dataset):
    correct = 0
    total = 0
    results = []
    
    for item in dataset:
        if item["question_type"] != "letter_position":
            continue
            
        prediction = generate_answer(model, tokenizer, item["question"])
        is_correct = prediction.lower() == item["answer"].lower()
        
        results.append({
            "question": item["question"],
            "expected": item["answer"],
            "prediction": prediction,
            "correct": is_correct
        })
        
        correct += int(is_correct)
        total += 1
    
    accuracy = correct / total if total > 0 else 0
    print(f"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})")
    
    return accuracy, results

# Run evaluation
count_accuracy, count_results = evaluate_letter_count(model, tokenizer, test_dataset)
position_accuracy, position_results = evaluate_letter_position(model, tokenizer, test_dataset)

# Log results to W&B
wandb.log({
    "letter_count_accuracy": count_accuracy,
    "letter_position_accuracy": position_accuracy,
    "count_examples": wandb.Table(dataframe=pd.DataFrame(count_results[:20])),
    "position_examples": wandb.Table(dataframe=pd.DataFrame(position_results[:20]))
})

# Save results locally
import json
with open("results/baseline_evaluation/metrics.json", "w") as f:
    json.dump({
        "letter_count_accuracy": count_accuracy,
        "letter_position_accuracy": position_accuracy,
        "count_results": count_results,
        "position_results": position_results
    }, f, indent=2)
```

# Test Strategy:
1. Verify the evaluation script runs without errors
2. Confirm output format is correct (single integer for count, single letter for position)
3. Check that results are properly logged to W&B
4. Verify baseline performance metrics are saved to `results/baseline_evaluation/metrics.json`
5. Create visualizations in `results/baseline_evaluation/figures/` comparing performance on different question types
6. Analyze error patterns in baseline model predictions and document in `results/baseline_evaluation/analysis.json`
7. Ensure documentation is complete in `docs/evaluation.md`, `docs/metrics.md`, and `docs/baseline_results.md`
8. Validate that notebooks in `notebooks/` can load and visualize the evaluation results

# Subtasks:
## 1. Evaluation Framework Setup with Metrics Definition [pending]
### Dependencies: None
### Description: Establish the baseline evaluation framework and define appropriate performance metrics for assessing model performance on letter-based questions.
### Details:
Create a comprehensive evaluation framework that includes: 1) Define key metrics such as accuracy, precision, F1-score, and recall for letter-based question evaluation in `src/evaluation/metrics.py`; 2) Set up validation thresholds for each metric in `configs/evaluation/base_config.yaml`; 3) Design a data structure to store and track evaluation results; 4) Create functions in `src/evaluation/framework.py` to calculate and compare metrics against established baselines; 5) Document the evaluation methodology and metrics selection rationale in `docs/evaluation.md` and `docs/metrics.md`.

## 2. Letter Count Question Evaluation Implementation [pending]
### Dependencies: 5.1
### Description: Implement specific evaluation components for assessing model performance on letter count questions.
### Details:
Develop evaluation components for letter count questions including: 1) Create a dataset of letter count questions with ground truth answers; 2) Implement functions in `src/evaluation/letter_count.py` to process model predictions for letter count questions; 3) Calculate accuracy and precision metrics specifically for count-based questions; 4) Establish baseline performance expectations for this question type; 5) Create validation tests to ensure the evaluation implementation is working correctly. Store results in `results/baseline_evaluation/metrics.json`.

## 3. Letter Position Question Evaluation Implementation [pending]
### Dependencies: 5.1
### Description: Implement specific evaluation components for assessing model performance on letter position questions.
### Details:
Develop evaluation components for letter position questions including: 1) Create a dataset of letter position questions with ground truth answers; 2) Implement functions in `src/evaluation/letter_position.py` to process model predictions for position-based questions; 3) Calculate accuracy and position-specific metrics (e.g., distance from correct position); 4) Establish baseline performance expectations for this question type; 5) Create validation tests to ensure the evaluation implementation is working correctly. Store results in `results/baseline_evaluation/metrics.json`.

## 4. Results Analysis and Visualization [pending]
### Dependencies: 5.2, 5.3
### Description: Analyze evaluation results across question types and create visualizations to communicate model performance.
### Details:
Develop comprehensive analysis and visualization components including: 1) Aggregate results from both question types to assess overall model performance and store in `results/baseline_evaluation/analysis.json`; 2) Create comparative visualizations in `results/baseline_evaluation/figures/` showing performance across different metrics and question types; 3) Implement statistical analysis to identify performance patterns and areas for improvement; 4) Generate automated performance reports with key findings and recommendations in `docs/baseline_results.md`; 5) Create interactive notebooks `notebooks/baseline_analysis.ipynb` and `notebooks/baseline_visualization.ipynb` for exploring evaluation results in detail.

## 5. Common Utilities Implementation [pending]
### Dependencies: 5.1
### Description: Implement shared utility functions for evaluation components
### Details:
Create `src/evaluation/utils.py` with common functions needed across evaluation components including: 1) Data loading and preprocessing utilities; 2) Result formatting and standardization functions; 3) File I/O helpers for consistent saving and loading of results; 4) Metric calculation helpers; 5) Visualization utilities that can be used by both evaluation scripts and notebooks.

## 6. Documentation Completion [pending]
### Dependencies: 5.1, 5.2, 5.3, 5.4
### Description: Complete all documentation for the evaluation framework and results
### Details:
Ensure all documentation is complete and comprehensive: 1) Create `docs/evaluation.md` describing the overall evaluation methodology; 2) Create `docs/metrics.md` with detailed explanations of all metrics used; 3) Create `docs/baseline_results.md` with interpretation of baseline model performance; 4) Add inline documentation to all code files; 5) Create README files for each directory explaining its purpose and contents.

