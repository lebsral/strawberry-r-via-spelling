# Task ID: 5
# Title: Baseline Model Evaluation
# Status: pending
# Dependencies: 3
# Priority: medium
# Description: Evaluate the base GPT-2 model on letter count and position tasks to establish a performance baseline for transfer learning effectiveness.
# Details:
1. Set up DsPy for multi-shot prompting
2. Create Python scripts to evaluate the base GPT-2 model
3. Test on both primary metrics (letter count, letter position) using multi-token words
4. Ensure output format is correct (single integer for character count or single letter for character position)
5. Document the baseline performance for comparison

Data Sources:
- Training data: Spelling variations with multicharacter tokens
- Evaluation data: Multi-token words for position and character count questions

Evaluation Approach:
- Focus on measuring transfer learning effectiveness from spelling training to position/count tasks
- No traditional train/val/test split since evaluation uses different data
- Separate evaluation pipeline for position and count metrics

File Structure:
- Main framework: `src/evaluation/framework.py`
- Metrics definitions: `src/evaluation/metrics.py`
- Evaluation config: `configs/evaluation/base_config.yaml`
- Letter count evaluator: `src/evaluation/letter_count.py`
- Letter position evaluator: `src/evaluation/letter_position.py`
- Common utilities: `src/evaluation/utils.py`
- Visualization utilities: `src/evaluation/visualization.py`
- Report generation: `src/evaluation/report.py`
- Results directory: `results/evaluation/`
- Visualizations: `results/evaluation/figures/`
- HTML reports: `results/evaluation/reports/`
- Processed data: `results/evaluation/data/`
- Raw metrics: `results/evaluation/data/metrics.json`
- Detailed analysis: `results/evaluation/data/analysis.json`

Implementation:
```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import dspy
import wandb
import argparse
import os
import json
import pandas as pd

def parse_args():
    parser = argparse.ArgumentParser(description="Evaluate baseline GPT-2 model on spelling tasks")
    parser.add_argument("--model", type=str, default="gpt2", help="Model to evaluate")
    parser.add_argument("--output_dir", type=str, default="results/evaluation", help="Directory to save results")
    parser.add_argument("--log_wandb", action="store_true", help="Whether to log results to W&B")
    return parser.parse_args()

def main():
    args = parse_args()
    
    # Create output directories
    os.makedirs(f"{args.output_dir}/data", exist_ok=True)
    os.makedirs(f"{args.output_dir}/figures", exist_ok=True)
    os.makedirs(f"{args.output_dir}/reports", exist_ok=True)
    
    # Initialize W&B if requested
    if args.log_wandb:
        wandb.init(project="llm-spelling-finetuning", name="baseline-evaluation")
    
    # Load base model and tokenizer
    model_name = args.model
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.eval()
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    # Load evaluation dataset with multi-token words
    from datasets import load_dataset
    eval_dataset = load_dataset("YOUR-USERNAME/llm-spelling-dataset", split="evaluation")
    
    # Define generation function
    def generate_answer(model, tokenizer, question, max_length=10):
        inputs = tokenizer(question, return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_length=len(inputs.input_ids[0]) + max_length,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=False
            )
        response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
        # Extract just the first token/character for letter position or first number for letter count
        if "How many" in question:
            # Extract first number
            import re
            numbers = re.findall(r'\d+', response)
            return numbers[0] if numbers else response.strip()
        else:
            # Extract first character
            return response.strip()[0] if response.strip() else ""
    
    # Evaluate on letter count questions
    def evaluate_letter_count(model, tokenizer, dataset):
        correct = 0
        total = 0
        results = []
        
        for item in dataset:
            if item["question_type"] != "letter_count":
                continue
                
            prediction = generate_answer(model, tokenizer, item["question"])
            is_correct = prediction == item["answer"]
            
            results.append({
                "question": item["question"],
                "expected": item["answer"],
                "prediction": prediction,
                "correct": is_correct
            })
            
            correct += int(is_correct)
            total += 1
        
        accuracy = correct / total if total > 0 else 0
        print(f"Letter Count Accuracy: {accuracy:.4f} ({correct}/{total})")
        
        return accuracy, results
    
    # Evaluate on letter position questions
    def evaluate_letter_position(model, tokenizer, dataset):
        correct = 0
        total = 0
        results = []
        
        for item in dataset:
            if item["question_type"] != "letter_position":
                continue
                
            prediction = generate_answer(model, tokenizer, item["question"])
            is_correct = prediction.lower() == item["answer"].lower()
            
            results.append({
                "question": item["question"],
                "expected": item["answer"],
                "prediction": prediction,
                "correct": is_correct
            })
            
            correct += int(is_correct)
            total += 1
        
        accuracy = correct / total if total > 0 else 0
        print(f"Letter Position Accuracy: {accuracy:.4f} ({correct}/{total})")
        
        return accuracy, results
    
    # Run evaluation
    count_accuracy, count_results = evaluate_letter_count(model, tokenizer, eval_dataset)
    position_accuracy, position_results = evaluate_letter_position(model, tokenizer, eval_dataset)
    
    # Log results to W&B if requested
    if args.log_wandb:
        wandb.log({
            "letter_count_accuracy": count_accuracy,
            "letter_position_accuracy": position_accuracy,
            "count_examples": wandb.Table(dataframe=pd.DataFrame(count_results[:20])),
            "position_examples": wandb.Table(dataframe=pd.DataFrame(position_results[:20]))
        })
    
    # Save results locally
    metrics_data = {
        "letter_count_accuracy": count_accuracy,
        "letter_position_accuracy": position_accuracy,
        "count_results": count_results,
        "position_results": position_results
    }
    
    with open(f"{args.output_dir}/data/metrics.json", "w") as f:
        json.dump(metrics_data, f, indent=2)
    
    # Generate visualizations
    from src.evaluation.visualization import create_accuracy_chart, create_error_analysis
    create_accuracy_chart(metrics_data, f"{args.output_dir}/figures/accuracy.png")
    create_error_analysis(metrics_data, f"{args.output_dir}/figures/error_analysis.png")
    
    # Generate HTML report
    from src.evaluation.report import generate_html_report
    generate_html_report(metrics_data, f"{args.output_dir}/reports/baseline_report.html")
    
    print(f"Evaluation complete. Results saved to {args.output_dir}")

if __name__ == "__main__":
    main()
```

# Test Strategy:
1. Verify the evaluation scripts run without errors
2. Test command-line arguments for flexibility
3. Confirm output format is correct (single integer for count, single letter for position)
4. Check that results are properly logged to W&B when specified
5. Verify baseline performance metrics are saved to `results/evaluation/data/metrics.json`
6. Ensure visualizations are correctly generated in `results/evaluation/figures/`
7. Validate HTML reports are generated in `results/evaluation/reports/`
8. Test error handling for edge cases
9. Analyze error patterns in baseline model predictions and document in `results/evaluation/data/analysis.json`
10. Ensure documentation is complete in `docs/evaluation.md`, `docs/metrics.md`, and `docs/baseline_results.md`
11. Verify that the Python scripts can be imported and used as modules by other components
12. Confirm the evaluation correctly uses multi-token words for position and count tasks
13. Validate that the evaluation framework properly measures transfer learning effectiveness
14. Test for correlation analysis between spelling training and position/count task performance

# Subtasks:
## 1. Evaluation Framework Setup [pending]
### Dependencies: None
### Description: Establish the hierarchical evaluation framework structure for NLP model assessment
### Details:
Create a modular evaluation framework that supports both automated and human evaluation components. Implement a transfer learning evaluation approach using multi-token words for position and count tasks. Ensure the framework can handle diverse linguistic structures and edge cases. Set up configuration files for evaluation parameters and thresholds.

## 2. Metrics Definition and Implementation [pending]
### Dependencies: 5.1
### Description: Define and implement comprehensive evaluation metrics for model assessment
### Details:
Implement position accuracy and count accuracy metrics for multi-token words. Add specialized metrics for transfer learning effectiveness evaluation. Create a metrics registry system that allows for easy addition of new metrics. Ensure all metrics are properly documented with mathematical formulations. Implement analysis of error patterns and potential correlation with spelling training.

## 3. Letter Count Evaluator Implementation [pending]
### Dependencies: 5.2
### Description: Develop specialized evaluator for letter count assessment
### Details:
Implement a dedicated evaluator that analyzes the model's ability to count letters in multi-token words. Create test cases with varying complexity levels. Implement error tolerance thresholds. Design the evaluator to track performance across different text lengths. Include detailed logging of evaluation results for later analysis of transfer learning effectiveness.

## 4. Position Evaluator Implementation [pending]
### Dependencies: 5.2
### Description: Develop specialized evaluator for letter position assessment
### Details:
Create an evaluator that tests the model's ability to identify letter positions within multi-token words. Implement position-based metrics including absolute and relative position accuracy. Design test cases with varying complexity. Include support for different character sets. Implement detailed error tracking for position-based mistakes to analyze transfer learning effectiveness.

## 5. Results Visualization System [pending]
### Dependencies: 5.2, 5.3, 5.4
### Description: Develop comprehensive visualization tools for evaluation results
### Details:
Create interactive dashboards showing performance across all metrics. Implement comparison visualizations between model versions. Design visualizations to show transfer learning effectiveness from spelling training to position/count tasks. Include error distribution visualizations. Ensure all visualizations are exportable in multiple formats (PNG, PDF, interactive HTML).

## 6. Error Analysis Framework [pending]
### Dependencies: 5.3, 5.4, 5.5
### Description: Implement systematic error analysis capabilities
### Details:
Develop tools to categorize and analyze error patterns. Create clustering algorithms to group similar errors. Implement analysis of correlation between spelling training and position/count task performance. Design interfaces for domain experts to review and annotate errors. Include recommendation generation for model improvements based on error patterns.

## 7. Documentation and Reporting [pending]
### Dependencies: 5.1, 5.2, 5.5, 5.6
### Description: Create comprehensive documentation and automated reporting
### Details:
Document the entire evaluation framework architecture with focus on transfer learning approach. Create user guides for running evaluations. Implement automated report generation with executive summaries and detailed technical appendices. Include benchmark comparisons against industry standards. Design templates for different stakeholder audiences (technical, management, etc.).

## 8. Integration with External Components [pending]
### Dependencies: 5.1, 5.2, 5.5, 5.7
### Description: Ensure seamless integration with other system components
### Details:
Develop APIs for integration with model training pipelines. Implement webhooks for continuous evaluation triggers. Create data exchange formats for evaluation results. Design integration with CI/CD pipelines for automated testing. Implement monitoring capabilities to track transfer learning effectiveness over time.

## 9. Transfer Learning Effectiveness Analysis [pending]
### Dependencies: 5.3, 5.4, 5.6
### Description: Implement analysis of transfer learning from spelling training to position/count tasks
### Details:
Develop metrics to quantify transfer learning effectiveness. Create visualization tools to show correlation between spelling training and position/count task performance. Implement statistical analysis to identify significant patterns. Design experiments to test different transfer learning hypotheses. Document findings in comprehensive reports.

